{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AzureRM Provider Contributor Guides","text":"<p>First, thank you for your interest in contributing to the Azure Provider! And if you're unsure or anything, please do reach out for help. You can open a draft pull request (PR) or an issue with what you know or join the Slack Workspace for Contributors (Request Invite) and we'll do our best to guide you in the right direction.</p> <p>Note: this documentation is a work-in-progress - if you see something that's not quite right or missing, we'd really appreciate a PR!</p> <p>This contribution guide assumes you have at least a basic understanding of both Go and Terraform itself (for example you know what a Data Source and a Resource are) - more information on those can be found in the Terraform documentation.</p> <p>The AzureRM Provider is a Plugin which is invoked by Terraform (Core) and comprised of Data Sources and Resources.</p> <p>Within the AzureRM Provider, these Data Sources and Resources are grouped into Service Packages - which are logical groupings of Data Sources/Resources based on the Azure Service they're related to.</p> <p>Each of these Data Sources and Resources has both Acceptance Tests and Documentation associated with each Data Source/Resource - the Acceptance Tests are also located within this Service Package, however the Documentation exists within a dedicated folder.</p> <p>More granular documentation covers how these fit together - and the most common types of contribution we see:</p>"},{"location":"#topics","title":"Topics","text":"<p>Basics:</p> <ul> <li>Overview of the Provider</li> <li>Building the Provider</li> <li>Debugging the Provider</li> <li>Running the Tests</li> <li>Opening a Pull Request</li> </ul> <p>Common Topics/Guides:</p> <ul> <li>Adding a new Feature</li> <li>Adding a new Service Package</li> <li>Adding a new Data Source</li> <li>Adding a new Resource</li> <li>Adding fields to an existing Data Source</li> <li>Adding fields to an existing Resource</li> <li>Adding State Migrations</li> <li>Adding Write-Only Attributes</li> <li>Breaking Changes and Deprecations</li> <li>When to create a new Resource vs Inline Block</li> </ul> <p>References:</p> <ul> <li>Acceptance Testing</li> <li>Best Practices</li> <li>Glossary</li> <li>Naming</li> <li>Provider Documentation Standards</li> <li>Resource IDs</li> <li>Schema Design</li> <li>Working with Errors</li> </ul> <p>Maintainer specific:</p> <ul> <li>Merging Prs</li> </ul> <p>FAQ:</p> <ul> <li>Frequently Asked Questions</li> </ul>"},{"location":"topics/best-practices/","title":"Best Practices","text":"<p>Since its inception, the provider has undergone various iterations and changes in convention, as a result there can be legacy by-products within the provider which are inadvertently used as references. This section contains a miscellaneous assortment of current best practices to be aware of when contributing to the provider.</p>"},{"location":"topics/best-practices/#separate-create-and-update-methods","title":"Separate Create and Update Methods","text":"<p>Historically the Provider has opted to combine the Create and Update methods due to the behaviour of the Azure API, where the same API is used for both Create and Update, meaning that the same payload has to be sent during both the Creation and Update of the resource.</p> <p>In order to properly support Terraform's <code>ignore_changes</code> feature, rather than using a combined method for Create and Update, we're now requiring that these be separate, and that in the Update partial/delta differences are performed, to only update the value for a field if it's marked as changed.</p> <p>For example, whilst a Create method may look similar to below:</p> <pre><code>payload := resources.Group{\nLocation: location.Normalize(d.Get(\"location\").(string)),\nTags: tags.Expand(d.Get(\"tags\").(map[string]interface{})),\n}\n\nif err := client.CreateThenPoll(ctx, id, payload); err != nil {\nreturn fmt.Errorf(\"creating %s: %+v\", id, err)\n}\n</code></pre> <p>The update method should be checking if the updatable fields (in this example, only tags) - have changes (using <code>d.HasChanges</code> - which will flag updated values in the config if they're not ignored via <code>ignore_changes</code>).</p> <p>Depending on the API there are two types of Updates, a patch/delta update (where only the fields containing changes are sent) - and a full update (which requires sending the full payload) - these are differentiable via the method name in the SDK, patch/delta updates are generally called <code>Update</code>, with a full update being called <code>CreateOrUpdate</code>.</p> <p>A patch/delta update would look similar to below:</p> <pre><code>payload := resources.GroupUpdate{}\nif d.HasChanges(\"tags\") {\n// this uses `pointer.To` since all fields are optional in a patch/delta update, so they'll only be updated if specified\npayload.Tags = pointer.To(tags.Expand(d.Get(\"tags\").(map[string]interface{})))\n}\n\nif err := client.UpdateThenPoll(ctx, id, payload); err != nil {\nreturn fmt.Errorf(\"updating %s: %+v\", id, err)\n}\n</code></pre> <p>A full update would retrieve the existing object from the API and then patch it, for example:</p> <pre><code>resp, err := client.Get(ctx, id)\nif err != nil {\nreturn fmt.Errorf(\"retrieving %s: %+v\", id, err)\n}\n\nif resp.Model == nil {\nreturn fmt.Errorf(\"retrieving %s: model was nil\", id)\n}\n\npayload := *resp.Model\nif d.HasChanges(\"tags\") {\npayload.Tags = tags.Expand(d.Get(\"tags\").(map[string]interface{}))\n}\n\nif err := client.UpdateThenPoll(ctx, id, payload); err != nil {\nreturn fmt.Errorf(\"updating %s: %+v\", id, err)\n}\n</code></pre>"},{"location":"topics/best-practices/#typed-vs-untyped-resources","title":"Typed vs. Untyped Resources","text":"<p>At this point in time the Provider supports Data Sources and Resources built using either the Typed SDK, or <code>hashicorp/terraform-plugin-sdk</code> (which we call <code>Untyped</code>). Whilst both of these output Terraform Data Sources and Resources, we're gradually moving from using Untyped Data Sources and Resources to Typed Resources since there's a number of advantages in doing so. We currently recommend using the internal sdk package to build Typed Resources.</p> <p>An example of both Typed and Untyped Resources can be found below - however as a general rule:</p> <ul> <li>When the Resource imports <code>\"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"</code> - it's using the Typed SDK.</li> <li>When the Resource doesn't import <code>\"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"</code> - then it's an Untyped Resource, which is backed by <code>hashicorp/terraform-plugin-sdk</code>.</li> </ul> <p>Data Sources and Resources built using the Typed SDK have a number of benefits over those using <code>hashicorp/terraform-plugin-sdk</code> directly:</p> <ul> <li>The Typed SDK requires that a number of Azure specific behaviours are present in each Data Source/Resource. For example, the <code>interface</code> defining the Typed SDK includes an <code>IDValidationFunc()</code> function, which is used during <code>terraform import</code> to ensure the Resource ID being specified matches what we're expecting. Whilst this is possible using the Untyped SDK, it's more work to do so, as such using the Typed SDK ensures that these behaviours become common across the provider.</li> <li>The Typed SDK exposes an <code>Encode()</code> and <code>Decode()</code> method, allowing the marshalling/unmarshalling of the Terraform Configuration into a Go Object - which both:<ol> <li>Avoids logic errors when an incorrect key is used in <code>d.Get</code> and <code>d.Set</code>, since we can validate that each of the HCL keys used for the models (to get and set these from the Terraform Config) is present within the Schema via a unit test, rather than failing during the <code>Read</code> function, which takes considerably longer.</li> <li>Default values can be implied for fields, rather than requiring an explicit <code>d.Set</code> in the Read function for every field - this allows us to ensure that an empty value/list is set for a field, rather than being <code>null</code> and thus not able to be referenced in user configs.</li> </ol> </li> <li>Using the Typed SDK allows Data Sources and Resources to (in the future) be migrated across to using <code>hashicorp/terraform-plugin-framework</code> rather than <code>hashicorp/terraform-plugin-sdk</code> without rewriting the resource - which will unlock a number of benefits to end-users, but does involve some configuration changes (and as such will need to be done in a major release).</li> <li>Using the Typed SDK means that these Data Sources/Resources can be more easily swapped out for generated versions down the line (since the code changes will be far smaller).</li> </ul> <p>To facilitate the migration across to Typed Resources, we ask that any new Data Source or Resource which is added to the Provider is added as a Typed Data Source/Resource. Enhancements to existing Data Sources/Resources which are Untyped Resources can remain as Untyped Resources, however these will need to be migrated across in the future.</p> <p>Here is an example of an Untyped Resource:</p> <pre><code>package someservice\n\nimport ...\n\nfunc someResource() *pluginsdk.Resource {\nreturn &amp;pluginsdk.Resource{\nCreate: someResourceCreate,\nRead:   someResourceRead,\nUpdate: someResourceUpdate,\nDelete: someResourceDelete,\n\nImporter: pluginsdk.ImporterValidatingResourceId(func(id string) error {\n_, err := someresource.ParseSomeResourceID(id)\nreturn err\n}),\n\nTimeouts: &amp;pluginsdk.ResourceTimeout{\nCreate: pluginsdk.DefaultTimeout(30 * time.Minute),\nRead:   pluginsdk.DefaultTimeout(5 * time.Minute),\nUpdate: pluginsdk.DefaultTimeout(30 * time.Minute),\nDelete: pluginsdk.DefaultTimeout(30 * time.Minute),\n},\n\nSchema: map[string]*pluginsdk.Schema{\n// schema fields are defined here\n},\n}\n}\n\nfunc someResourceCreate(d *pluginsdk.ResourceData, meta interface{}) error {\n// create logic is defined here\n}\n\nfunc someResourceUpdate(d *pluginsdk.ResourceData, meta interface{}) error {\n// update logic is defined here\n}\n\nfunc someResourceRead(d *pluginsdk.ResourceData, meta interface{}) error {\n// read logic is defined here\n}\n\nfunc someResourceDelete(d *pluginsdk.ResourceData, meta interface{}) error {\n// delete logic is defined here\n}\n</code></pre> <p>Typed resources are initialised using interfaces and methods from the <code>sdk</code> package within the provider and will look something like the example below:</p> <pre><code>package someservice\n\nimport ...\n\ntype SomeResource struct{}\n\nvar _ sdk.ResourceWithUpdate = SomeResource{}\n\ntype SomeResourceModel struct {\nDisplayName           string            `tfschema:\"display_name\"`\nResourceGroup         string            `tfschema:\"resource_group_name\"`\nSku                   string            `tfschema:\"sku_name\"`\nTags                  map[string]string `tfschema:\"tags\"`\nTenantId              string            `tfschema:\"tenant_id\"`\n}\n\nfunc (r SomeResource) ResourceType() string {\nreturn \"azurerm_some_resource\"\n}\n\nfunc (r SomeResource) ModelObject() interface{} {\nreturn &amp;SomeResourceModel{}\n}\n\nfunc (r SomeResource) IDValidationFunc() pluginsdk.SchemaValidateFunc {\nreturn someService.ValidateSomeResourceID\n}\n\nfunc (r SomeResource) Arguments() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n// settable schema fields are set here\n}\n}\n\nfunc (r SomeResource) Attributes() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n// read-only schema fields are set here\n}\n}\n\nfunc (r SomeResource) Create() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nTimeout: 30 * time.Minute,\nFunc:    func(ctx context.Context, metadata sdk.ResourceMetaData) error {\n// create logic is defined here \n},\n}\n}\n\nfunc (r SomeResource) Update() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nTimeout: 30 * time.Minute,\nFunc:    func(ctx context.Context, metadata sdk.ResourceMetaData) error {\n// update logic is defined here\n},\n}\n}\n\nfunc (r SomeResource) Read() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nTimeout: 5 * time.Minute,\nFunc:    func(ctx context.Context, metadata sdk.ResourceMetaData) error {\n// read logic is defined here\n},\n}\n}\n\nfunc (r SomeResource) Delete() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nTimeout: 5 * time.Minute,\nFunc:    func(ctx context.Context, metadata sdk.ResourceMetaData) error {\n// delete logic is defined here\n},\n}\n}\n</code></pre>"},{"location":"topics/best-practices/#setting-properties-to-optional-computed","title":"Setting Properties to Optional + Computed","text":"<p>There are many APIs within Azure that will specify a default value for a field if one isn't specified, for example the <code>createMode</code> field is typically defaulted (server-side) to <code>Default</code>.</p> <p>The Azure Provider currently makes use of <code>hashicorp/terraform-plugin-sdk@v2</code> to define Data Sources and Resources, which under the hood uses v5 of the Terraform Protocol to interact with Terraform Core.</p> <p>In version 5 of the Terraform Protocol, if a field is created with one value at Create time and returns a different value immediately after creation, then an internal warning is logged (but no error is raised) - meaning that the only way this change is visible is through a diff when <code>terraform plan</code> is run. The next version of the Terraform Protocol (v6 - used by <code>hashicorp/terraform-plugin-framework</code>) changes this from a logged warning to an error at runtime - meaning that these diffs will become more visible to users (and need to be accounted for in the provider).</p> <p>To work around situations where we need to expose the default value from the Azure API - we've historically marked fields as both <code>Optional</code> and <code>Computed</code> - meaning that a value will be returned from the API when it's not defined.</p> <p>Whilst this works, a side effect is that it's hard for users to reset a field to its default value when this is done - as such some fields today (such as the subnets block within the azurerm_virtual_network resource) require that an explicit empty list is specified (for example <code>subnets = []</code>) to remove this value, where this field is <code>Optional</code> and <code>Computed</code>.</p> <p>Due to some of the issues surrounding <code>Optional</code> + <code>Computed</code> properties, avoid this usage where other options exist, e.g. specifying a <code>Default</code> if Azure consistently sets the same value. However, if no other options exist, we should mark the property <code>Optional</code> + <code>Computed</code> in favour of having users specify <code>ignore_changes</code>.</p> <p>If you encounter a field that must be <code>Optional</code> and <code>Computed</code>, make sure it follows the following conventions: * The properties are in this sequence: Optional, Explanatory Comment, Computed * The comment should start with <code>// NOTE: O+C</code>, and then explain the reason for the field being <code>Optional</code> and <code>Computed</code></p> <p>Example:</p> <pre><code>    \"etag\": {\nType: pluginsdk.TypeString,\nOptional: true,\n// NOTE: O+C Azure generates a new value every time this resource is updated\nComputed: true,\n},\n</code></pre>"},{"location":"topics/building-the-provider/","title":"Building the Provider","text":"<p>See DEVELOPER.md.</p>"},{"location":"topics/debugging-the-provider/","title":"Debugging the Provider","text":"<p>The provider can be debugged in a number of ways:</p> <ul> <li>Adding Log Messages</li> <li>Proxying Traffic</li> <li>Attaching a Debugger</li> </ul>"},{"location":"topics/debugging-the-provider/#logs","title":"Logs","text":"<p>Adding logging is the most basic, and simplest of ways to debug the provider. Log messages can be added with logging statements such as:</p> <pre><code>// info message\nid, err := parse.SomeResourceId(d.Id())\nif err != nil {\nreturn err\n}\nlog.Printf(\"[INFO] %s was not found - removing from state\", *id)\n\n// debug message\nlog.Printf(\"[DEBUG] Importing Resource - parsing %q\", d.Id())\n</code></pre> <p>Note: When logging, lean on the Resource ID Struct (returned from the Parse method above - as shown in the 'info' example above) rather than outputting the Raw Resource ID value (as shown in the debug example above)</p> <p>These can be viewed by running Terraform (or the Acceptance Test) with logging enabled:</p> <pre><code>$ TF_LOG=INFO terraform apply\n$ TF_LOG=DEBUG make acctests SERVICE='&lt;service&gt;' TESTARGS='-run=&lt;nameOfTheTest&gt;' TESTTIMEOUT='60m'\n</code></pre> <p>For more information see the official Terraform plugin logging documentation.</p>"},{"location":"topics/debugging-the-provider/#proxy","title":"Proxy","text":"<p>A useful step between logging and actual debugging is proxying the traffic through a web debugging proxy such as Charles Proxy (macOS) or Fiddler (Windows). These allow inspection of the web traffic between the provider and Azure to confirm what is actually going across the wire.</p> <p>You will need to enable HTTPS proxy support (usually by adding a certificate to your system) and then assuming the proxy is running on port <code>8888</code>:</p> <pre><code>$ http_proxy=http://localhost:8888 https_proxy=http://localhost:8888 terraform apply\n$ http_proxy=http://localhost:8888 https_proxy=http://localhost:8888 make acctests SERVICE='&lt;service&gt;' TESTARGS='-run=&lt;nameOfTheTest&gt;' TESTTIMEOUT='60m' </code></pre>"},{"location":"topics/debugging-the-provider/#debugger-delve","title":"Debugger (delve)","text":"<p>And finally the most advanced and powerful debugging tool is attaching a debugger such as delve to the provider whilst it is running.</p> <p>We generally recommend using Goland as it provides (amongst other features) native integrations for debugging - see OpenCredo's blog post for an example - however it's also possible to use VSCode and the delve CLI - configuring these is outside of the scope of this project.</p>"},{"location":"topics/frequently-asked-questions/","title":"Frequently Asked Questions","text":"<p>Note: This is a work-in-progress and will be extended over time.</p>"},{"location":"topics/frequently-asked-questions/#how-can-i-help","title":"How can I help?","text":"<p>Great question, we assign labels to each GitHub issue to try and group them, a number of these are relevant for users looking to contribute:</p> <ul> <li><code>good-first-issue</code> - this label is used to indicate that we think this would make a good issue for users looking to start contributing to the Provider. These are generally small enhancements, such as adding a new field to an existing resource - or documentation changes - and where we're adding this (in more recent issues) we're trying to provide a little context in one of the comments.</li> <li><code>help-wanted</code> - we use this to highlight enhancement issues that are possible and will have a great impact, but that the maintainers are unlikely to reach in the near future.</li> </ul> <p>The Contributor Readme contains guides on the most common contribution types we see, but if you have any questions not answered in this documentation, please reach out (either in our community slack, or by opening an issue - details can be found in the contributor readme).</p>"},{"location":"topics/frequently-asked-questions/#how-often-is-the-provider-released","title":"How often is the Provider released?","text":"<p>The estimated dates for each release of the Provider can be found on the Milestones page.</p> <p>As a general rule the Provider is typically released weekly on a Thursday, however this can vary (for example during the winter holidays), as such we recommend checking the Milestones page for the most up to date information.</p>"},{"location":"topics/frequently-asked-questions/#my-pull-request-has-merge-conflicts-should-i-rebasemerge-from-the-main-branch","title":"My Pull Request has merge conflicts, should I rebase/merge from the <code>main</code> branch?","text":"<p>Whilst we do our best to review pull requests as they come in, unfortunately there are cases where it can take some time and merge conflicts can result if they have been sitting for a while. Generally speaking we recommend rebasing/merging from <code>main</code> only once a maintainer has taken a look through the PR and explicitly requested it.  </p>"},{"location":"topics/frequently-asked-questions/#once-a-major-release-is-published-will-new-features-and-fixes-be-backported-to-previous-versions","title":"Once a major release is published, will new features and fixes be backported to previous versions?","text":"<p>Generally new features and fixes will only be added to the most recent major version.</p> <p>Due to the high touch nature of provider development and the extensive regression testing required to ensure stability, maintaining multiple versions of the provider is not sustainable at this time. An exception to this could be a discovered security vulnerability for which backporting may be the most reasonable course of action. These will be reviewed on a case by case basis.</p>"},{"location":"topics/frequently-asked-questions/#what-do-the-different-github-labels-mean","title":"What do the different GitHub labels mean?","text":"<p>As a general rule the different Azure Services are represented as <code>service/{serviceName}</code> - for other labels we're working through adding descriptions which can be found on the GitHub Labels page for this repository.</p>"},{"location":"topics/frequently-asked-questions/#why-was-my-comment-marked-as-off-topic","title":"Why was my comment marked as off-topic?","text":"<p>Whilst we thank you for your feedback, we mark comments along the lines of \"me too\" / \"when will this be fixed?\" (or generally off-topic comments) as off-topic so that they're hidden by default.</p> <p>As this repository has a large/active community, we instead ask that you use a thumbs-up GitHub reaction to the original issue so that we can prioritise this work without notifying everybody subscribed to the repository.</p> <p>We appreciate this may be frustrating to have a comment marked as off-topic - when we've not done this we've noticed a number of users regularly adding \"+1\" / \"me too\" comments, which ends up causing more distractions for both the maintainers and community in general.</p>"},{"location":"topics/frequently-asked-questions/#why-did-you-close-my-question","title":"Why did you close my question?","text":"<p>Whilst we thank you for reaching out, unfortunately we're unable to assist with individual usage questions related to the Azure Provider.</p> <p>We've closed your issue because we believe it's an issue with the Terraform Configuration being used (or, that the credentials being used to interact with Azure may not have permission to the resources in question), rather than a bug in the Azure Provider.</p> <p>We instead ask that configuration issues/usage questions related to the Provider are opened on the Community Discuss forum so that we can keep this repository focused on bugs/feature enhancements related to the Azure Provider.</p>"},{"location":"topics/guide-breaking-changes/","title":"Guide: Breaking Changes and Deprecations","text":"<p>To keep up with and accommodate the changing pace of Azure, the provider needs to be able to gracefully introduce and handle breaking changes. A \"breaking change\" within the provider is considered to be anything that requires an end user to modify previously valid terraform configuration after a provider upgrade to either deploy new resources or to maintain existing deployments. Even if a change does not affect the user's current deployment, it is still considered a breaking change if it requires the user to modify their configuration to deploy new resources. </p> <p>The <code>azurerm</code> provider attempts to be as \"surface stable\" as possible during minor and patch releases meaning breaking changes are typically only made during major releases, however exceptions are sometimes made for minor releases when the breaking change is deemed necessary or is unavoidable. Terraform users rely on the stability of Terraform providers as not only can configuration changes be costly to make, test, and deploy they can also affect downstream tooling such as modules. Even as part of a major release, breaking changes that are overly large or have little benefit can delay users upgrading to the next major version.</p> <p>Generally we can safely introduce breaking changes into the provider for the major release using a feature flag. For the next major release that would be the <code>features.FivePointOh()</code> flag which is available in the provider today. This guide includes several topics on how to do common deprecations and breaking changes in the provider using this feature flag, as well as additional guidance on how to deal with changing default values in the Azure API. </p> <p>Types of breaking changes covered are:</p> <ul> <li>Removing Resources or Data Sources</li> <li>Breaking Schema Changes</li> <li>Updating Default Values</li> <li>Post Release Breaking Change Clean Up</li> </ul>"},{"location":"topics/guide-breaking-changes/#removing-resources-or-data-sources","title":"Removing Resources or Data Sources","text":"<p>Resources can be removed for several reasons, the service could be retiring, the API may no longer support creation of that resource or the resource has been renamed or superseded by a new version.</p> <p>In all cases the resources cannot be removed from the provider in a minor release but must be deprecated and the registration of the resource made conditional using the major release feature flag.</p> <p>The steps outlined below uses an example resource that is deprecated, but the same principles and steps apply for data sources as well.</p> <ol> <li>Add the appropriate deprecation message to the resource.</li> </ol> <p>For Typed Resources     <pre><code>// For resources that have no replacement\n\nvar _ sdk.ResourceWithDeprecationAndNoReplacement = ResourceWithNoReplacement{}\n\nfunc (r ResourceWithNoReplacement) DeprecationMessage() string {\nreturn \"The `azurerm_resource_with_no_replacement` resource has been deprecated and will be removed in v5.0 of the AzureRM Provider\"\n}\n\n\n// For resources that have a replacement\n\nvar _ sdk.ResourceWithDeprecationReplacedBy = ResourceWithReplacement{}\n\nfunc (r ResourceWithReplacement) DeprecatedInFavourOfResource() string {\nreturn \"azurerm_new_resource\"\n}\n</code></pre></p> <pre><code>For Untyped Resources\n```go\nfunc resourceExample() *pluginsdk.Resource {\n    return &amp;pluginsdk.Resource{\n        Create: resourceExampleCreate,\n        Read:   resourceExampleRead,\n        Update: resourceExampleUpdate,\n        Delete: resourceExampleDelete,\n\n        Timeouts: &amp;pluginsdk.ResourceTimeout{\n        Create: pluginsdk.DefaultTimeout(30 * time.Minute),\n        Read:   pluginsdk.DefaultTimeout(5 * time.Minute),\n        Update: pluginsdk.DefaultTimeout(30 * time.Minute),\n        Delete: pluginsdk.DefaultTimeout(30 * time.Minute),\n        },\n\n        DeprecationMessage: \"The `azurerm_example` resource has been deprecated and will be removed in v5.0 of the AzureRM Provider\"\n        ...\n    }\n}\n```\n</code></pre> <ol> <li>Conditionally register the resource in the <code>registration.go</code> file of the service package.</li> </ol> <p>For Typed Resources     <pre><code>func (r Registration) Resources() []sdk.Resource {\nresources := []sdk.Resource{\nMySqlFlexibleServerResource{},\n}\n\nif !features.FivePointOh() {\nresources = append(resources, ExampleResource{})\n}\n\nreturn resources\n}\n</code></pre></p> <pre><code>For Untyped Resources\n```go\nfunc (r Registration) SupportedResources() map[string]*pluginsdk.Resource {\n    resources := map[string]*pluginsdk.Resource{\n        \"azurerm_mysql_flexible_server\": resourceMysqlFlexibleServer(),\n\n    }\n\n    if !features.FivePointOh() {\n        resources[\"azurerm_example\"] = resourceExample()\n    }\n\n    return resources\n}\n```\n</code></pre> <ol> <li> <p>Skip all tests related to the deprecated resource.</p> <pre><code>func TestAccExample_basic(t *testing.T) {\nif features.FivePointOh() {\nt.Skipf(\"Skipping since `azurerm_example` is deprecated and will be removed in 5.0\")\n}\ndata := acceptance.BuildTestData(t, \"azurerm_example\", \"test\")\nr := ExampleResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{\nConfig: r.basic(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep()\n})\n}\n</code></pre> </li> <li> <p>Update the upgrade guide under <code>website/docs/5.0-upgrade-guide.markdown</code>.</p> </li> </ol> <pre><code>## Removed Resources\n\n### `azurerm_example`\n\nThis deprecated resources has been removed from the Azure Provider.\n</code></pre> <ol> <li>Update the resource (or data source) documentation</li> </ol> <pre><code>~&gt; **Note:** The `azurerm_example` resource has been deprecated because [reason here e.g. the service is retiring by 2025-10-10] and will be removed in v5.0 of the AzureRM Provider.\n</code></pre>"},{"location":"topics/guide-breaking-changes/#breaking-schema-changes-and-deprecations","title":"Breaking Schema Changes and Deprecations","text":"<p>Breaking schema changes can include: - Property renames - When properties become Required - When properties have Computed removed and need to be added to <code>ignore_changes</code> to prevent diffs - Changes to the validation e.g. the validation becomes more restrictive - Changing the default value - Changing the type</p> <p>In all cases the deprecation is handled the same way and will be illustrated by the example below.</p> <p>The following example follows a fictional resource that will have the following breaking changes made: - The property <code>enable_scaling</code> renamed to <code>scaling_enabled</code> - The property <code>version</code> has its default changed from <code>1</code> to <code>2</code></p> <ol> <li>Update the Schema with the target or desired breaking schema change and patch over the breaking schema change with the current behaviour using the major release feature flag.</li> </ol> <pre><code>func (r ExampleResource) Arguments() map[string]*pluginsdk.Schema{\nargs := map[string]*pluginsdk.Schema{\n\"scaling_enabled\": {\nType:     pluginsdk.TypeBool,\nOptional: true,\nDefault: false,\n},      \"version\": {\nType:     pluginsdk.TypeString,\nOptional: true,\nDefault: 2,\n},\n}\n\n// Regardless of the number of arguments changing, the whole schema definition should be updated like the following rather than inline changes for the current schema definition.\n// This is to make cleanup easy so we can delete this block when the next major version releases.\nif !features.FivePointOh() {\nargs[\"enable_scaling\"] = &amp;pluginsdk.Schema{\nType:          pluginsdk.TypeBool,\nOptional:      true,\nComputed:      true,\nConflictsWith: []string{\"scaling_enabled\"},\nDeprecated:    \"`enable_scaling` has been deprecated in favour of `scaling_enabled` and will be removed in v5.0 of the AzureRM Provider\",\n}\n// When renaming a property both properties need to have `Computed` set on them until the old property is removed in the next major release\n// We also need to remember to set ConflictsWith on both the old and the renamed property to ensure users don't set both in their config\nargs[\"scaling_enabled\"] = &amp;pluginsdk.Schema{\nType:          pluginsdk.TypeBool,\nOptional:      true,\nComputed:      true,\nConflictsWith: []string{\"enable_scaling\"},\n}\n\nargs[\"version\"].Default = 1\n}\n\nreturn args\n}\n</code></pre> <p>Note: In the past we've accepted in-lined anonymous functions in a property's schema definition to conditionally change the default value, validation function etc. these will no longer be accepted in the provider. This is a deliberate decision to reduce the variation in how deprecations are done in the provider and also simplifies the clean-up effort of feature flagged code after the major release.</p> <ol> <li>Update the Create/Read/Update methods.</li> </ol> <p>For Create function, you can do:  <pre><code>payload := example.Payload{\n// ...\nEnableScaling: pointer.To(model.ScalingEnabled),\n// ...\n}\n\nif !features.FivePointOh() {\nif !pluginsdk.IsExplicitlyNullInConfig(metadata.ResourceData, \"enable_scaling\") {\npayload.EnableScaling = pointer.To(model.EnableScaling);\n}\n}\n</code></pre></p> <ol> <li>Update the test configurations.</li> </ol> <p>Here are some guidelines on what good testing coverage for renamed properties looks like:    * All test configurations that reference the old property should be updated to use the renamed property    * One test configuration should continue using the old property to ensure that it still works as expected, but switch to using the renamed property in the major release mode. An example of what that looks like is provided below.</p> <pre><code>func (ExampleResource) complete(data acceptance.TestData) string {\nif !features.FivePointOh() {\nreturn fmt.Sprintf(`\nprovider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_resource_group\" \"test\" {\n  name     = \"acctestRG-example-%[1]d\"\n  location = \"%[2]s\"\n}\n\nresource \"azurerm_example\" \"test\" {\n  name           = \"acctestexample%[1]d\"\n  enable_scaling = true\n}\n`, data.RandomInteger, data.Locations.Primary)\n}\nreturn fmt.Sprintf(`\nprovider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_resource_group\" \"test\" {\n  name     = \"acctestRG-example-%[1]d\"\n  location = \"%[2]s\"\n}\n\nresource \"azurerm_example\" \"test\" {\n  name            = \"acctestexample%[1]d\"\n  scaling_enabled = true\n}\n`, data.RandomInteger, data.Locations.Primary)\n}\n</code></pre> <p>Note: Wherever possible, only update the test configuration and avoid updating the test case since changes to the test cases are more involved and higher effort to clean up.</p> <ol> <li>Update the upgrade guide under <code>website/docs/5.0-upgrade-guide.markdown</code></li> </ol> <p>Under the appropriate section of the upgrade guide, add a line for the deprecation    <pre><code>## Breaking changes in Resources\n\n### `azurerm_example_resource`\n\n* The deprecated `enable_scaling` property has been removed in favour of the `scaling_enabled` property.\n* The property `version` now defaults to `2`.\n</code></pre></p> <p>The resources/data sources should be added in alphabetical order.</p> <ol> <li> <p>Update the resource documentation</p> </li> <li> <p>The resource documentation should only be updated when a property is undergoing a soft deprecation. In the example above the only update to the resource documentation we need to do is to remove the property <code>enable_scaling</code> and add the property <code>scaling_enabled</code>.</p> </li> <li> <p>Breaking changes such as the default value changing, or other property behaviour changing in a way that will only be active when the major release has gone out should not be added to the documentation since these do not apply yet. Please do not add any <code>**Note:** This property will do x in 5.0</code> notes in the documentation. </p> </li> </ol>"},{"location":"topics/guide-breaking-changes/#updating-default-values","title":"Updating Default Values","text":"<p>There are some cases where Azure updates the default value for an attribute when creating a new resource, and we would want to do the same for the provider but this is an easy breaking change to miss.</p> <p>We have a property like the following and Azure added a new spark version <code>3.4</code> and said that all new resources being created will be created with <code>3.4</code> as the default. </p> <p>In Terraform, we start with:</p> <pre><code>    \"spark_version\": {\n        Type:     pluginsdk.TypeString,\n        Optional: true,\n        Default:  \"2.4\",\n        ValidateFunc: validation.StringInSlice([]string{\n            \"2.4\",\n            \"3.1\",\n            \"3.2\",\n            \"3.3\",\n        }, false),\n    },\n</code></pre> <p>Then we would want to update <code>ValidateFunc</code> to include the new accepted value and update <code>Default</code> to <code>3.4</code> to keep it in line with Azure like so:</p> <pre><code>    \"spark_version\": {\n        Type:     pluginsdk.TypeString,\n        Optional: true,\n        Default:  \"3.4\",\n        ValidateFunc: validation.StringInSlice([]string{\n            \"2.4\",\n            \"3.1\",\n            \"3.2\",\n            \"3.3\",\n            \"3.4\",\n        }, false),\n    },\n</code></pre> <p>But if we do that, people who have created that resource without the attribute specified will see a plan diff when upgrading to this version of the provider like so:</p> <p>This config does not specify <code>spark_version</code> because we know we can rely on the default to fill it for us</p> <pre><code>resource \"azurerm_synapse_spark_pool\" \"example\" {\n  name                 = \"example\"\n  synapse_workspace_id = azurerm_synapse_workspace.test.id\n  node_size_family     = \"MemoryOptimized\"\n  node_size            = \"Small\"\n  node_count           = 3\n}\n</code></pre> <p>Running <code>terraform show</code> we can see <code>spark_version</code> has been filled in with the default of <code>2.4</code></p> <pre><code># azurerm_synapse_spark_pool.example:\nresource \"azurerm_synapse_spark_pool\" \"example\" {\n    name                                = \"example\"\n.\n.\n.\n    spark_version                       = \"2.4\"\n}\n</code></pre> <p>When running the version of the provider where the default has changed from <code>2.4</code> to <code>3.4</code>, we'll see the following plan:</p> <pre><code>Terraform will perform the following actions:\n\n  # azurerm_synapse_spark_pool.example will be updated in-place\n  ~ resource \"azurerm_synapse_spark_pool\" \"test\" {\n        id                                  = \"exampleid\"\n        name                                = \"example\"\n      ~ spark_version                       = \"2.4\" -&gt; \"3.4\"\n        tags                                = {}\n        # (12 unchanged attributes hidden)\n    }\n\nPlan: 0 to add, 1 to change, 0 to destroy.\n</code></pre> <p>This is a breaking change as Terraform should not trigger a plan between minor version upgrades. Instead, what we can do is use the major release feature flag as shown in the example below or mark the field as Required if that default value is going to continue to change in the future:</p> <pre><code>func (r SparkResource) Arguments() map[string]*pluginsdk.Schema{\nargs := map[string]*pluginsdk.Schema{\n\"spark_version\": {\nType:     pluginsdk.TypeString,\nOptional: true,\nDefault: \"3.4\",\nValidateFunc: validation.StringInSlice([]string{\n\"2.4\",\n\"3.1\",\n\"3.2\",\n\"3.3\",\n\"3.4\",\n}, false),\n},\n}\n\nif !features.FivePointOh() {\nargs[\"spark_version\"].Default = \"2.4\"\n}\n\nreturn args\n}\n</code></pre>"},{"location":"topics/guide-breaking-changes/#adding-a-new-property-with-a-default-value","title":"Adding a new property with a default value","text":"<p>When adding a new property with a default value, we can introduce a similar breaking change as the one noted above, but it's even harder to pinpoint. Take for example the following property recently added to <code>azurerm_kusto_account</code>:</p> <p>It originally came in like this:</p> <pre><code>\"auto_stop_enabled\": {\n    Type:     pluginsdk.TypeBool,\n    Optional: true,\n},\n</code></pre> <p>Our tests were failing because the Azure API was returning this value as true while Terraform does not expect this value to be set because it isn't specified in the config file. To fix this breaking change, we need to add a Default like so:</p> <pre><code>\"auto_stop_enabled\": {\n    Type:     pluginsdk.TypeBool,\n    Optional: true,\n    Default:  true,\n},\n</code></pre> <p>There are many ways to accidentally add a breaking change when looking at properties with a Default or lack thereof so extra work needs to be done to confirm what Terraform and the Azure API are returning before deciding how best to incorporate the Default tag.</p>"},{"location":"topics/guide-breaking-changes/#post-release-breaking-change-clean-up","title":"Post Release Breaking Change Clean Up","text":"<p>Once the next major release has happened, all blocks of code that were conditionally included for that version (e.g. <code>if !features.FivePointOh() { ... }</code>) need to be removed. Most should be fine to simply remove, however there are a few things to watch out for:</p> <ol> <li>For typed resources, if you are removing a property, make sure you also remove it from the model(s). The fields should have a <code>removedInNextMajorVersion</code> tag. </li> <li>For typed resources, there may be properties that were only included once the major version was released, make sure you remove the <code>addedInNextMajorVersion</code> tag from these properties in the model(s).</li> <li>Confirm the documentation is up-to-date with what is in code, generally this should already be the case, but it's good to double-check.</li> </ol>"},{"location":"topics/guide-new-data-source/","title":"Guide: New Data Source","text":"<p>This guide covers adding a new Data Source to a Service Package, see adding a New Service Package if the Service Package doesn't exist yet.</p>"},{"location":"topics/guide-new-data-source/#related-topics","title":"Related Topics","text":"<ul> <li>Acceptance Testing</li> <li>Our Recommendations for opening a Pull Request</li> </ul>"},{"location":"topics/guide-new-data-source/#stages","title":"Stages","text":"<p>At this point in time the AzureRM Provider supports both Typed and Untyped Data Sources - more information can be found in the High Level Overview.</p> <p>This guide covers adding a new Typed Data Source, which makes use of the Typed SDK within this repository and requires the following steps:</p> <ol> <li>Ensure all the dependencies are installed (see Building the Provider).</li> <li>Add an SDK Client (if required).</li> <li>Define the Resource ID.</li> <li>Scaffold an empty/new Data Source.</li> <li>Register the new Data Source.</li> <li>Add Acceptance Test(s) for this Data Source.</li> <li>Run the Acceptance Test(s).</li> <li>Add Documentation for this Data Source.</li> <li>Send the Pull Request.</li> </ol> <p>We'll go through each of those steps in turn, presuming that we're creating a Data Source for a Resource Group.</p>"},{"location":"topics/guide-new-data-source/#step-1-ensure-the-tools-are-installed","title":"Step 1: Ensure the Tools are installed","text":"<p>See Building the Provider.</p>"},{"location":"topics/guide-new-data-source/#step-2-add-an-sdk-client-if-required","title":"Step 2: Add an SDK Client (if required)","text":"<p>If you're creating a new Data Source for a Resource that's already created by Terraform, the SDK Client you need to use is likely already supported (and so you can skip this section).</p> <p>However if the SDK Client you need to use isn't already configured in the Provider, we'll cover how to add and configure the SDK Client.</p> <p>Determining which SDK Client you should be using is a little complicated unfortunately, in this case the SDK Client we want to use is: <code>github.com/Azure/azure-sdk-for-go/services/resources/mgmt/2020-06-01/resources</code>.</p> <p>The Client for the Service Package can be found in <code>./internal/services/{name}/client/client.go</code> - and we can add an instance of the SDK Client we want to use (here <code>resources.GroupsClient</code>) and configure it (adding credentials etc): </p> <pre><code>package client\n\nimport (\n\"github.com/hashicorp/go-azure-sdk/resource-manager/resources/2022-09-01/resources\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/common\"\n)\n\ntype Client struct {\nGroupsClient *resources.GroupsClient\n}\n\nfunc NewClient(o *common.ClientOptions) (*Client, error) {\ngroupsClient, err := resources.NewResourcesClientWithBaseURI(o.Environment.ResourceManager)\nif err != nil {\nreturn nil, fmt.Errorf(\"building Resources Client: %+v\", err)\n}\no.Configure(groupsClient.Client, o.Authorizer.ResourceManager)\n\n// ...\n\nreturn &amp;Client{\nGroupsClient: groupsClient,\n}\n}\n</code></pre> <p>Things worth noting here:</p> <ul> <li>The call to <code>o.Configure</code> configures the authorization token which should be used for this SDK Client - in most cases <code>ResourceManager</code> is the authorizer you want to use.</li> </ul> <p>At this point, this SDK Client should be usable within the Data Sources via:</p> <pre><code>client := metadata.Client.{ServicePackage}.{ClientField}\n</code></pre> <p>For example, in this case:</p> <pre><code>client := metadata.Client.Resource.GroupsClient\n</code></pre>"},{"location":"topics/guide-new-data-source/#step-3-scaffold-an-emptynew-data-source","title":"Step 3: Scaffold an empty/new Data Source","text":"<p>Since we're creating a Data Source for a Resource Group, which is a part of the Resources API - we'll want to create an empty Go file within the Service Package for Resources, which is located at <code>./internal/services/resources</code>.</p> <p>In this case, this would be a file called <code>resource_group_example_data_source.go</code>, which we'll start out with the following:</p> <p>Note: We'd normally name this file <code>resource_group_data_source.go</code> - but there's an existing Data Source for Resource Groups, so we're appending <code>example</code> to the name throughout this guide. </p> <pre><code>package resources\n\nimport \"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"\n\nvar _ sdk.DataSource = ResourceGroupExampleDataSource{}\n\ntype ResourceGroupExampleDataSource struct {}\n</code></pre> <p>Note: Your editor may show a suggestion to implement the methods defined in <code>sdk.DataSource</code> for the <code>ResourceGroupExampleDataSource</code> struct - we'd recommend holding off the first time around to explain each of the methods.</p> <p>In this case the interface <code>sdk.DataSource</code> defines all of the methods required for a Data Source which the newly created struct for the Resource Group Data Source need to implement, which are:</p> <pre><code>type DataSource interface {\nArguments() map[string]*schema.Schema\nAttributes() map[string]*schema.Schema\nModelObject() interface{}\nResourceType() string\nRead() ResourceFunc\n}\n</code></pre> <p>To go through these in turn:</p> <ul> <li><code>Arguments</code> returns a list of schema fields which are user-specifiable - either Required or Optional.</li> <li><code>Attributes</code> returns a list of schema fields which are Computed (read-only).</li> <li><code>ModelObject</code> returns a reference to a Go struct which is used as the Model for this Data Source.</li> <li><code>ResourceType</code> returns the name of this resource within the Provider (for example <code>azurerm_resource_group_example</code>).</li> <li><code>Read</code> returns a function defining both the Timeout and the Read function (which retrieves information from the Azure API) for this Data Source.</li> </ul> <pre><code>type ResourceGroupExampleDataSourceModel struct {\nName     string            `tfschema:\"name\"`\nLocation string            `tfschema:\"location\"`\nTags     map[string]string `tfschema:\"tags\"`\n}\n\nfunc (ResourceGroupExampleDataSource) Arguments() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n\"name\": {\nType:         pluginsdk.TypeString,\nRequired:     true,\nValidateFunc: validation.StringIsNotEmpty,\n},\n}\n}\n\nfunc (ResourceGroupExampleDataSource) Attributes() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n\"location\": commonschema.LocationComputed(),\n\n\"tags\": commonschema.TagsDataSource(),\n}\n}\n\nfunc (ResourceGroupExampleDataSource) ModelObject() interface{} {\nreturn &amp;ResourceGroupExampleDataSourceModel{}\n}\n\nfunc (ResourceGroupExampleDataSource) ResourceType() string {\nreturn \"azurerm_resource_group_example\"\n}\n</code></pre> <p>In this case we're using the resource type <code>azurerm_resource_group_example</code> as an existing Data Source for <code>azurerm_resource_group</code> exists and the names need to be unique.</p> <p>These functions define a Data Source called <code>azurerm_resource_group_example</code>, which has one Required argument called <code>name</code> and two Computed arguments called <code>location</code> and <code>tags</code>.</p> <p>Schema fields should be ordered as follows:</p> <ol> <li>Any fields that make up the resource's ID, with the last user specified segment (usually the resource's name) first. (e.g. <code>name</code>, <code>resource_group_name</code>, or <code>name</code>, <code>parent_resource_id</code>)</li> <li>The <code>location</code> field.</li> <li>Required fields, sorted alphabetically.</li> <li>Optional fields, sorted alphabetically.</li> <li>Computed fields, sorted alphabetically. (Although in a typed data source these are always added within the <code>Attributes</code> method)</li> </ol> <p>Next up, let's implement the Read function - which retrieves the information about the Resource Group from Azure:</p> <pre><code>func (ResourceGroupExampleDataSource) Read() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\n\n// the Timeout is how long Terraform should wait for this function to run before returning an error\n// whilst 5 minutes may initially seem excessive, we set this as a default to account for rate\n// limiting - but having this here means that users can override this in their config as necessary\nTimeout: 5 * time.Minute,\n\n// the Func returns a function which retrieves the current state of the Resource Group into the state \nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\nclient := metadata.Client.Resource.GroupsClient\n\n// retrieve the Name for this Resource Group from the Terraform Config\n// and then create a Resource ID for this Resource Group\n// using the Subscription ID &amp; name \nsubscriptionId := metadata.Client.Account.SubscriptionId\n\n// declare a variable called state which we use to decode and encode values into\n// this simultaneously gets values that have been set in the config for us\n// and also allows us to set values into state\nvar state ResourceGroupExampleDataSourceModel\nif err := metadata.Decode(&amp;state); err != nil {\nreturn fmt.Errorf(\"decoding: %+v\", err)\n}   id := resources.NewResourceGroupExampleID(subscriptionId, state.Name)\n\n// then retrieve the Resource Group by its ID \nresp, err := client.Get(ctx, id)\nif err != nil {\n// if the Resource Group doesn't exist (e.g. we get a 404 Not Found)\n// since this is a Data Source we must return an error if it's Not Found \nif response.WasNotFound(resp.HttpResponse) {\nreturn fmt.Errorf(\"%s was not found\", id)\n}\n\n// otherwise it's a genuine error (auth/api error etc) so raise it\n// there should be enough context for the user to interpret the error\n// or raise a bug report if there's something we should handle \nreturn fmt.Errorf(\"retrieving %s: %+v\", id, err)\n}\n\n// now we know the Resource Group exists, set the Resource ID for this Data Source\n// this means that Terraform will track this as existing \nmetadata.SetID(id)\n\n// at this point we can set information about this Resource Group into the State\n// whilst traditionally we would do this via `metadata.ResourceData.Set(\"foo\", \"somevalue\")\n// the Location and Tags fields are a little different - and we have a couple of normalization\n// functions for these.\n\n// whilst this may seem like a weird thing to call out in an example, because these two fields\n// are present on the majority of resources, we hope it explains why they're a little different\n\n// in this case the Location can be returned in various different forms, for example\n// \"West Europe\", \"WestEurope\" or \"westeurope\" - as such we normalize these into a\n// lower-cased singular word with no spaces (e.g. \"westeurope\") so this is consistent\n// for users\nif model := resp.Model; model != nil {\nstate.Location = location.NormalizeNilable(model.Location)\nstate.Tags = pointer.From(model.Tags)\nprops := model.Properties; props != nil {\n// If the data source exposes additional properties that live within the Properties\n// model of the response they would be set into state here. \n}\n}   return metadata.Encode(&amp;state)\n},\n}\n}\n</code></pre> <p>At this point the finished Data Source should look like (including imports):</p> <pre><code>package resource\n\nimport (\n\"context\"\n\"fmt\"\n\"time\"\n\n\"github.com/hashicorp/go-azure-helpers/resourcemanager/commonschema\"\n\"github.com/hashicorp/go-azure-helpers/resourcemanager/location\"\n\"github.com/hashicorp/go-azure-helpers/resourcemanager/tags\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/tf/pluginsdk\"\n)\n\ntype ResourceGroupExampleDataSource struct{}\n\ntype ResourceGroupExampleDataSourceModel struct {\nName     string            `tfschema:\"name\"`\nLocation string            `tfschema:\"location\"`\nTags     map[string]string `tfschema:\"tags\"`\n}\n\n\nfunc (d ResourceGroupExampleDataSource) Arguments() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n\"name\": {\nType:         pluginsdk.TypeString,\nRequired:     true,\nValidateFunc: validation.StringIsNotEmpty,\n},\n}\n}\n\nfunc (d ResourceGroupExampleDataSource) Attributes() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n\"location\": commonschema.LocationComputed(),\n\n\"tags\": commonschema.TagsDataSource(),\n}\n}\n\nfunc (d ResourceGroupExampleDataSource) ModelObject() interface{} {\nreturn nil\n}\n\nfunc (d ResourceGroupExampleDataSource) ResourceType() string {\nreturn \"azurerm_resource_group_example\"\n}\n\nfunc (d ResourceGroupExampleDataSource) Read() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nTimeout: 5 * time.Minute,\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\nclient := metadata.Client.Resource.GroupsClient\nsubscriptionId := metadata.Client.Account.SubscriptionId\n\nvar state ResourceGroupExampleDataSourceModel\nif err := metadata.Decode(&amp;state); err != nil {\nreturn fmt.Errorf(\"decoding: %+v\", err)\n}\n\nid := resources.NewResourceGroupExampleID(subscriptionId, state.Name)\n\nresp, err := client.Get(ctx, id)\nif err != nil {\nif response.WasNotFound(resp.HttpResponse) {\nreturn fmt.Errorf(\"%s was not found\", id)\n}\nreturn fmt.Errorf(\"retrieving %s: %+v\", id, err)\n}\n\nmetadata.SetID(id)\n\nif model := resp.Model; model != nil {\nstate.Location = location.NormalizeNilable(model.Location)\nstate.Tags = pointer.From(model.Tags)\n}\nreturn metadata.Encode(&amp;state)\n},\n}\n}\n</code></pre> <p>At this point in time this Data Source is now code-complete - there's an optional extension to make this cleaner by using a Typed Model, however this isn't necessary.</p>"},{"location":"topics/guide-new-data-source/#step-4-register-the-new-data-source","title":"Step 4: Register the new Data Source","text":"<p>Data Sources are registered within the <code>registration.go</code> within each Service Package - and should look something like this:</p> <pre><code>package resource\n\nimport \"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"\n\nvar _ sdk.TypedServiceRegistration = Registration{}\n\ntype Registration struct{}\n\n// ...\n\n// DataSources returns a list of Data Sources supported by this Service\nfunc (Registration) DataSources() []sdk.DataSource {\nreturn []sdk.DataSource{}\n}\n</code></pre> <p>Note: It's possible that the Service Registration (above) doesn't currently support Typed Resources, in which case you may need to add the following:</p> <pre><code>var _ sdk.TypedServiceRegistration = Registration{}\n\ntype Registration struct {\n}\n\nfunc (Registration) Name() string {\nreturn \"Some Service\"\n}\n\nfunc (Registration) DataSources() []sdk.DataSource {\nreturn []sdk.DataSource{}\n}\n\nfunc (Registration) Resources() []sdk.Resource {\nreturn []sdk.Resource{}\n}\n\nfunc (Registration) WebsiteCategories() []string {\nreturn []string{\n\"Some Service\",\n}\n}\n</code></pre> <p>In this case you'll also need to add a line to register this Service Registration in the list of Typed Service Registrations.</p> <p>To register the Data Source we need to add an instance of the struct used for the Data Source to the list of Data Sources, for example:</p> <pre><code>// DataSources returns a list of Data Sources supported by this Service\nfunc (Registration) DataSources() []sdk.DataSource {\nreturn []sdk.DataSource{\nResourceGroupExampleDataSource{},   }\n}\n</code></pre> <p>At this point the Data Source is registered, as when the Azure Provider builds up a list of supported Data Sources during initialization, it parses each of the Service Registrations to put together a definitive list of the Data Sources that we support.</p> <p>This means that if you Build the Provider, at this point you should be able to apply the following Data Source:</p> <pre><code>provider \"azurerm\" {\n  features {}\n}\n\ndata \"azurerm_resource_group_example\" \"test\" {\n  name = \"some-pre-existing-resource-group\" # presuming this resource group exists ;)\n}\n\noutput \"location\" {\n  value = data.azurerm_resource_group_example.test.location\n}\n</code></pre>"},{"location":"topics/guide-new-data-source/#step-5-add-acceptance-tests-for-this-data-source","title":"Step 5: Add Acceptance Test(s) for this Data Source","text":"<p>We're going to test the Data Source that we've just built by dynamically provisioning a Resource Group using the Azure Provider, then asserting that we can look up that Resource Group using the new <code>azurerm_resource_group_example</code> Data Source.</p> <p>In Go tests are expected to be in a file name in the format <code>{original_file_name}_test.go</code> - in our case that'd be <code>resource_group_example_data_source_test.go</code>, into which we'll want to add: </p> <pre><code>package resource_test\n\nimport (\n\"fmt\"\n\"testing\"\n\n\"github.com/hashicorp/go-azure-helpers/resourcemanager/location\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/acceptance\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/acceptance/check\"\n)\n\ntype ResourceGroupExampleDataSource struct{}\n\nfunc TestAccResourceGroupExampleDataSource_basic(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"data.azurerm_resource_group_example\", \"test\")\nr := ResourceGroupExampleDataSource{}\n\ndata.DataSourceTest(t, []acceptance.TestStep{\n{\nConfig: r.basic(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).Key(\"location\").HasValue(location.Normalize(data.Locations.Primary)),\ncheck.That(data.ResourceName).Key(\"tags.%\").HasValue(\"1\"),\ncheck.That(data.ResourceName).Key(\"tags.env\").HasValue(\"test\"),\n),\n},\n})\n}\n\nfunc (ResourceGroupExampleDataSource) basic(data acceptance.TestData) string {\nreturn fmt.Sprintf(`\nprovider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_resource_group\" \"test\" {\n  name     = \"acctestRg-%d\"\n  location = \"%s\"\n\n  tags = {\n    env = \"test\"\n  }\n}\n\ndata \"azurerm_resource_group_example\" \"test\" {\n  name = azurerm_resource_group.test.name\n}\n`, data.RandomInteger, data.Locations.Primary)\n}\n</code></pre> <p>There's a more detailed breakdown of how this works in the Acceptance Testing reference - but to summarize what's going on here:</p> <ol> <li>Test Terraform Configurations are defined as methods on the struct <code>ResourceGroupExampleDataSource</code> so that they're easily accessible (this helps to avoid them being unintentionally used in other resources).</li> <li>The <code>acceptance.TestData</code> object contains a number of helpers, including both random integers, strings and the Azure Locations where resources should be provisioned - which are used to ensure when tests are run in parallel that we provision unique resources for testing purposes.</li> <li>We're asserting on the Computed (e.g. read-only) fields returned from the Resource - we don't check the user-specified fields (<code>name</code> in this case) as if it's missing, the test will fail to find the Resource Group.</li> <li>We append <code>_test</code> to the Go package name (e.g. <code>resource_test</code>) since we need to be able to access both the <code>resource</code> package and the <code>acceptance</code> package (which is a circular reference, otherwise).</li> </ol> <p>At this point we should be able to run this test.</p>"},{"location":"topics/guide-new-data-source/#step-6-run-the-acceptance-tests","title":"Step 6: Run the Acceptance Test(s)","text":"<p>Detailed instructions on Running the Tests can be found in this guide - when a Service Principal is configured you can run the test above using:</p> <pre><code>make acctests SERVICE='resource' TESTARGS='-run=TestAccResourceGroupExampleDataSource_basic' TESTTIMEOUT='60m'\n</code></pre> <p>Which should output:</p> <pre><code>==&gt; Checking that code complies with gofmt requirements...\n==&gt; Checking that Custom Timeouts are used...\n==&gt; Checking that acceptance test packages are used...\nTF_ACC=1 go test -v ./internal/services/resource -run=TestAccResourceGroupExampleDataSource_basic -timeout 60m -ldflags=\"-X=github.com/hashicorp/terraform-provider-azurerm/version.ProviderVersion=acc\"\n=== RUN   TestAccResourceGroupExampleDataSource_basic\n=== PAUSE TestAccResourceGroupExampleDataSource_basic\n=== CONT  TestAccResourceGroupExampleDataSource_basic\n--- PASS: TestAccResourceGroupExampleDataSource_basic (88.15s)\nPASS\nok      github.com/hashicorp/terraform-provider-azurerm/internal/services/resource  88.735s\n</code></pre>"},{"location":"topics/guide-new-data-source/#step-7-add-documentation-for-this-data-source","title":"Step 7: Add Documentation for this Data Source","text":"<p>At this point in time documentation for each Data Source (and Resource) is written manually, located within the <code>./website</code> folder - in this case this will be located at <code>./website/docs/d/resource_group_example.html.markdown</code>.</p> <p>There is a tool within the repository to help scaffold the documentation for a Data Source - the documentation for this Data Source can be scaffolded via the following command:</p> <pre><code>$ make scaffold-website BRAND_NAME=\"Resource Group Example\" RESOURCE_NAME=\"azurerm_resource_group_example\" RESOURCE_TYPE=\"data\"\n</code></pre> <p>The documentation should look something like below - containing both an example usage and the required, optional and computed fields:</p> <p>Note: In the example below you'll need to replace each <code>[]</code> with a backtick \"`\" - as otherwise this gets rendered incorrectly, unfortunately.</p> <pre><code>---\nsubcategory: \"Base\"\nlayout: \"azurerm\"\npage_title: \"Azure Resource Manager: Data Source: azurerm_resource_group_example\"\ndescription: |-\n  Gets information about an existing Resource Group.\n---\n\n# Data Source: azurerm_resource_group_example\n\nUse this data source to access information about an existing Resource Group.\n\n## Example Usage\n\n[][][]hcl\ndata \"azurerm_resource_group_example\" \"example\" {\n  name = \"existing\"\n}\n\noutput \"id\" {\n  value = data.azurerm_resource_group_example.example.id\n}\n[][][]\n\n## Arguments Reference\n\nThe following arguments are supported:\n\n* `name` - (Required) The Name of this Resource Group.\n\n## Attributes Reference\n\nIn addition to the Arguments listed above - the following Attributes are exported:\n\n* `id` - The ID of the Resource Group.\n\n* `location` - The Azure Region where the Resource Group exists.\n\n* `tags` - A mapping of tags assigned to the Resource Group.\n\n## Timeouts\n\nThe `timeouts` block allows you to specify [timeouts](https://developer.hashicorp.com/terraform/language/resources/configure#define-operation-timeouts) for certain actions:\n\n* `read` - (Defaults to 5 minutes) Used when retrieving the Resource Group.\n</code></pre> <p>Note: In the example above you'll need to replace each <code>[]</code> with a backtick \"`\" - as otherwise this gets rendered incorrectly, unfortunately.</p>"},{"location":"topics/guide-new-data-source/#step-8-send-the-pull-request","title":"Step 8: Send the Pull Request","text":"<p>See our recommendations for opening a Pull Request.</p>"},{"location":"topics/guide-new-feature/","title":"Guide: Adding a new Feature to the Features Block","text":"<p>This guide covers how to add a new Feature to the Features Block (Terraform Docs) that will change the default behaviour for how a resource or service works. Reasons for this can include:</p> <ul> <li> <p>Purging a resource during delete</p> </li> <li> <p>Recovering a resource that has been soft deleted during create</p> </li> <li> <p>Detach a connected resource during deletion</p> </li> </ul> <p>Following are the steps needed to add a new Feature to the Feature Block:</p> <p>Note: The Azure Provider is in the process of moving towards a new Framework Plugin for the provider. Because of this, we must update the provider in a few areas when adding a new feature. We'll update the following areas <code>internal/features</code>, <code>internal/provider</code>, <code>internal/provider/framework</code>, and the resource file itself.</p>"},{"location":"topics/guide-new-feature/#updating-internalfeatures","title":"Updating <code>internal/features</code>","text":"<ol> <li>Update <code>internal/features/user_flags.go</code> with either a new block for the service package or updating an existing service package with the new feature to add. Added struct names should represent the service package they affect, and feature names should concisely describe their effect.</li> </ol> <pre><code>type UserFeatures struct {\nKeyVault KeyVaultFeatures\n}\n\ntype KeyVaultFeatures struct {\nPurgeSoftDeleteOnDestroy bool\n}\n</code></pre> <ol> <li>Update <code>internal/features/defaults.go</code> with what the default value for the new feature will be. This must represent the current default behaviour of the resource(s) to avoid this becoming a breaking change when the feature flagged behaviour is added to the target resource(s).</li> </ol> <pre><code>func Default() UserFeatures {\nreturn UserFeatures{\n...\nKeyVault: KeyVaultFeatures{\nPurgeSoftDeleteOnDestroy: true,\n}\n...\n}\n</code></pre>"},{"location":"topics/guide-new-feature/#updating-internalprovider","title":"Updating <code>internal/provider</code>","text":"<ol> <li>Update <code>internal/provider/feature.go</code> with what the Terraform schema will look like and how to thread it into the features block</li> </ol> <pre><code>func schemaFeatures(supportLegacyTestSuite bool) *pluginsdk.Schema {\nfeaturesMap := map[string]*pluginsdk.Schema{\n...\n\"key_vault\": {\nType:     pluginsdk.TypeList,\nOptional: true,\nMaxItems: 1,\nElem: &amp;pluginsdk.Resource{\nSchema: map[string]*pluginsdk.Schema{\n\"purge_soft_delete_on_destroy\": {\nDescription: \"When enabled soft-deleted `azurerm_key_vault` resources will be permanently deleted (e.g purged), when destroyed\",\nType:        pluginsdk.TypeBool,\nOptional:    true,\nDefault:     true,\n},\n},\n},\n...\n}\n}\n\nfunc expandFeatures(input []interface{}) features.UserFeatures {\n...\nif raw, ok := val[\"key_vault\"]; ok {\nitems := raw.([]interface{})\nif len(items) &gt; 0 &amp;&amp; items[0] != nil {\nkeyVaultRaw := items[0].(map[string]interface{})\nif v, ok := keyVaultRaw[\"purge_soft_delete_on_destroy\"]; ok {\nfeaturesMap.KeyVault.PurgeSoftDeleteOnDestroy = v.(bool)\n}\n}\n}\n...\n}\n</code></pre> <ol> <li>Update <code>internal/provider/feature_test.go</code> to include a test for every permutation of the feature you are adding to the TestExpandFeatures test and a test dedicated to the service package of the feature.</li> </ol> <pre><code>func TestExpandFeatures(t *testing.T) {\ntestData := []struct {\nName     string\nInput    []interface{}\nEnvVars  map[string]interface{}\nExpected features.UserFeatures\n}{\n{\nName:  \"Empty Block\",\nInput: []interface{}{},\nExpected: features.UserFeatures{\n...\nKeyVault: features.KeyVaultFeatures{\nPurgeSoftDeleteOnDestroy:         true,\n},\n...\n}\n},\n{\nName: \"Complete Enabled\",\nInput: []interface{}{\nmap[string]interface{}{\n...\n\"key_vault\": []interface{}{\nmap[string]interface{}{\n\"purge_soft_delete_on_destroy\": true,\n},\n},   ...\n},\n},\nExpected: features.UserFeatures{\n...\nKeyVault: features.KeyVaultFeatures{\nPurgeSoftDeleteOnDestroy: true,\n},\n...\n},\n},\n{\nName: \"Complete Disabled\",\nInput: []interface{}{\nmap[string]interface{}{\n...\n\"key_vault\": []interface{}{\nmap[string]interface{}{\n\"purge_soft_delete_on_destroy\": false,\n},\n},\n...\n},\n},\nExpected: features.UserFeatures{\n...\nKeyVault: features.KeyVaultFeatures{\nPurgeSoftDeleteOnDestroy: false,\n},\n...\n},\n},\n},  }\n\n\nfunc TestExpandFeaturesKeyVault(t *testing.T) {\ntestData := []struct {\nName     string\nInput    []interface{}\nEnvVars  map[string]interface{}\nExpected features.UserFeatures\n}{\n{\nName: \"Empty Block\",\nInput: []interface{}{\nmap[string]interface{}{\n\"key_vault\": []interface{}{},\n},\n},\nExpected: features.UserFeatures{\nKeyVault: features.KeyVaultFeatures{\nPurgeSoftDeleteOnDestroy: true,\n},\n},\n},\n{\nName: \"Purge Soft Delete On Destroy\",\nInput: []interface{}{\nmap[string]interface{}{\n\"key_vault\": []interface{}{\nmap[string]interface{}{\n\"purge_soft_delete_on_destroy\": true,\n},\n},\n},\n},\nExpected: features.UserFeatures{\nKeyVault: features.KeyVaultFeatures{\nPurgeSoftDeleteOnDestroy: true,\n},\n},\n},\n{\nName: \"Purge Soft Delete On Destroy Disabled\",\nInput: []interface{}{\nmap[string]interface{}{\n\"key_vault\": []interface{}{\nmap[string]interface{}{\n\"purge_soft_delete_on_destroy\": false,\n},\n},\n},\n},\nExpected: features.UserFeatures{\nKeyVault: features.KeyVaultFeatures{\nPurgeSoftDeleteOnDestroy: false,\n},\n},\n},\n}\n\nfor _, testCase := range testData {\nt.Logf(\"[DEBUG] Test Case: %q\", testCase.Name)\nresult := expandFeatures(testCase.Input)\nif !reflect.DeepEqual(result.KeyVault, testCase.Expected.KeyVault) {\nt.Fatalf(\"Expected %+v but got %+v\", result.KeyVault, testCase.Expected.KeyVault)\n}\n}\n}\n</code></pre>"},{"location":"topics/guide-new-feature/#updating-internalproviderframework","title":"Updating <code>internal/provider/framework</code>","text":"<ol> <li>Update <code>internal/provider/framework/model.go</code></li> </ol> <pre><code>// For new services, add a List type for the new block with a `tfsdk` struct tag that matches the schema name for the block, for new features in an existing block/service, this can be skipped.\ntype Features struct {\n...\nKeyVault types.List `tfsdk:\"key_vault\"`\n...\n}\n\n// and an attribute map variable for the block, or add to the appropriate existing var\nvar FeaturesAttributes = map[string]attr.Type{\n...\n\"key_vault\": types.ListType{}.WithElementType(types.ObjectType{}.WithAttributeTypes(KeyVaultAttributes)),\n...\n}\n\n// Add a Go struct that matches the new block or add to the appropriate existing struct\ntype KeyVault struct {\nPurgeSoftDeleteOnDestroy types.Bool `tfsdk:\"purge_soft_delete_on_destroy\"`\n}\n\n// finally, create the attribute map variable for the new block, or add the feature to the appropriate existing map\nvar KeyVaultAttributes = map[string]attr.Type{\n\"purge_soft_delete_on_destroy\": types.BoolType\n}\n</code></pre> <ol> <li>Update <code>internal/provider/framework/provider.go</code></li> </ol> <pre><code>func (p *azureRmFrameworkProvider) Schema(_ context.Context, _ provider.SchemaRequest, response *provider.SchemaResponse) {\nresponse.Schema = schema.Schema{\n...\nBlocks: map[string]schema.Block{\n\"features\": schema.ListNestedBlock{\nValidators: []validator.List{\nlistvalidator.SizeBetween(1, 1),\n},\nNestedObject: schema.NestedBlockObject{\nBlocks: map[string]schema.Block{\n...\n// Add an attribute map variable for the new block or add to the existing map inside the Nested Object\n\"key_vault\": schema.ListNestedBlock{\nNestedObject: schema.NestedBlockObject{\nAttributes: map[string]schema.Attribute{\n\"purge_soft_delete_on_destroy\": schema.BoolAttribute{\nDescription: \"When enabled soft-deleted `azurerm_key_vault` resources will be permanently deleted (e.g purged), when destroyed\",\nOptional:    true,\n},\n},\n},\n},\n...\n},\n},\n},\n},  ...\n}\n}\n</code></pre> <ol> <li>Update <code>internal/provider/framework/config.go</code></li> </ol> <pre><code>// Add a new check that the feature has been specified in the config that then loads the feature into the provider or add the new feature to the existing block.\nfunc (p *ProviderConfig) Load(ctx context.Context, data *ProviderModel, tfVersion string, diags *diag.Diagnostics) {\n...\nif !features.KeyVault.IsNull() &amp;&amp; !features.KeyVault.IsUnknown() {\nvar feature []KeyVault\nd := features.KeyVault.ElementsAs(ctx, &amp;feature, true)\ndiags.Append(d...)\nif diags.HasError() {\nreturn\n}\n\nf.KeyVault.PurgeSoftDeleteOnDestroy = true\nif !feature[0].PurgeSoftDeleteOnDestroy.IsNull() &amp;&amp; !feature[0].PurgeSoftDeleteOnDestroy.IsUnknown() {\nf.KeyVault.PurgeSoftDeleteOnDestroy = feature[0].PurgeSoftDeleteOnDestroy.ValueBool()\n}\n}\n...\n}\n</code></pre> <ol> <li>Update  <code>internal/provider/framework/config_test.go</code> with the Features Model and Attributes</li> </ol> <pre><code>func defaultFeaturesList() types.List {\n...\n// Add a NewObjectValueFrom that holds what type of feature you have or append to the existing ObjectValueFrom\nkeyVault, _ := basetypes.NewObjectValueFrom(context.Background(), KeyVaultAttributes, map[string]attr.Value{\n\"purge_soft_delete_on_destroy\":                            basetypes.NewBoolNull(),\n})\nkeyVaultList, _ := basetypes.NewListValue(types.ObjectType{}.WithAttributeTypes(KeyVaultAttributes), []attr.Value{keyVault})\n...\n// If the added feature is supporting a new service, add it to the following list of services\nfData, d := basetypes.NewObjectValue(FeaturesAttributes, map[string]attr.Value{\n...\n\"key_vault\": keyVaultList,\n...\n}\n}\n</code></pre>"},{"location":"topics/guide-new-feature/#update-the-resource","title":"Update the resource","text":"<ol> <li>Update <code>internal/service/serviceName/resourceName.go</code> in this case <code>internal/service/keyvault/key_vault_resource.go</code> to include the functionality of the added feature.</li> </ol> <pre><code>func resourceKeyVaultDelete(d *pluginsdk.ResourceData, meta interface{}) error {\n...\nif meta.(*clients.Client).Features.KeyVault.PurgeSoftDeleteOnDestroy {\n// Purge the Keyvault\n}\n...\n}\n</code></pre> <ol> <li>Update <code>internal/service/serviceName/resourceName_test.go</code> in this case <code>internal/service/keyvault/key_vault_resource_test.go</code> to test the new feature.</li> </ol> <pre><code>func TestAccKeyVault_softDeleteRecoveryDisabled(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_key_vault\", \"test\")\nr := KeyVaultResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{\n// create it regularly\nConfig: r.softDeleteRecoveryDisabled(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\ncheck.That(data.ResourceName).Key(\"purge_protection_enabled\").HasValue(\"false\"),\n),\n},\ndata.ImportStep(),\n{\n// delete the key vault\nConfig: r.softDeleteAbsent(data),\n},\n{\n// attempting to re-create it requires recovery, which is enabled by default\nConfig:      r.softDeleteRecoveryDisabled(data),\nExpectError: regexp.MustCompile(\"An existing soft-deleted Key Vault exists with the Name\"),\n},\n})\n}\n\nfunc (KeyVaultResource) softDeleteRecoveryDisabled(data acceptance.TestData) string {\nreturn fmt.Sprintf(`\nprovider \"azurerm\" {\n  features {\n    key_vault {\n      recover_soft_deleted_key_vaults = false\n    }\n  }\n}\n...\n`)\n}\n</code></pre> <p>At this point, if all tests have passed including the tests found in <code>internal/provider/function/normalise_resource_id_test.go</code> and <code>internal/provider/function/parse_resource_id_test.go</code>, the Feature should be implemented and ready for use. </p>"},{"location":"topics/guide-new-fields-to-data-source/","title":"Guide: Extending a Data Source","text":"<p>It is sometimes necessary to make changes to an existing Data Source. Reasons include:</p> <ul> <li> <p>A new property is added in the referenced resource</p> </li> <li> <p>A property is deprecated and/or no longer available in the referenced resource</p> </li> <li> <p>An API update changes the behaviour of the referenced resource</p> </li> </ul> <p>When updating an existing Data Source keep in mind the configuration of the end user that may be using it.  Mitigations must be taken, where possible, to prevent the change breaking existing user configurations.</p> <p>The process is similar to extending an existing Resource, in that modifications in multiple places are required.</p>"},{"location":"topics/guide-new-fields-to-data-source/#schema","title":"Schema","text":"<p>Building on the example from adding a new data source the new property will need to be added into the <code>Attributes</code> list which contains a list of schema fields that are Computed only.</p> <p>The location of the new property within this list is determined based on the order found in adding a new data source. Taking the hypothetical property <code>public_network_access_enabled</code> as an example this would then end up looking like this in <code>Attributes</code>.</p> <pre><code>func (ResourceGroupExampleDataSource) Attributes() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n\"location\": {\nType:      pluginsdk.TypeString,\nComputed:  true,\n},\n\n\"public_network_access_enabled\": {\nType: pluginsdk.TypeBool,\nComputed: true,\n},       \"tags\": commonschema.TagsDataSource(),\n}\n}\n</code></pre> <ul> <li>For new properties that are ambiguous in their functionality or nature, follow the property naming guidelines when choosing a name.</li> </ul>"},{"location":"topics/guide-new-fields-to-data-source/#read-function","title":"Read function","text":"<ul> <li>The only thing to consider here is setting the new value into state and nil checking beforehand if required.</li> </ul> <pre><code>publicNetworkAccess := true\nif v := props.PublicNetworkAccess; v != nil {\npublicNetworkAccess = *v\n}\nd.Set(\"public_network_access_enabled\", publicNetworkAccess)\n</code></pre> <ul> <li>For simple types we can use the helper function <code>pointer.From</code> to condense the nil check.</li> </ul> <pre><code>d.Set(\"a_simple_string_property\", pointer.From(props.AStringPointer))\n</code></pre>"},{"location":"topics/guide-new-fields-to-data-source/#tests","title":"Tests","text":"<ul> <li>New properties should be added to the basic data source test with an explicit check.</li> </ul> <pre><code>func TestAccDataSourceSomeResource_basic(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"data.azurerm_some_resource\", \"test\")\nr := AvailabilitySetDataSource{}\n\ndata.DataSourceTest(t, []acceptance.TestStep{\n{\nConfig: r.basic(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).Key(\"name\").Exists(),\ncheck.That(data.ResourceName).Key(\"resource_group_name\").Exists(),\ncheck.That(data.ResourceName).Key(\"new_property\").HasValue(\"1\"),\n),\n},\n})\n}\n</code></pre>"},{"location":"topics/guide-new-fields-to-data-source/#docs","title":"Docs","text":"<ul> <li>Lastly, don't forget to update the docs where the property ordering is determined alphabetically.</li> </ul>"},{"location":"topics/guide-new-fields-to-resource/","title":"Guide: Extending a Resource","text":"<p>As Azure services evolve and new features or functionalities enter public preview or become GA, their corresponding Terraform resources will need to be extended and/or modified in order to expose newly added functionalities.</p> <p>Oftentimes this involves the addition of a new property or perhaps even the renaming of an existing property.</p>"},{"location":"topics/guide-new-fields-to-resource/#adding-a-new-property","title":"Adding a new property","text":"<p>In order to incorporate a new property into a resource, modifications need to be made in multiple places. These are outlined below with pointers on what to consider and look out for as well as examples.</p>"},{"location":"topics/guide-new-fields-to-resource/#schema","title":"Schema","text":"<p>Building on the example found in adding a new resource the new property will need to be added to either the user configurable list of <code>Arguments</code>, or <code>Attributes</code> if non-configurable.</p> <p>Our hypothetical property <code>logging_enabled</code> will be user configurable and thus will need to be added to the <code>Arguments</code> list.</p> <p>The position of the new property is determined based on the order found in adding a new resource and will end up looking like the code block below. Here is an example for a typed resource:</p> <pre><code>func (ResourceGroupExampleResource) Arguments() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n\"name\": {\nType:     pluginsdk.TypeString,\nRequired: true,\n},\n\n\"location\": commonschema.Location(),\n\n\"logging_enabled\": {\nType:     pluginsdk.TypeBool,\nOptional: true,\n}\n\n\"tags\": commonschema.TagsDataSource(),\n}\n}\n</code></pre> <ul> <li> <p>Remember to choose an appropriate name, see our property naming guidelines.</p> </li> <li> <p>Ensure there is appropriate validation, at the very least <code>validation.StringIsNotEmpty</code> should be set for strings where a validation pattern cannot be determined.</p> </li> <li> <p>When adding multiple properties or blocks thought should be given on how to map these, see schema design considerations for specific examples. </p> </li> </ul>"},{"location":"topics/guide-new-fields-to-resource/#create-function","title":"Create function","text":"<ul> <li>The new property needs to be set in the properties struct for the resource.</li> </ul> <pre><code>props := machinelearning.Workspace{\nProperties: &amp;machinelearning.WorkspaceProperties{\nLoggingEnabled: pointer.To(model.LoggingEnabled)\n}\n}\n</code></pre>"},{"location":"topics/guide-new-fields-to-resource/#update-function","title":"Update function","text":"<ul> <li>When performing selective updates check whether the property has changed.</li> </ul> <pre><code>if metadata.ResourceData.HasChange(\"logging_enabled\") {\nexisting.Model.Properties.LoggingEnabled = pointer.From(model.LoggingEnabled)\n}\n</code></pre>"},{"location":"topics/guide-new-fields-to-resource/#read-function","title":"Read function","text":"<ul> <li> <p>Generally speaking all properties should have a value set into state.</p> </li> <li> <p>If the value returned by the API is a pointer we should account for the possibility of a nil reference to prevent panics in the provider. One way to do this is to use <code>pointer.From()</code>:</p> </li> </ul> <pre><code>state := MyResourceModel{}\n\nif model := resp.Model; model != nil {\nstate.LoggingEnabled = pointer.From(model.LoggingEnabled)\n}\n\nreturn metadata.Encode(&amp;state)\n</code></pre>"},{"location":"topics/guide-new-fields-to-resource/#tests","title":"Tests","text":"<ul> <li> <p>It is often sufficient to add a new property to one of the existing, non-basic tests.</p> </li> <li> <p>If the property is <code>Optional</code> then it can be added to the <code>complete</code> test.</p> </li> <li> <p>Properties that are <code>Required</code> will need to be added to all the existing tests for that resource.</p> </li> <li> <p>In cases where a new property or block requires additional setup or pre-requisites it makes sense to create a dedicated test for it.</p> </li> <li> <p>Adding a new property or updating an existing property with a default should be checked properly against the current version of Terraform State and the Azure API as tests won't be able to catch a potential breaking change, see our section on defaults and breaking changes</p> </li> </ul>"},{"location":"topics/guide-new-fields-to-resource/#docs","title":"Docs","text":"<ul> <li> <p>Lastly, don't forget to update the docs with this new property!</p> </li> <li> <p>Property ordering within the docs follows the same conventions as in the Schema.</p> </li> <li> <p><code>Computed</code> only values should be added under <code>Attributes Reference</code></p> </li> </ul>"},{"location":"topics/guide-new-fields-to-resource/#renaming-and-deprecating-a-property","title":"Renaming and Deprecating a Property","text":"<p>Fixing typos in property names or renaming them to improve the meaning is unfortunately not just a matter of updating the name in the resource's code.</p> <p>This is a breaking change and can be done by deprecating the old property, replacing it with a new one, as well as feature flagging its removal in the next major release of the provider.</p> <p>A feature flag is essentially a function that returns a boolean and allows the provider to accommodate alternate behaviours that are meant for major releases. A release feature flag will always return <code>false</code> until it has been hooked up to an environment variable that allows users to toggle the behaviour and can be found in the <code>./internal/features</code> directory.</p> <p>As an example, let's deprecate and replace the property <code>enable_compression</code> with <code>compression_enabled</code>. Here is an example for an untyped resource (for more information about typed and untyped resource, see the Best Practices guide):</p> <pre><code>Schema: map[string]*pluginsdk.Schema{\n...\n\"enable_compression\": {\nType:     pluginsdk.TypeBool,\nOptional: true,\n},\n</code></pre> <p>Here is an example for a typed resource:</p> <pre><code>func (r ExampleResource) Arguments() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n\"enable_compression\": {\nType:     pluginsdk.TypeBool,\nOptional: true,\n},\n}\n</code></pre> <p>After deprecation the schema might look like the code below. Here is an example for an untyped resource:</p> <pre><code>func resource() *pluginsdk.Resource {\nresource := &amp;pluginsdk.Resource{\nSchema: map[string]*pluginsdk.Schema{\n// The deprecated property is moved out of the schema and conditionally added back via the feature flag\n\"compression_enabled\": {\nType:     pluginsdk.TypeBool,\nOptional: true,\n},\n},\n}\n\nif !features.FivePointOh() {\nresource[\"compression_enabled\"] = &amp;pluginsdk.Schema{\nType:          pluginsdk.TypeBool,\nOptional:      true,\nComputed:      true,\nConflictsWith: []string{\"enable_compression\"}\n}\n\nresource[\"enable_compression\"] = &amp;pluginsdk.Schema{\nType:      pluginsdk.TypeBool,\nOptional:   true,\nComputed:   true,\nDeprecated: \"This property has been renamed to `compression_enabled` and will be removed in v5.0 of the provider\",\nConflictsWith: []string{\"compression_enabled\"}\n}   }\n\nreturn resource\n}\n</code></pre> <p>Here is an example for a typed resource:</p> <pre><code>func (r ExampleResource) Arguments() map[string]*pluginsdk.Schema {\nschema := map[string]*pluginsdk.Schema{\n// The deprecated property is moved out of the schema and conditionally added back via the feature flag\n\"compression_enabled\": {\nType:      pluginsdk.TypeBool,\nOptional:   true,\n},\n}\n}\n\nif !features.FivePointOh() {\nschema[\"compression_enabled\"] = &amp;pluginsdk.Schema{\nType:          pluginsdk.TypeBool,\nOptional:      true,\nComputed:      true,\nConflictsWith: []string{\"enable_compression\"}\n}\n\nschema[\"enable_compression\"] = &amp;pluginsdk.Schema{\nType:      pluginsdk.TypeBool,\nOptional:   true,\nComputed:   true,\nDeprecated: \"This property has been renamed to `compression_enabled` and will be removed in v5.0 of the provider\",\nConflictsWith: []string{\"compression_enabled\"}\n}   }\n\nreturn schema\n}\n</code></pre> <p>Also make sure to feature flag the behaviour in the <code>Create()</code>, <code>Update()</code> and <code>Read()</code> methods. Here is an example for an untyped resource:</p> <pre><code>func myResourceCreate() {\n...\nenableCompression := false\nif !features.FivePointOh() {\nif v, ok := d.GetOkExists(\"enable_compression\"); ok {\nenableCompression = v.(bool)\n}       }\n\nif v, ok := d.GetOkExists(\"compression_enabled\"); ok {\nenableCompression = v.(bool)\n}\n}\n\nfunc myResourceRead() {\n...\nd.Set(\"compression_enabled\", props.EnableCompression)\n\nif !features.FivePointOh() {\nd.Set(\"enable_compression\", props.EnableCompression)\n}\n...\n}\n</code></pre> <p>Here is an example for a typed resource: <pre><code>func (r ExampleResource) Create() sdk.ResourceFunc {\n...\ncompressionEnabled := false\nif !features.FivePointOh() {\ncompressionEnabled = model.EnableCompression\n}\n\ncompressionEnabled = model.CompressionEnabled\n...\n}\n\nfunc (r ExampleResource) Read() sdk.ResourceFunc {\n...\nstate.CompressionEnabled = pointer.From(props.CompressionEnabled)\n\nif !features.FivePointOh() {\nstate.EnableCompression = pointer.From(props.CompressionEnabled)\n}   ...\n}\n</code></pre></p> <p>When deprecating a property in a Typed Resource, it is important to ensure that the Go struct representing the schema is correctly tagged to prevent the SDK decoding the removed property when the major version beta / feature flag is in use. In these cases the struct tags must be updated to include <code>,removedInNextMajorVersion</code>.  </p> <pre><code>type ExampleResourceModel struct {\nName               string `tfschema:\"name\"`\nEnableCompression  bool `tfschema:\"enable_compression,removedInNextMajorVersion\"`\nCompressionEnabled bool `tfschema:\"compression_enabled\"`\n}\n</code></pre>"},{"location":"topics/guide-new-resource-vs-inline/","title":"Guide: When to inline new functionality (as either a block or property) versus a new resource","text":"<p>Sometimes when implementing new functionality it can be a bit unclear whether it is necessary to create a new resource versus to add a new property or block to an existing resource.</p> <p>To get a bit of insight in how a decision can be made, these are some rules of thumb to decide. In case it is unclear, please contact one of the HashiCorp Maintainers.</p>"},{"location":"topics/guide-new-resource-vs-inline/#inline","title":"Inline","text":"<p>Most additional functionality will end up inline in an existing resource.</p> <p>APIs to enable or disable functionality, define the resource functionality or configure details on a resource are most of the time inlined. Relations between resources with clearly separate concern (i.e. which VNet a K8s cluster will land in) are most of the time inlined.</p> <p>A few categories of inlined functionality with possible motivations to inline are summed up below.</p>"},{"location":"topics/guide-new-resource-vs-inline/#category-1-properties","title":"Category 1: properties","text":"<ul> <li>When it is 'just a property' of this resource, like <code>sku</code> in the example below.</li> <li>It would require a lot of extra work to make these separate resources.</li> </ul> <pre><code>resource \"azurerm_example_resource\" \"example\" {\n  name = \"ThePerfectExample\"\n  sku  = \"Gold\"\n}\n</code></pre>"},{"location":"topics/guide-new-resource-vs-inline/#category-2-child-resources-which-cannot-be-separated","title":"Category 2: child resources which cannot be separated","text":"<ul> <li>It has a strict <code>1:1</code> relation with its parent resource</li> <li>It cannot be deleted, only returned to a default state (i.e. you might find an API, but only to update the resource)</li> <li>It doesn't have its own unique Resource ID or name (i.e. <code>&lt;parentId&gt;/default</code> or <code>&lt;parentId&gt;/keyrotationpolicy</code>, not something like <code>&lt;parentId&gt;/subResource/MySubResourceName</code>)</li> <li>It does not have its own API endpoint but uses the parent resource endpoint</li> <li>It does not cross security/team boundaries in the most common client situation</li> <li>It does not contain backwards compatibility issues</li> </ul>"},{"location":"topics/guide-new-resource-vs-inline/#category-3-relations-between-resources","title":"Category 3: relations between resources","text":"<ul> <li>Resources are really separate and have <code>1:many</code> and <code>many:1</code> relations with a (i.e. a relation property with a resource within a completely different Resource Provider <code>/subscriptions/&lt;subscriptionId&gt;/resourceGroups/&lt;resourceGroup&gt;/providers/Microsoft.Network/networkSecurityGroups/example-nsg</code> and <code>/subscriptions/&lt;subscriptionId&gt;/resourceGroups/&lt;resourceGroup&gt;/providers/Microsoft.Storage/storageAccounts/example-storage</code>: which <code>azurerm_storage_account</code> is used to store the logs from a resource).</li> <li>These relations are created with API calls to the original resource provider, not the connected one</li> <li>Reading the connections between the resources does not require extra permissions than previously necessary to create the resource (i.e. the Service Principal used to read the connection/relation should be also <code>Owner</code>/<code>Contributor</code> on the resource group the connection is made to)</li> </ul>"},{"location":"topics/guide-new-resource-vs-inline/#separate-resource","title":"Separate resource","text":"<p>While inlining might make a lot of sense for many APIs, there are also good reasons to separate them out. These arguments may not be conclusive, but can help steer in the right direction.</p>"},{"location":"topics/guide-new-resource-vs-inline/#category-1-the-obvious-new-resource","title":"Category 1: the obvious new resource","text":"<ul> <li>It is a new resource with its own lifecycle, own API endpoints (at least <code>Update</code> and <code>Delete</code>), it just feels natural to put it in a separate resource</li> </ul>"},{"location":"topics/guide-new-resource-vs-inline/#category-2-child-resources","title":"Category 2: child resources","text":"<ul> <li>It does have its own unique Resource ID or name (i.e. <code>&lt;parentId&gt;/subResource/MySubResourceName</code>)</li> <li>It has its own API endpoint for <code>Create</code>, <code>Update</code> and <code>Delete</code> actions</li> <li>It needs more permissions on the already existing resource than the current parent resource requires (i.e. Key Vault Key Rotation Policies require more permissions than the Key Vault Key it really belongs to)</li> <li>Control Plane vs Data plane: the functionality is acting on the Data Plane instead of Control Plane of the service or vice versa. (i.e. Azure Storage Account management vs the actual Blobs put in there, Azure Key Vault management vs the Keys/Certs/Secrets inside)</li> <li>Its functionality and therefore the scope of the resource crosses team/security boundaries (i.e. Infra team vs Application team).</li> </ul>"},{"location":"topics/guide-new-resource-vs-inline/#category-3-relations-between-resources-it-is-complicated","title":"Category 3: relations between resources (\"It is complicated\")","text":"<ul> <li>It is a mediator: there is a separate endpoint to create a relation between two existing resources</li> <li>It requires more permissions on another resource to create the connection than to create the resource itself (i.e. connecting a NSG resource to a Subnet)</li> </ul>"},{"location":"topics/guide-new-resource-vs-inline/#both-inline-and-separate","title":"Both inline and separate","text":"<p>It might be that there are multiple use-cases and scenarios necessary. Sometimes it makes sense to create it inline, sometimes it makes more sense to separate them.</p> <p>This requires caution from both the implementer and the user. In most cases it should be explained with some notes in the <code>docs</code>. Within the inline resource implementation it requires that it doesn't delete or update properties created externally when it is not explicitly configured in the resource. For the user this might have the drawback that the inlined resource is not strict in enforcing the existing inlined properties. Mixed use within the same context might end up in a mess and is advised not to do.</p> <p>A few examples of resources which are both inlined and separate resources: - Subnets (part of VNet resource as well) - NSG rules (part of NSG resource as well) - Key Vault permissions (part of Key Vault resource as well)</p>"},{"location":"topics/guide-new-resource/","title":"Guide: New Resource","text":"<p>This guide covers adding a new Resource to a Service Package, see adding a New Service Package if the Service Package doesn't exist yet.</p>"},{"location":"topics/guide-new-resource/#related-topics","title":"Related Topics","text":"<ul> <li>Acceptance Testing</li> <li>Our Recommendations for opening a Pull Request</li> </ul>"},{"location":"topics/guide-new-resource/#stages","title":"Stages","text":"<p>At this point in time the AzureRM Provider supports both Typed and Untyped Resources - more information can be found in the High Level Overview.</p> <p>This guide covers adding a new Typed Resource, which makes uses the Typed SDK within this repository, which requires the following steps:</p> <ol> <li>Ensure all the dependencies are installed (see Building the Provider).</li> <li>Add an SDK Client (if required).</li> <li>Define the Resource ID.</li> <li>Scaffold an empty/new Resource.</li> <li>Register the new Resource.</li> <li>Add Acceptance Test(s) for this Resource.</li> <li>Run the Acceptance Test(s).</li> <li>Add Documentation for this Resource.</li> <li>Send the Pull Request.</li> </ol> <p>We'll go through each of those steps in turn, presuming that we're creating a Resource for a Resource Group.</p>"},{"location":"topics/guide-new-resource/#step-1-ensure-the-tools-are-installed","title":"Step 1: Ensure the Tools are installed","text":"<p>See Building the Provider.</p>"},{"location":"topics/guide-new-resource/#step-2-add-an-sdk-client-if-required","title":"Step 2: Add an SDK Client (if required)","text":"<p>This section covers how to add and configure the SDK Client.</p> <p>Determining which SDK Client you should be using is a little complicated unfortunately.</p> <p>The Client for the Service Package can be found in <code>./internal/services/{name}/client/client.go</code> - and we can add an instance of the SDK Client we want to use (here <code>resources.GroupsClient</code>) and configure it (adding credentials etc):</p> <pre><code>package client\n\nimport (\n\"fmt\"\n\n\"github.com/hashicorp/go-azure-sdk/resource-manager/resources/2022-09-01/resources\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/common\"\n)\n\ntype Client struct {\nGroupsClient *resources.GroupsClient\n}\n\nfunc NewClient(o *common.ClientOptions) (*Client, error) {\ngroupsClient, err := resources.NewResourcesClientWithBaseURI(o.Environment.ResourceManager)\nif err != nil {\nreturn nil, fmt.Errorf(\"building Resources Client: %+v\", err)\n}\no.Configure(groupsClient.Client, o.Authorizer.ResourceManager)\n\n// ...\n\nreturn &amp;Client{\nGroupsClient: groupsClient,\n}, nil\n}\n</code></pre> <p>Things worth noting here:</p> <ul> <li>The call to <code>o.Configure</code> configures the authorization token which should be used for this SDK Client - in most cases <code>ResourceManager</code> is the authorizer you want to use.</li> </ul> <p>At this point, this SDK Client should be usable within the Resource via:</p> <pre><code>client := metadata.Client.{ServicePackage}.{ClientField}\n</code></pre> <p>For example, in this case:</p> <pre><code>client := metadata.Client.Resource.GroupsClient\n</code></pre>"},{"location":"topics/guide-new-resource/#step-3-scaffold-an-emptynew-resource","title":"Step 3: Scaffold an empty/new Resource","text":"<p>Since we're creating a Resource for a Resource Group, which is a part of the Resources API - we'll want to create an empty Go file within the Service Package for Resources, which is located at <code>./internal/services/resource</code>.</p> <p>In this case, this'd be a file called <code>resource_group_example_resource.go</code>, which we'll start out with the following:</p> <p>Note: We'd normally name this file <code>resource_group_resource.go</code> - but there's an existing Resource for Resource Groups, so we're appending <code>example</code> to the name throughout this guide.</p> <pre><code>package resource\n\nimport \"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"\n\nvar _ sdk.Resource = ResourceGroupExampleResource{}\n\ntype ResourceGroupExampleResource struct {}\n</code></pre> <p>Note: Your editor may show a suggestion to implement the methods defined in <code>sdk.Resource</code> for the <code>ResourceGroupExampleResource</code> struct - we'd recommend holding off the first time around to explain each of the methods.</p> <p>In this case the interface <code>sdk.Resource</code> defines all of the methods required for a Resource which the newly created struct for the Resource Group Resource need to implement, which are:</p> <pre><code>type Resource interface {\nArguments() map[string]*schema.Schema\nAttributes() map[string]*schema.Schema\nModelObject() interface{}\nResourceType() string\nCreate() ResourceFunc\nRead() ResourceFunc\nDelete() ResourceFunc\nIDValidationFunc() pluginsdk.SchemaValidateFunc\n}\n</code></pre> <p>To go through these in turn:</p> <ul> <li><code>Arguments</code> returns a list of schema fields which are user-specifiable - either Required or Optional.</li> <li><code>Attributes</code> returns a list of schema fields which are Computed (read-only).</li> <li><code>ModelObject</code> returns a reference to a Go struct which is used as the Model for this Resource (this can also return <code>nil</code> if there's no model).</li> <li><code>ResourceType</code> returns the name of this resource within the Provider (for example <code>azurerm_resource_group_example</code>).</li> <li><code>Create</code> returns a function defining both the Timeout and the Create function (which creates this Resource Group using the Azure API) for this Resource.</li> <li><code>Read</code> returns a function defining both the Timeout and the Read function (which retrieves information from the Azure API) for this Resource.</li> <li><code>Delete</code> returns a function defining both the Timeout and the Delete function (which deletes this Resource Group using the Azure API) for this Resource.</li> <li><code>IDValidationFunc</code> returns a function which validates the Resource ID provided during <code>terraform import</code> to ensure it matches what we expect for this Resource.</li> </ul> <pre><code>type ResourceGroupExampleResourceModel struct {\nName     string            `tfschema:\"name\"`\nLocation string            `tfschema:\"location\"`\nTags     map[string]string `tfschema:\"tags\"`\n}\n\nfunc (ResourceGroupExampleResource) Arguments() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n\"name\": {\nType:         pluginsdk.TypeString,\nRequired:     true,\nValidateFunc: validation.StringIsNotEmpty,\n},\n\n\"location\": commonschema.Location(),\n\n\"tags\": commonschema.Tags(),\n}\n}\n\nfunc (ResourceGroupExampleResource) Attributes() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{}\n}\n\nfunc (ResourceGroupExampleResource) ModelObject() interface{} {\nreturn &amp;ResourceGroupExampleResourceModel{}\n}\n\nfunc (ResourceGroupExampleResource) ResourceType() string {\nreturn \"azurerm_resource_group_example\"\n}\n</code></pre> <p>In this case we're using the resource type <code>azurerm_resource_group_example</code> as an existing Resource for <code>azurerm_resource_group</code> exists and the names need to be unique.</p> <p>These functions define a Resource called <code>azurerm_resource_group_example</code>, which has two Required arguments (<code>name</code> and <code>location</code>) and one Optional argument (<code>tags</code>). We'll come back to <code>ModelObject</code> later.</p> <p>Schema fields should be ordered as follows:</p> <ol> <li>Any fields that make up the resource's ID, with the last user specified segment (usually the resource's name) first. (e.g. <code>name</code> then <code>resource_group_name</code>, or <code>name</code> then <code>parent_resource_id</code>)</li> <li>The <code>location</code> field.</li> <li>Required fields, sorted alphabetically.</li> <li>Optional fields, sorted alphabetically.</li> <li>Computed fields, sorted alphabetically. (Although in a typed resource these are always added within the <code>Attributes</code> method)</li> </ol> <p>Let's start by implementing the Create function:</p> <pre><code>func (r ResourceGroupExampleResource) Create() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\n// the Timeout is how long Terraform should wait for this function to run before returning an error\n// whilst 30 minutes may initially seem excessive, we set this as a default to account for rate\n// limiting - but having this here means that users can override this in their config as necessary\nTimeout: 30 * time.Minute,\n\n// the Func returns a function which retrieves the current state of the Resource Group into the state\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\nclient := metadata.Client.Resource.GroupsClient\n\n// retrieve the Name for this Resource Group from the Terraform Config\n// and then create a Resource ID for this Resource Group\n// using the Subscription ID &amp; name\nsubscriptionId := metadata.Client.Account.SubscriptionId\n\nvar config ResourceGroupExampleResourceModel\nif err := metadata.Decode(&amp;config); err != nil {\nreturn fmt.Errorf(\"decoding: %+v\", err)\n}\nid := resources.NewResourceGroupID(subscriptionId, config.Name)\n\n// then we want to check for the presence of an existing resource with the resource's ID\n// this is because the Azure API uses the `name` as a unique idenfitier and Upserts\n// so we don't want to unintentionally adopt this resource by using the same name\nexisting, err := client.Get(ctx, id)\nif err != nil &amp;&amp; !response.WasNotFound(existing.HttpResponse) {\nreturn fmt.Errorf(\"checking for presence of existing %s: %+v\", id, err)\n}\nif !response.WasNotFound(existing.HttpResponse) {\nreturn metadata.ResourceRequiresImport(r.ResourceType(), id)\n}\n\n// create the Resource Group\nparam := resources.Group{\nLocation: pointer.To(location.Normalize(config.Location)),\nTags:     pointer.To(config.Tags),\n}\nif _, err := client.CreateOrUpdate(ctx, id, param); err != nil {\nreturn fmt.Errorf(\"creating %s: %+v\", id, err)\n}\n\n// set the Resource ID, meaning that we track this resource\nmetadata.SetID(id)\nreturn nil\n},\n}\n}\n</code></pre> <p>Let's implement the Update function:</p> <pre><code>func (r ResourceGroupExampleResource) Update() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\n// the Timeout is how long Terraform should wait for this function to run before returning an error\n// whilst 30 minutes may initially seem excessive, we set this as a default to account for rate\n// limiting - but having this here means that users can override this in their config as necessary\nTimeout: 30 * time.Minute,\n\n// the Func returns a function which retrieves the current state of the Resource Group into the state\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\nclient := metadata.Client.Resource.GroupsClient\n\n// parse the existing Resource ID from the State\nid, err := resources.ParseResourceGroupID(metadata.ResourceData.Id())\nif err != nil {\nreturn err\n}\n\nvar config ResourceGroupExampleResourceModel\nif err := metadata.Decode(&amp;config); err != nil {\nreturn fmt.Errorf(\"decoding: %+v\", err)\n}\n// update the Resource Group\n// NOTE: for a more complex resource we'd recommend retrieving the existing Resource from the\n// API and then conditionally updating it when fields in the config have been updated, which\n// can be determined by using `metadata.ResourceData.HasChange` - for example:\n//\n//   existing, err := client.Get(ctx, *id)\n//   if err != nil {\n//     return fmt.Errorf(\"retrieving %s: %+v\", *id, err)\n//   }\n//\n// Although the SDK will catch and error in cases where Model is nil we should still check for\n// a non-nil Model and nested Properties object to prevent panics\n//   if existing.Model == nil {\n//      return fmt.Errorf(\"retrieving %s: `model` was nil\", id)\n//   }\n//   if existing.Model.Properties == nil {\n//      return fmt.Errorf(\"retrieving %s: `properties` was nil\", id)\n//   }\n//\n//   if metadata.ResourceData.HasChange(\"tags\") {\n//     existing.Model.Properties.Tags = tags.Expand(config.Tags)\n//   }\n//\n// doing so allows users to take advantage of Terraform's `ignore_changes` functionality.\n//\n// However since a Resource Group only has one field which is updatable (tags) we'll only\n// enter the update function if `tags` has been updated.\nparam := resources.Group{\nLocation: pointer.To(location.Normalize(config.Location)),\nTags:     pointer.To(config.Tags),\n}\nif _, err := client.CreateOrUpdate(ctx, *id, param); err != nil {\nreturn fmt.Errorf(\"updating %s: %+v\", id, err)\n}\n\nreturn nil\n// The Update function in **untyped** resources should return `Read()`\n},\n}\n}\n</code></pre> <p>Next up, let's implement the Read function - which retrieves the information about the Resource Group from Azure:</p> <pre><code>func (ResourceGroupExampleResource) Read() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\n// the Timeout is how long Terraform should wait for this function to run before returning an error\n// whilst 5 minutes may initially seem excessive, we set this as a default to account for rate\n// limiting - but having this here means that users can override this in their config as necessary\nTimeout: 5 * time.Minute,\n\n// the Func returns a function which looks up the state of the Resource Group and sets it into the state\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\nclient := metadata.Client.Resource.GroupsClient\n\n// parse the Resource Group ID from the `id` field\nid, err := resources.ParseResourceGroupID(metadata.ResourceData.Id())\nif err != nil {\nreturn err\n}\n\n// then retrieve the Resource Group by its ID\nresp, err := client.Get(ctx, *id)\nif err != nil {\n// if the Resource Group doesn't exist (e.g. we get a 404 Not Found)\n// since this is a Resource (e.g. we created it/it was imported into the state)\n// it previously existed - so we must mark this as \"gone\" for Terraform\nif response.WasNotFound(resp.HttpResponse) {\nreturn metadata.MarkAsGone(id)\n}\n\n// otherwise it's a genuine error (auth/api error etc) so raise it\n// there should be enough context for the user to interpret the error\n// or raise a bug report if there's something we should handle\nreturn fmt.Errorf(\"retrieving %s: %+v\", id, err)\n}\n\n// at this point we can set information about this Resource Group into the State\n// identifier fields such as the name, resource group name to name a few need to be sourced\n// from the Resource ID instead of the API response\nstate := ResourceGroupExampleResourceModel{\nName: id.ResourceGroupName,\n}\n\n// the SDK will return a Model as well as a nested Properties object for the resource\n// for readability and consistency we assign the Model to a variable and nil check as shown below.\n// since the SDK accounts for responses where the Model is nil we do not need to throw an error if\n// the Model is nil since this will be caught earlier on. We still nil check to prevent the provider from\n// crashing.\nif model := resp.Model; model != nil {\n// the Location field is a little different - and we have a normalization\n// function for this.\n\n// whilst this may seem like a weird thing to call out in an example, because these two fields\n// are present on the majority of resources, we hope it explains why they're a little different\n\n// in this case the Location can be returned in various different forms, for example\n// \"West Europe\", \"WestEurope\" or \"westeurope\" - as such we normalize these into a\n// lower-cased singular word with no spaces (e.g. \"westeurope\") so this is consistent\n// for users\nstate.Location = location.NormalizeNilable(model.Location)\nstate.Tags = pointer.From(model.Tags)\nif props := model.Properties; props != nil {\n// if there are properties to set into state do that here\n}\n}\nreturn metadata.Encode(&amp;state)\n},\n}\n}\n</code></pre> <p>Next we can add the Delete function:</p> <pre><code>func (ResourceGroupExampleResource) Delete() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\n// the Timeout is how long Terraform should wait for this function to run before returning an error\n// whilst 30 minutes may initially seem excessive, it can take a while to delete the nested items\n// particularly if we're rate-limited - but users can override this in their config as necessary\nTimeout: 30 * time.Minute,\n\n// the Func returns a function which deletes the Resource Group\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\nclient := metadata.Client.Resource.GroupsClient\n\nid, err := resources.ParseResourceGroupID(metadata.ResourceData.Id())\nif err != nil {\nreturn err\n}\n\n// trigger the deletion of the Resource Group\n// Delete calls that require request options can be populated by the `DefaultDeleteOperationOptions()`\n// method in the SDK\nif err := client.DeleteThenPoll(ctx, *id, resources.DefaultDeleteOperationOptions()); err != nil {\nreturn fmt.Errorf(\"deleting %s: %+v\", *id, err)\n}\nreturn nil\n},\n}\n}\n</code></pre> <p>Finally we can add the <code>IDValidationFunc</code> function:</p> <pre><code>func (ResourceGroupExampleResource) IDValidationFunc() pluginsdk.SchemaValidateFunc {\nreturn resources.ValidateResourceGroupID\n}\n</code></pre> <p>At this point the finished Resource should look like (including imports):</p> <pre><code>package resource\n\nimport (\n\"context\"\n\"fmt\"\n\"time\"\n\n\"github.com/hashicorp/go-azure-sdk/resource-manager/resources/2022-09-01/resources\"\n\"github.com/hashicorp/go-azure-helpers/resourcemanager/commonschema\"\n\"github.com/hashicorp/go-azure-helpers/resourcemanager/location\"\n\"github.com/hashicorp/go-azure-helpers/resourcemanager/tags\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/tf/pluginsdk\"\n)\n\nvar _ sdk.Resource = ResourceGroupExampleResource{}\n\ntype ResourceGroupExampleResource struct{}\n\ntype ResourceGroupExampleResourceModel struct {\nName     string            `tfschema:\"name\"`\nLocation string            `tfschema:\"location\"`\nTags     map[string]string `tfschema:\"tags\"`\n}\n\nfunc (ResourceGroupExampleResource) Arguments() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n\"name\": {\nType:         pluginsdk.TypeString,\nRequired:     true,\nValidateFunc: validation.StringIsNotEmpty,\n},\n\n\"location\": commonschema.Location(),\n\n\"tags\": commonschema.Tags(),\n}\n}\n\nfunc (ResourceGroupExampleResource) Attributes() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{}\n}\n\nfunc (ResourceGroupExampleResource) ModelObject() interface{} {\nreturn &amp;ResourceGroupExampleResourceModel{}\n}\n\nfunc (ResourceGroupExampleResource) ResourceType() string {\nreturn \"azurerm_resource_group_example\"\n}\n\nfunc (r ResourceGroupExampleResource) Create() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nTimeout: 30 * time.Minute,\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\nclient := metadata.Client.Resource.GroupsClient\nsubscriptionId := metadata.Client.Account.SubscriptionId\n\nvar config ResourceGroupExampleResourceModel\nif err := metadata.Decode(&amp;config); err != nil {\nreturn fmt.Errorf(\"decoding: %+v\", err)\n}\nid := resources.NewResourceGroupID(subscriptionId, config.Name)\n\nexisting, err := client.Get(ctx, id)\nif err != nil &amp;&amp; !response.WasNotFound(existing.HttpResponse) {\nreturn fmt.Errorf(\"checking for presence of existing %s: %+v\", id, err)\n}\nif !response.WasNotFound(existing.HttpResponse) {\nreturn metadata.ResourceRequiresImport(r.ResourceType(), id)\n}\n\nparam := resources.Group{\nLocation: pointer.To(location.Normalize(config.Location)),\nTags:     pointer.To(config.Tags),\n}\nif _, err := client.CreateOrUpdate(ctx, id, param); err != nil {\nreturn fmt.Errorf(\"creating %s: %+v\", id, err)\n}\n\nmetadata.SetID(id)\nreturn nil\n},\n}\n}\n\nfunc (r ResourceGroupExampleResource) Update() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nTimeout: 30 * time.Minute,\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\nclient := metadata.Client.Resource.GroupsClient\n\nid, err := resources.ParseResourceGroupID(metadata.ResourceData.Id())\nif err != nil {\nreturn err\n}\n\nvar config ResourceGroupExampleResourceModel\nif err := metadata.Decode(&amp;config); err != nil {\nreturn fmt.Errorf(\"decoding: %+v\", err)\n}\n\nparam := resources.Group{\nLocation: pointer.To(location.Normalize(config.Location)),\nTags:     pointer.To(config.Tags),\n}\nif _, err := client.CreateOrUpdate(ctx, *id, param); err != nil {\nreturn fmt.Errorf(\"updating %s: %+v\", id, err)\n}\nreturn nil\n},\n}\n}\n\nfunc (ResourceGroupExampleResource) Read() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nTimeout: 5 * time.Minute,\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\nclient := metadata.Client.Resource.GroupsClient\n\nid, err := resources.ParseResourceGroupID(metadata.ResourceData.Id())\nif err != nil {\nreturn err\n}\n\nresp, err := client.Get(ctx, *id)\nif err != nil {\nif response.WasNotFound(resp.HttpResponse) {\nreturn metadata.MarkAsGone(id)\n}\n\nreturn fmt.Errorf(\"retrieving %s: %+v\", id, err)\n}\n\n\nstate := ResourceGroupExampleResourceModel{\nName: id.ResourceGroupName,\n}\n\nif model := resp.Model; model != nil {\nstate.Location = location.NormalizeNilable(model.Location)\nstate.Tags = pointer.From(model.Tags)\nif props := model.Properties; props != nil {\n// if there are properties to set into state do that here\n}\n}\nreturn metadata.Encode(&amp;state)\n},\n}\n}\n\nfunc (ResourceGroupExampleResource) Delete() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nTimeout: 30 * time.Minute,\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\nclient := metadata.Client.Resource.GroupsClient\n\nid, err := resources.ParseResourceGroupID(metadata.ResourceData.Id())\nif err != nil {\nreturn err\n}\n\nif err := client.DeleteThenPoll(ctx, *id, resources.DefaultDeleteOperationOptions()); err != nil {\nreturn fmt.Errorf(\"deleting %s: %+v\", *id, err)\n}\n\nreturn nil\n},\n}\n}\n\nfunc (ResourceGroupExampleResource) IDValidationFunc() pluginsdk.SchemaValidateFunc {\nreturn resources.ValidateResourceGroupID\n}\n</code></pre> <p>Things worth noting here:</p> <ul> <li>In addition to the <code>sdk.Resource</code> interface, we also have other interfaces, such as the <code>sdk.ResourceWithUpdate</code> interface, which includes an <code>update</code> method. Since these interfaces inherit from <code>sdk.Resource</code>, you do not need to redefine the <code>sdk.Resource</code> interface when defining them.</li> </ul> <p>For example, in this case:</p> <p>:white_check_mark: DO</p> <pre><code>var _ sdk.ResourceWithUpdate = ResourceGroupExampleResource{}\n</code></pre> <p>:no_entry: DO NOT</p> <pre><code>var (\n    _ sdk.Resource           = ResourceGroupExampleResource{}\n    _ sdk.ResourceWithUpdate = ResourceGroupExampleResource{}\n)\n</code></pre> <ul> <li>Sometimes, for complex data types like <code>pluginsdk.TypeList</code>, we need to define <code>expand</code> and <code>flatten</code> methods. When defining such methods, please make sure to define them as global methods.</li> </ul> <p>For example, in this case:</p> <p>:white_check_mark: DO</p> <pre><code>func expandComplexResource(input []ComplexResource) *resource.ComplexResource {\n    ...\n}\n</code></pre> <p>:no_entry: DO NOT</p> <pre><code>func (ResourceGroupExampleResource) expandComplexResource(input []ComplexResource) *resource.ComplexResource {\n    ...\n}\n</code></pre> <ul> <li>Historically, we used <code>pluginsdk.StateChangeConf</code> to address certain issues related to LRO APIs. This method has now been deprecated and replaced by custom pollers. Please refer to this example.</li> </ul>"},{"location":"topics/guide-new-resource/#step-4-adding-resource-identity","title":"Step 4: Adding Resource Identity","text":"<p>New resources should add support for Resource Identity, please reference the Resource Identity guide.</p>"},{"location":"topics/guide-new-resource/#step-5-register-the-new-resource","title":"Step 5: Register the new Resource","text":"<p>Resources are registered within the <code>registration.go</code> within each Service Package - and should look something like this:</p> <pre><code>package resource\n\nimport \"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"\n\nvar _ sdk.TypedServiceRegistration = Registration{}\n\ntype Registration struct{}\n\n// ...\n\n// Resources returns a list of Resources supported by this Service\nfunc (r Registration) Resources() []sdk.Resource {\nreturn []sdk.Resource{}\n}\n</code></pre> <p>Note: It's possible that the Service Registration (above) doesn't currently support Typed Resources, in which case you may need to add the following:</p> <pre><code>var _ sdk.TypedServiceRegistration = Registration{}\n\ntype Registration struct {\n}\n\nfunc (Registration) Name() string {\nreturn \"Some Service\"\n}\n\nfunc (Registration) DataSources() []sdk.DataSource {\nreturn []sdk.DataSource{}\n}\n\nfunc (Registration) Resources() []sdk.Resource {\nreturn []sdk.Resource{}\n}\n\nfunc (Registration) WebsiteCategories() []string {\nreturn []string{\n\"Some Service\",\n}\n}\n</code></pre> <p>In this case you'll also need to add a line to register this Service Registration in the list of Typed Service Registrations.</p> <p>To register the Resource we need to add an instance of the struct used for the Resource to the list of Resources, for example:</p> <pre><code>// Resources returns a list of Resources supported by this Service\nfunc (Registration) Resources() []sdk.Resource {\nreturn []sdk.Resource{\nResourceGroupExampleResource{},\n}\n}\n</code></pre> <p>At this point the Resource is registered, as when the Azure Provider builds up a list of supported Resources during initialization, it parses each of the Service Registrations to put together a definitive list of the Resources that we support.</p> <p>This means that if you Build the Provider, at this point you should be able to apply the following Resource:</p> <pre><code>provider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_resource_group_example\" \"test\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n\noutput \"id\" {\n  value = azurerm_resource_group_example.test.id\n}\n</code></pre>"},{"location":"topics/guide-new-resource/#step-6-add-acceptance-tests-for-this-resource","title":"Step 6: Add Acceptance Test(s) for this Resource","text":"<p>We're going to test the Resource that we've just built by dynamically provisioning a Resource Group using the new <code>azurerm_resource_group_example</code> Resource.</p> <p>In Go tests are expected to be in a file name in the format <code>{original_file_name}_test.go</code> - in our case that'd be <code>resource_group_example_resource_test.go</code>, into which we'll want to add:</p> <pre><code>package resource_test\n\nimport (\n\"context\"\n\"fmt\"\n\"testing\"\n\n\"github.com/hashicorp/terraform-provider-azurerm/internal/acceptance\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/acceptance/check\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/clients\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/services/resource/parse\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/tf/pluginsdk\"\n\"github.com/hashicorp/terraform-provider-azurerm/utils\"\n)\n\ntype ResourceGroupExampleTestResource struct{}\n\nfunc TestAccResourceGroupExample_basic(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_resource_group_example\", \"test\")\nr := ResourceGroupExampleTestResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{\nConfig: r.basic(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n})\n}\n\nfunc TestAccResourceGroupExample_requiresImport(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_resource_group_example\", \"test\")\nr := ResourceGroupExampleTestResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{\nConfig: r.basic(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.RequiresImportErrorStep(r.requiresImport),\n})\n}\n\nfunc TestAccResourceGroupExample_complete(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_resource_group_example\", \"test\")\nr := ResourceGroupExampleTestResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{\nConfig: r.complete(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n})\n}\n\nfunc TestAccResourceGroupExample_update(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_resource_group_example\", \"test\")\nr := ResourceGroupExampleTestResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{\nConfig: r.basic(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n{\nConfig: r.complete(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n})\n}\n\nfunc (ResourceGroupExampleTestResource) Exists(ctx context.Context, client *clients.Client, state *pluginsdk.InstanceState) (*bool, error) {\nid, err := resources.ParseResourceGroupID(state.ID)\nif err != nil {\nreturn nil, err\n}\n\nresp, err := client.Resource.GroupsClient.Get(ctx, *id)\nif err != nil {\nreturn nil, fmt.Errorf(\"retrieving %s: %+v\", *id, err)\n}\n\nreturn pointer.To(resp.Model != nil), nil\n}\n\nfunc (ResourceGroupExampleTestResource) basic(data acceptance.TestData) string {\nreturn fmt.Sprintf(`\nprovider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_resource_group_example\" \"test\" {\n  name     = \"acctestRG-%d\"\n  location = \"%s\"\n}\n`, data.RandomInteger, data.Locations.Primary)\n}\n\nfunc (r ResourceGroupExampleTestResource) requiresImport(data acceptance.TestData) string {\nreturn fmt.Sprintf(`\n%s\n\nresource \"azurerm_resource_group_example\" \"import\" {\n  name     = azurerm_resource_group_example.test.name\n  location = azurerm_resource_group_example.test.location\n}\n`, r.basic(data))\n}\n\nfunc (ResourceGroupExampleTestResource) complete(data acceptance.TestData) string {\nreturn fmt.Sprintf(`\nprovider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_resource_group_example\" \"test\" {\n  name     = \"acctestRG-%d\"\n  location = \"%s\"\n\n  tags = {\n    Hello = \"World\"\n  }\n}\n`, data.RandomInteger, data.Locations.Primary)\n}\n</code></pre> <p>There's a more detailed breakdown of how this works in the Acceptance Testing reference - but to summarize what's going on here:</p> <ol> <li>Test Terraform Configurations are defined as methods on the struct <code>ResourceGroupExampleResource</code> so that they're easily accessible (this helps to avoid them being unintentionally used in other resources).</li> <li>The <code>acceptance.TestData</code> object contains a number of helpers, including both random integers, strings and the Azure Locations where resources should be provisioned - which are used to ensure when tests are run in parallel that we provision unique resources for testing purposes.</li> <li>The <code>ApplyStep</code>'s apply the Terraform Configuration specified and then assert there's no changes after (e.g. <code>terraform apply</code> and then checking that <code>terraform plan</code> shows no changes).</li> <li>The <code>ImportStep</code> takes the Resource ID for the Resource and runs <code>terraform import azurerm_resource_group_example.test {resourceId}</code>, checking that the fields defined in the state match the fields returned from the Read function.</li> <li>We append <code>_test</code> to the Go package name (e.g. <code>resource_test</code>) since we need to be able to access both the <code>resource</code> package and the <code>acceptance</code> package (which is a circular reference, otherwise).</li> </ol> <p>At this point we should be able to run this test.</p>"},{"location":"topics/guide-new-resource/#step-7-run-the-acceptance-tests","title":"Step 7: Run the Acceptance Test(s)","text":"<p>Detailed instructions on Running the Tests can be found in this guide - when a Service Principal is configured you can run the test above using:</p> <pre><code>make acctests SERVICE='resource' TESTARGS='-run=TestAccResourceGroupExample_' TESTTIMEOUT='60m'\n</code></pre> <p>Note: We're using the test prefix <code>TestAccResourceGroupExample_</code> and not the name of an individual test, but you can do that too by specifying <code>\"(TestName1|TestName2)\"</code> etc</p> <p>Which should output:</p> <pre><code>==&gt; Checking that code complies with gofmt requirements...\n==&gt; Checking that Custom Timeouts are used...\n==&gt; Checking that acceptance test packages are used...\nTF_ACC=1 go test -v ./internal/services/resource -run=TestAccResourceGroupExample_ -timeout 60m -ldflags=\"-X=github.com/hashicorp/terraform-provider-azurerm/version.ProviderVersion=acc\"\n=== RUN   TestAccResourceGroupExample_basic\n=== PAUSE TestAccResourceGroupExample_basic\n=== CONT  TestAccResourceGroupExample_basic\n--- PASS: TestAccResourceGroupExample_basic (88.15s)\n=== RUN   TestAccResourceGroupExample_complete\n=== PAUSE TestAccResourceGroupExample_complete\n=== CONT  TestAccResourceGroupExample_complete\n--- PASS: TestAccResourceGroupExample_complete (120.23s)\n=== RUN   TestAccResourceGroupExample_requiresImport\n=== PAUSE TestAccResourceGroupExample_requiresImport\n=== CONT  TestAccResourceGroupExample_requiresImport\n--- PASS: TestAccResourceGroupExample_requiresImport (116.15s)\nPASS\nok      github.com/hashicorp/terraform-provider-azurerm/internal/services/resource  324.753s\n</code></pre>"},{"location":"topics/guide-new-resource/#step-8-add-documentation-for-this-resource","title":"Step 8: Add Documentation for this Resource","text":"<p>At this point in time documentation for each Resource (and Data Source) is written manually, located within the <code>./website</code> folder - in this case this will be located at <code>./website/docs/d/resource_group_example.html.markdown</code>.</p> <p>There is a tool within the repository to help scaffold the documentation for a Resource - the documentation for this Resource can be scaffolded via the following command:</p> <pre><code>$ make scaffold-website BRAND_NAME=\"Resource Group Example\" RESOURCE_NAME=\"azurerm_resource_group_example\" RESOURCE_TYPE=\"resource\" RESOURCE_ID=\"/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/group1\"\n</code></pre> <p>The documentation should look something like below - containing both an example usage and the required, optional and computed fields:</p> <pre><code>---\nsubcategory: \"Base\"\nlayout: \"azurerm\"\npage_title: \"Azure Resource Manager: azurerm_resource_group_example\"\ndescription: |-\n  Manages a Resource Group.\n---\n\n# azurerm_resource_group_example\n\nManages a Resource Group.\n\n## Example Usage\n\n```hcl\nresource \"azurerm_resource_group_example\" \"example\" {\n  name     = \"example\"\n  location = \"West Europe\"\n}\n```\n\n## Arguments Reference\n\nThe following arguments are supported:\n\n* `location` - (Required) The Azure Region where the Resource Group should exist. Changing this forces a new Resource Group to be created.\n\n* `name` - (Required) The Name which should be used for this Resource Group. Changing this forces a new Resource Group to be created.\n\n---\n\n* `tags` - (Optional) A mapping of tags which should be assigned to the Resource Group.\n\n## Attributes Reference\n\nIn addition to the Arguments listed above - the following Attributes are exported:\n\n* `id` - The ID of the Resource Group.\n\n## Timeouts\n\nThe `timeouts` block allows you to specify [timeouts](https://developer.hashicorp.com/terraform/language/resources/configure#define-operation-timeouts) for certain actions:\n\n* `create` - (Defaults to 30 minutes) Used when creating the Resource Group.\n* `read` - (Defaults to 5 minutes) Used when retrieving the Resource Group.\n* `update` - (Defaults to 30 minutes) Used when updating the Resource Group.\n* `delete` - (Defaults to 30 minutes) Used when deleting the Resource Group.\n\n## Import\n\nResource Groups can be imported using the `resource id`, e.g.\n\n```shell\nterraform import azurerm_resource_group_example.example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/example\n```\n</code></pre>"},{"location":"topics/guide-new-resource/#step-9-send-the-pull-request","title":"Step 9: Send the Pull Request","text":"<p>See our recommendations for opening a Pull Request.</p>"},{"location":"topics/guide-new-service-package/","title":"Guide: New Service Package","text":"<p>There's a few steps involved in adding a new Service Package.</p> <ol> <li> <p>Create a new directory within <code>./internal/services</code> with the Service Name (see naming).</p> </li> <li> <p>Create an empty Client within the Service Package (<code>./internal/services/{name}/client/client.go</code>):</p> </li> </ol> <pre><code>package client\n\nimport (\n\"github.com/hashicorp/terraform-provider-azurerm/internal/common\"\n)\n\ntype Client struct {\n}\n\nfunc NewClient(o *common.ClientOptions) *Client {\nreturn &amp;Client{}\n}\n</code></pre> <ol> <li>Create an empty Registration within the Service Package (<code>./internal/services/{name}/registration.go</code>) which implements the <code>TypedServiceRegistration</code> interface:</li> </ol> <pre><code>package {name}\n\nimport (\n\"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"\n)\n\ntype Registration struct{}\n\nvar (\n_ sdk.TypedServiceRegistration = Registration{}\n)\n\nfunc (r Registration) DataSources() []sdk.DataSource {\nreturn []sdk.DataSource{}\n}\n\nfunc (r Registration) Resources() []sdk.Resource {\nreturn []sdk.Resource{}\n}\n\n// Name is the name of this Service\nfunc (r Registration) Name() string {\nreturn \"App Service\"\n}\n\n// WebsiteCategories returns a list of categories which can be used for the sidebar\nfunc (r Registration) WebsiteCategories() []string {\nreturn []string{\n\"App Service\",\n}\n}\n</code></pre> <ol> <li>Register the Service Registration.</li> <li>Define and Register the Client for this Service Package.</li> <li>Add this to the Client struct.</li> <li>Call the Register function.</li> <li>Re-run the generation to ensure the generated files are up to date (<code>make generate</code>).</li> </ol> <p>At this point the Service Package should be registered, and you can build a new Data Source or a new Resource as required.</p>"},{"location":"topics/guide-new-write-only-attribute/","title":"Guide: Adding a new Write-Only Attribute","text":"<p>This guide covers how to add a new Write-Only (WO) Attribute to a resource. A WO Attribute can accept ephemeral values and is never persisted in state. </p> <p>Note: Write-Only Attributes are only available in Terraform version 1.11 or higher.</p> <p>Good candidates for WO Attributes are sensitive user supplied properties, e.g. passwords, certificates, and keys, can be added in addition to an existing sensitive property.</p> <p>There are however limitations on what can be added as a WO Attribute, the original sensitive property: * Cannot be <code>ForceNew</code> * Cannot be <code>Computed</code> * Cannot be within a set of nested blocks or set or nested attributes * Cannot be a block (list or set) or a map * Cannot be used in data sources or the provider schemas</p> <p>Adding a new WO Attribute consists of the following steps:</p> <ol> <li>Updating the Resource Schema</li> <li>Updating the Create, Read and Update functions</li> <li>Adding Validation to prefer the WO Attribute over the non-WO Sensitive Attribute</li> <li>Adding the Tests</li> <li>Updating the Documentation</li> </ol> <p>In the steps outlined above we're going to look at a fictional resource called <code>azurerm_some_database</code> that has an existing sensitive property called <code>password</code> for which we're going to add a WO attribute.</p>"},{"location":"topics/guide-new-write-only-attribute/#updating-the-resource-schema","title":"Updating the Resource Schema","text":"<p>A new WO attribute must be accompanied by the addition of a regular attribute whose presence and value is used to determine when the value of a WO Attribute has changed and signals the provider to send the value of the WO attribute.</p> <p>As a result we add two new properties to the schema, <code>password_wo</code> and <code>password_wo_version</code>.</p> <pre><code>... // omitted for brevity\n\n\"password\": {\nType:          pluginsdk.TypeString,\nOptional:      true,\nSensitive:     true,\nConflictsWith: []string{\"password_wo\"} // this must be set to prevent both the sensitive `password` and the wo attribute `password_wo` from being set\n},\n\n\"password_wo\": {\nType:          pluginsdk.TypeString, Optional:      true,\nWriteOnly:     true, RequiredWith:  []string{\"password_wo_version\"} // this must be set to ensure the \"trigger\" property is provided with the wo attribute \nConflictsWith: []string{\"password_wo_version\"} // this must be set to prevent both the sensitive `password` and the wo attribute `password_wo` from being set\n}\n\n\"password_wo_version\": {\nType:         pluginsdk.TypeInt,\nOptional:     true,\nRequiredWith: []string{\"password_wo\"} // this must be set to ensure the \"trigger\" property is provided with the wo attribute\n}\n\n... // omitted for brevity\n</code></pre>"},{"location":"topics/guide-new-write-only-attribute/#updating-the-create-read-and-update-functions","title":"Updating the Create, Read and Update functions","text":"<p>In the <code>Create</code> function we make use of the helper function <code>pluginsdk.GetWriteOnly</code> to retrieve the WO attribute.</p> <pre><code>func (SomeDatabase) Create() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\n... // omitted for brevity\n\n// use the GetWriteOnly helper to retrieve the WO attribute\nwoPassword, err := pluginsdk.GetWriteOnly(metadata.ResourceData, \"password_wo\", cty.String)\nif err != nil {\nreturn err\n}\n\n// set it in the payload if the WO attribute is not null\nif !woPassword.IsNull() {\npayload.Properties.Password = woPassword.AsString()\n}\n\n... // omitted for brevity\n}\n}\n}\n</code></pre> <p>The only update in the <code>Read</code> function is to retrieve the value for the WO attribute's trigger property <code>password_wo_version</code> and to set that into state.</p> <pre><code>func (SomeDatabase) Read() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\n... // omitted for brevity\n\n// since WO attributes are not persisted in state we do not need to write it back to state\n// but we do need to retrieve the value for the trigger attribute from the config and set that into\n// state to prevent a perma diff\nstate.PasswordWOVersion = metadata.ResourceData.Get(\"password_wo_version\").(int)\n\n... // omitted for brevity\n}\n}\n}\n</code></pre> <p>In the <code>Update</code> function we rely on changes to the trigger attribute <code>password_wo_version</code> to know when to update the WO attribute.</p> <pre><code>func (SomeDatabase) Update() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\n... // omitted for brevity\n\n// check if the trigger attribute has any changes\nif metadata.ResourceData.HasChange(\"password_wo_version\") {\nwoPassword, err := pluginsdk.GetWriteOnly(metadata.ResourceData, \"password_wo\", cty.String)\nif err != nil {\nreturn err\n}\n\n// set it in the payload if the WO attribute is not null\nif !woPassword.IsNull() {\npayload.Properties.Password = woPassword.AsString()\n}\n}\n\n... // omitted for brevity\n}\n}\n}\n</code></pre>"},{"location":"topics/guide-new-write-only-attribute/#adding-validation","title":"Adding Validation","text":"<p>The <code>terraform-plugin-sdk@v2</code> provides a helpful validation for WO attributes that surfaces a warning to users if they are on a version of Terraform that supports WO attributes but are using the non-WO attribute version of a sensitive property.</p> <p>Note: We currently recommend not adding this validation to the resource since the only way to remove the warning diagnostic is to use to WO attribute.</p> <pre><code>// update the interface that the resource should implement\nvar _ sdk.ResourceWithConfigValidation = SomeDatabase{}\n\n// add the config validation method to satisfy the new interface\nfunc (r SomeDatabase) ValidateRawResourceConfig() []schema.ValidateRawResourceConfigFunc {\nreturn []schema.ValidateRawResourceConfigFunc{\npluginSdkValidation.PreferWriteOnlyAttribute(cty.GetAttrPath(\"password\"), cty.GetAttrPath(\"password_wo\")),\n}\n}\n\n... // omitted for brevity\n</code></pre>"},{"location":"topics/guide-new-write-only-attribute/#adding-tests","title":"Adding Tests","text":"<p>To cover our bases we should test the following paths for a WO attribute: * Creating a resource with the WO attribute * Updating a resource with the WO attribute * Updating a resource that uses the original sensitive property to the WO attribute * Updating a resource that uses the WO attribute back to the original sensitive property</p> <p>These paths can be tested by the addition of two test cases:</p> <pre><code>func TestAccSomeDatabase_writeOnlyPassword(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_some_database\", \"test\")\nr := SomeDatabaseResource{}\n\nresource.ParallelTest(t, resource.TestCase{\nTerraformVersionChecks: []tfversion.TerraformVersionCheck{\ntfversion.SkipBelow(version.Must(version.NewVersion(\"1.11.0\"))),\n},\nProtoV5ProviderFactories: framework.ProtoV5ProviderFactoriesInit(context.Background(), \"azurerm\"),\nSteps: []resource.TestStep{\n{\nConfig: r.writeOnlyPassword(data, \"a-secret-from-kv\", 1),\nCheck:  check.That(data.ResourceName).ExistsInAzure(r),\n},\ndata.ImportStep(\"password_wo_version\"),\n{\nConfig: r.writeOnlyPassword(data, \"a-secret-from-kv-updated\", 2),\nCheck:  check.That(data.ResourceName).ExistsInAzure(r),\n},\ndata.ImportStep(\"password_wo_version\"),\n},\n})\n}\n</code></pre> <pre><code>func TestAccSomeDatabase_updateToWriteOnlyPassword(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_some_database\", \"test\")\nr := SomeDatabaseResource{}\n\nresource.ParallelTest(t, resource.TestCase{\nTerraformVersionChecks: []tfversion.TerraformVersionCheck{\ntfversion.SkipBelow(version.Must(version.NewVersion(\"1.11.0\"))),\n},\nProtoV5ProviderFactories: framework.ProtoV5ProviderFactoriesInit(context.Background(), \"azurerm\"),\nSteps: []resource.TestStep{\n{\nConfig: r.basic(data),\nCheck:  check.That(data.ResourceName).ExistsInAzure(r),\n},\ndata.ImportStep(\"password\"),\n{\nConfig: r.writeOnlyPassword(data, \"a-secret-from-kv\", 1),\nCheck:  check.That(data.ResourceName).ExistsInAzure(r),\n},\ndata.ImportStep(\"password\", \"password_wo_version\"),\n{\nConfig: r.basic(data),\nCheck:  check.That(data.ResourceName).ExistsInAzure(r),\n},\ndata.ImportStep(\"password\"),\n},\n})\n}\n</code></pre> <p>To reduce the amount of unnecessary test templating, we should make use of the <code>acceptance.WriteOnlyKeyVaultSecretTemplate</code> test config template which provisions all the necessary dependencies to reference a secret value using the <code>azurerm_key_vault_secret</code> ephemeral resource.</p> <pre><code>func (r SomeDatabaseResource) writeOnlyPassword(data acceptance.TestData, secret string, version int) string {\nreturn fmt.Sprintf(`\n%s\n\n%s\n\nresource \"azurerm_some_database\" \"test\" {\n  name                = \"acctest-db-%[3]d\"\n  resource_group_name = azurerm_resource_group.test.name\n  location            = azurerm_resource_group.test.location\n  login               = \"some_admin_login\"\n  password_wo         = ephemeral.azurerm_key_vault_secret.test.value\n  password_wo_version = %[4]d\n}\n`, r.template(data), acceptance.WriteOnlyKeyVaultSecretTemplate(data, secret), data.RandomInteger, version)\n}\n</code></pre>"},{"location":"topics/guide-new-write-only-attribute/#updating-the-documentation","title":"Updating the Documentation","text":"<p>When documenting WO attributes we specify <code>Write-Only</code> in the parentheses that contains the <code>Required</code> and <code>Optional</code> information.</p> <pre><code>...\n\n* `password` - (Optional) The Password associated with the `login` for the Database.\n\n* `password_wo` - (Optional, Write-Only) The Password associated with the `login` for the Database.\n\n* `password_wo_version` - (Optional) An integer value used to trigger an update for `password_wo`. This property should be incremented when updating `password_wo`.\n\n...\n</code></pre>"},{"location":"topics/guide-opening-a-pr/","title":"Opening a PR","text":"<p>Firstly all contributions are welcome!</p> <p>There is no change too small for us to accept and minor formatting, consistency and documentation PRs are very welcome! However, before making any large or structural changes it is recommended to seek feedback (preferably by reaching out in our community slack) to prevent wasted time and effort. We may already be working on a solution, or have a different direction we would like to take.</p> <p>If you are ever unsure please just reach out, we are more than happy to guide you in the right direction!</p>"},{"location":"topics/guide-opening-a-pr/#considerations","title":"Considerations","text":"<p>As a general rule, the smaller the PR the quicker it's merged - as such when upgrading an SDK and introducing new properties we'd ask that you split that into multiple smaller PR's, for example if you were planning on updating an SDK to add a new resource and update an existing one we would prefer <code>3</code> separate PRs:</p> <ol> <li>Update the Cosmos DB SDK to use API Version <code>2022-02-02</code> from <code>2020-01-01</code>.</li> <li>Add the new property <code>new_feature</code> to the <code>azurerm_cosmosdb_*</code> resources.</li> <li>Introduce the New Resource <code>azurerm_cosmosdb_resource</code>.</li> </ol> <p>We also recommend not opening a PR based on your <code>main</code> branch. By doing this any changed pushed to the PR may inadvertently be also pushed to your <code>main</code> branch without warning.</p> <p>Due to the high volume of PRs on the project and to ensure maintainers are able to focus on changes which are ready to review, please do not open Draft PRs or work that is not yet ready to be reviewed.</p>"},{"location":"topics/guide-opening-a-pr/#process","title":"Process","text":"<p>Pull Requests generally go through a number of phases which vary slightly depending on what's being changed.</p> <p>The following guides cover the more common scenarios we see:</p> <ul> <li>Extending an existing Resource</li> <li>Extending an existing Data Source</li> <li>Adding a new Resource</li> <li>Adding a new Data Source</li> <li>Adding a new Service Package</li> </ul> <p>In general, Pull Requests which add/change either code or SDK's go through the following steps:</p> <ol> <li>Make / commit the changes.</li> <li>Run GitHub Actions linting and checks locally with the make command <code>make pr-check</code>.</li> <li>Run all relevant Acceptance Tests.</li> <li>Open a Pull Request (see below on <code>What makes a good PR?</code>).</li> <li>GitHub actions will trigger and run all linters.</li> <li>A Maintainer will review the PR and also run acceptance tests against our test subscription.</li> <li>Once all comments have been addressed and tests pass the PR will be merged</li> <li>The maintainer will update the CHANGELOG.md.</li> </ol>"},{"location":"topics/guide-opening-a-pr/#what-makes-a-good-pr","title":"What makes a good PR?","text":"<ul> <li>Don't change your forked repo's <code>main</code> branch, instead, make a feature branch.</li> <li>The PR Title is obvious/clear about what it's changing (see <code>Title</code> below).</li> <li>The PR Body contains a summary of what/why is included (see <code>Body</code> below).</li> <li>any linked Issues (see <code>Body</code> below)</li> </ul>"},{"location":"topics/guide-opening-a-pr/#title","title":"Title","text":"<p>The title of the PR should clearly state what the PR is doing, and ideally should match the entry that will end up in the changelog.</p> <p>Examples of good PR titles:</p> <ul> <li><code>azurerm_storage_management_policy - Mark rule.filters.blob_type as required</code></li> <li><code>azurerm_container_registry - support updating replications on demand</code></li> <li><code>azurerm_automation_account - support for the encryption, local_authentication_enabled, and tags properties</code></li> <li><code>Data Source: azurerm_automation_account - prevent panic (#15474) by adding a nil check</code></li> <li><code>Upgrade bot API version from 2021-03-01 to 2021-05-01-preview</code></li> <li><code>New Resource: azurerm_managed_disk_sas_token</code></li> <li><code>New Data Source: azurerm_managed_disk_sas_token</code></li> <li><code>Docs: Fix wrong command in 3.0-upgrade-guide</code></li> </ul> <p>Examples of poorly written PR titles:</p> <ul> <li><code>fix sql bug</code></li> <li><code>fixes #1234</code></li> <li><code>new resource</code></li> <li><code>upgrade sdk</code></li> <li><code>upgrade compute api</code></li> <li><code>add cosmos property</code></li> <li><code>support encryption, local_authentication_enabled properties</code></li> </ul>"},{"location":"topics/guide-opening-a-pr/#body","title":"Body","text":"<p>An example of our PR template is shown below.</p>"},{"location":"topics/guide-opening-a-pr/#community-note","title":"Community Note","text":"<ul> <li>Please vote on this PR by adding a :thumbsup: reaction to the original PR to help the community and maintainers prioritize for review</li> <li>Please do not leave \"+1\" or \"me too\" comments, they generate extra noise for PR followers and do not help prioritize for review</li> </ul> <p>#### PR Checklist</p> <ul> <li>[ ] Have you followed the guidelines in our Contributing Documentation?</li> <li>[ ] Have you checked to ensure there aren't other open Pull Requests for the same update/change?</li> <li>[ ] Have you used a meaningful PR description to help maintainers and other users understand this change and help prevent duplicate work? Example:  \u201c<code>resource_name_here</code> - description of change e.g. adding property <code>new_property_name_here</code>\u201d</li> <li>[ ] Do your changes close any open issues? If so please include appropriate closing keywords below.</li> </ul>"},{"location":"topics/guide-opening-a-pr/#new-feature-submissions","title":"New Feature Submissions","text":"<ul> <li>[ ] Does your submission include Test coverage as described in the Contribution Guide and the tests pass? (if this is not possible for any reason, please include details of why below)</li> </ul>"},{"location":"topics/guide-opening-a-pr/#changes-to-existing-resource-data-source","title":"Changes to existing Resource / Data Source","text":"<ul> <li>[ ] Have you added an explanation of what your changes do and why you'd like us to include them? (This may be covered by linking to an issue above, but may benefit from additional explanation)</li> <li>[ ] Have you written new tests for your resource or datasource changes?</li> <li>[ ] Have you successfully run tests with your changes locally? If not, please provide details on testing challenges that prevented you running the tests.</li> </ul>"},{"location":"topics/guide-opening-a-pr/#documentation-changes","title":"Documentation Changes","text":"<ul> <li>[ ] Documentation is written in International English.</li> <li>[ ] Documentation is written in a helpful and kind way to assist users that may be unfamiliar with the resource / data source.</li> </ul>"},{"location":"topics/guide-opening-a-pr/#description","title":"Description","text":""},{"location":"topics/guide-opening-a-pr/#related-issues","title":"Related Issue(s)","text":"<p>Use linking keywords here like \"fixes\", \"closes\", \"resolves\", etc:  <pre><code>Fixes #1234, fixes #5678, fixes #9101\n</code></pre></p>"},{"location":"topics/guide-opening-a-pr/#change-log","title":"Change Log","text":"<p>Changelog Format</p> <ul> <li><code>azurerm_resource</code> - support for the <code>thing1</code> property [GH-00000]</li> </ul> <ul> <li>[ ] Bug Fix</li> <li>[ ] New Feature</li> </ul> <p>[!NOTE] If this PR changes meaningfully during the course of review please update the title and description as required.</p>"},{"location":"topics/guide-resource-identity/","title":"Guide: Resource Identity","text":"<p>This guide covers adding Resource Identity to a new or existing resource. For more information on Resource Identity, see Resources - Identity.</p> <p>The provider's Resource Identity generator does not yet support all identity types. <code>commonids.CompositeResourceID</code> and any custom resource IDs (i.e. not one provided by <code>commonids</code> or <code>go-azure-sdk/resource-manager</code>) are not supported.</p>"},{"location":"topics/guide-resource-identity/#adding-resource-identity","title":"Adding Resource Identity","text":""},{"location":"topics/guide-resource-identity/#typed-resources","title":"Typed Resources","text":"<p>To add Resource Identity to a typed resource, we will need to implement the <code>sdk.ResourceWithIdentity</code> interface and modify the <code>Read()</code> function.</p> <ol> <li> <p>Define a variable of type <code>sdk.ResourceWithIdentity</code> and assign it a value of the resource type struct.</p> <pre><code>package example\n\nimport \"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"\n\ntype ExampleResource struct{}\n\nvar _ sdk.ResourceWithIdentity = ExampleResource{}\n</code></pre> </li> <li> <p>Add the <code>Identity()</code> method, this method should return a pointer to the correct resource ID, if you are unsure, you can reference the <code>IDValidationFunc</code> method, the ID that is being validated here is the one you'll want to use.</p> <pre><code>package example\n\nimport \"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"\nimport \"github.com/hashicorp/go-azure-helpers/resourceids\"\n\ntype ExampleResource struct{}\n\nvar _ sdk.ResourceWithIdentity = ExampleResource{}\n\nfunc (r ExampleResource) Identity() resourceids.ResourceId {\nreturn &amp;examplepackage.ExampleResourceId{}\n}\n</code></pre> </li> <li> <p>Update the <code>Read()</code> function to include a step setting the Resource Identity data into state. Resource Identity data does not have to be set manually, we can make use of the <code>pluginsdk.SetResourceIdentityData</code> helper function.</p> <pre><code>func (r ExampleResource) Read() sdk.ResourceFunc {\nreturn sdk.ResourceFunc{\nTimeout: 5 * time.Minute,\nFunc: func(ctx context.Context, metadata sdk.ResourceMetaData) error {\nclient := metadata.Client.Service.ExampleClient\nid, err := examplepackage.ParseExampleResourceID(metadata.ResourceData.Id())\nif err != nil {\nreturn err\n}\n\n...\n\nif err := pluginsdk.SetResourceIdentityData(metadata.ResourceData, id); err != nil {\nreturn err\n}\n\nreturn metadata.Encode(&amp;model)\n},\n}\n}\n</code></pre> </li> <li> <p>Add an acceptance test to ensure the identity data is accurately set into state, please reference Resource Identity Tests.</p> </li> </ol>"},{"location":"topics/guide-resource-identity/#untyped-resources","title":"Untyped Resources","text":"<p>To add Resource Identity to an untyped resource, follow the steps below.</p> <ol> <li> <p>Add the <code>Identity</code> schema. Here, we make use of the <code>pluginsdk.GenerateIdentitySchema</code> function, which takes in a pointer to a <code>resourceids.ResourceId</code>. The ID provided here should be the same as the ID that is being parsed in the <code>Importer</code> field.</p> <pre><code>package example\n\nimport (\n\"github.com/hashicorp/terraform-provider-azurerm/internal/tf/pluginsdk\"\n)\n\nfunc resourceExample() *pluginsdk.Resource {\nreturn &amp;pluginsdk.Resource{\nCreate: resourceExampleCreate,\nRead: resourceExampleRead,\nUpdate: resourceExampleUpdate,\nDelete: resourceExampleDelete,\n\nImporter: pluginsdk.ImporterValidatingResourceId(func(id string) error {\n_, err := examplepackage.ParseExampleID(id)\nreturn err\n}),\n\n// We will be including the new `Identity` field\nIdentity: &amp;schema.ResourceIdentity{\nSchemaFunc: pluginsdk.GenerateIdentitySchema(&amp;examplepackage.ExampleId{}),\n},\n\n...\n}\n}\n</code></pre> </li> <li> <p>Update the <code>Importer</code> field, we'll want to use the <code>pluginsdk.ImporterValidatingIdentity</code> function and provide it with the same resource ID as the <code>pluginsdk.GenerateIdentitySchema</code> function.</p> <pre><code>    package example\n\nimport (\n\"github.com/hashicorp/terraform-provider-azurerm/internal/tf/pluginsdk\"\n)\n\nfunc resourceExample() *pluginsdk.Resource {\nreturn &amp;pluginsdk.Resource{\nCreate: resourceExampleCreate,\nRead: resourceExampleRead,\nUpdate: resourceExampleUpdate,\nDelete: resourceExampleDelete,\n\nImporter: pluginsdk.ImporterValidatingIdentity(&amp;examplepackage.ExampleId{}),\n\n// We will be including the new `Identity` field\nIdentity: &amp;schema.ResourceIdentity{\nSchemaFunc: pluginsdk.GenerateIdentitySchema(&amp;examplepackage.ExampleId{}),\n},\n\n...\n}\n}\n</code></pre> </li> <li> <p>Update the <code>resourceExampleRead</code> function to include a step setting the Resource Identity data into state. Resource Identity data does not have to be set manually, we can make use of the <code>pluginsdk.SetResourceIdentityData</code> helper function.</p> <pre><code>    func resourceExampleRead(d *pluginsdk.ResourceData, meta interface{}) error {\nclient := meta.(*clients.Client).Service.ExampleClient\nctx, cancel := timeouts.ForRead(meta.(*clients.Client).StopContext, d)\ndefer cancel()\n\nid, err := examplepackage.ParseExampleResourceID(d.Id())\nif err != nil {\nreturn err\n}\n\n...\n\n// Most of the time we can simply replace the final `return nil` line with the return below.\n// Note: there are a number of resources that conditionally return earlier in the read function before reaching the final `return nil` line.\n// Keep an eye out for these, as they can cause test failures that are tedious to diagnose.\nreturn pluginsdk.SetResourceIdentityData(d, id)\n}\n</code></pre> </li> <li> <p>Add an acceptance test to ensure the identity data is accurately set into state, please reference Resource Identity Tests.</p> </li> </ol>"},{"location":"topics/guide-resource-identity/#resource-identity-tests","title":"Resource Identity Tests","text":"<p>Just like the schema, Resource Identity tests are entirely generated. This is done by adding a <code>go:generate</code> comment. Both untyped and typed resources use the same format. To make this easy to find and modify, place it underneath the imports.</p> <p>The schema is generated for us by taking different parts of the ID and converting them to snake_case. By default, if the last segment ends in <code>Name</code>, it will not be converted to snake case in the schema but rather set to <code>name</code>. </p> <p>For the tests to generate properly, you will need to specify a combination of <code>-properties</code>, <code>-known-values</code>, and <code>-compare-values</code> inputs. All fields in the ID struct must be mapped to one of these options.</p> <p>To go through these in order:</p> <ul> <li> <p><code>-properties</code>: This flag specifies the 1:1 relationship between the Resource Schema and the Resource Identity Schema fields (i.e name, resource_group_name, etc), this would be specified as <code>name,resource_group_name</code>. If the schema property name does not match the Resource Identity schema name these should be mapped accordingly. This would be specified as <code>{id_field_name}:{schema_field_name}</code>, e.g. <code>api_management_id:api_management_name</code>.</p> </li> <li> <p><code>-known-values</code>: This flag specifies values that are not exposed in the resource schema, but are present in the Resource Identity schema, e.g. a subscription ID. This would be specified as <code>{id_field_name}:{known_value}</code>, e.g. <code>subscription_id:data.Subscriptions.Primary</code>.</p> </li> <li> <p><code>-compare-values</code>: This flag allows for comparing values that are exposed in the resource schema through another resource ID. This comes up when we use a parent resource ID in the schema but the Resource Identity Schema uses the individual parts of that parent ID. This would be specified as <code>{id_field_name}:{schema_field_id_name}</code>, e.g. <code>virtual_network_name:virtual_network_id</code>.</p> </li> </ul> <p>Please reference the Resource Identity Test Generator for additional options that are used less frequently.</p> <p>```go package example</p> <p>import (     \"time\"</p> <pre><code>\"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"\n</code></pre> <p>)</p> <p>// A basic example where the Resource Identity fields map directly to the resource schema //go:generate go run ../../tools/generator-tests resourceidentity -resource-name example_resource -service-package-name example -properties \"name,resource_group_name\" -known-values \"subscription_id:data.Subscriptions.Primary\"</p> <p>// An example where individual Resource Identity field values exist in a parent ID  //go:generate go run ../../tools/generator-tests resourceidentity -resource-name example_resource -service-package-name example -properties \"name\" -compare-values \"parent_name:parent_resource_id\" -known-values \"subscription_id:data.Subscriptions.Primary\"</p> <p>type ExampleResource struct{}</p> <p>var _ sdk.ResourceWithIdentity = ExampleResource{}</p> <p>func (r ExampleResource) Identity() resourceids.ResourceId {     return &amp;examplepackage.ExampleResourceId{} }</p> <p>func (r ExampleResource) Read() sdk.ResourceFunc {     return sdk.ResourceFunc{         Timeout: 5 * time.Minute,         Func: func(ctx context.Context, metadata sdk.ResourceMetaData) error {             client := metadata.Client.Service.ExampleClient             id, err := examplepackage.ParseExampleResourceID(metadata.ResourceData.Id())             if err != nil {                 return err             }</p> <pre><code>        ...\n\n        if err := pluginsdk.SetResourceIdentityData(metadata.ResourceData, id); err != nil {\n            return err\n        }\n\n        return metadata.Encode(&amp;model)\n    },\n}\n</code></pre> <p>}  ```</p>"},{"location":"topics/guide-resource-ids/","title":"Guide: Resource IDs","text":"<p>Resource IDs are an essential component of all resources and data sources within the provider and are required in order to interact with the corresponding API and to be tracked in state correctly by Terraform.</p> <p>Due to their fundamental importance in the provider, resource IDs should be handled through the use of various helper functions that are available.</p>"},{"location":"topics/guide-resource-ids/#resource-id-parsers-and-validators-in-hashicorpgo-azure-sdk","title":"Resource ID Parsers and Validators in <code>hashicorp/go-azure-sdk</code>","text":"<p>The SDK <code>hashicorp/go-azure-sdk</code> used in the provider contains the necessary parsing and validation functions required for a resource ID and will exist in the resource's package.</p> <p>As an example the function to parse a Machine Learning Workspace Resource ID will be accessible by importing the workspace resource package into the provider:</p> <pre><code>import \"github.com/hashicorp/go-azure-sdk/resource-manager/machinelearningservices/2024-04-01/workspaces\"\n\nfunc (r MachineLearningWorkspace) Create() sdk.ResourceFunc {\n...\nid := workspaces.NewWorkspaceID(subscriptionId, resourceGroupName, workspaceName)\n...\n}\n\nfunc (r MachineLearningWorkspace) Read() sdk.ResourceFunc {\n...\nid := workspaces.ParseWorkspaceID(subscriptionId, resourceGroupName, workspaceName)\n...\n}\n</code></pre>"},{"location":"topics/guide-resource-ids/#resource-id-parsers-and-validators-from-hashicorpgo-azure-helpersresourcemanagercommonids","title":"Resource ID Parsers and Validators from <code>hashicorp/go-azure-helpers/resourcemanager/commonids</code>","text":"<p>Some resource types that are referenced across multiple services, will have their parser and validation functions defined in <code>hashicorp/go-azure-helpers</code> <code>commonids</code> package.</p> <p>This is done to avoid having to convert the same ID between different types.</p> <pre><code>import `github.com/hashicorp/go-azure-helpers/resourcemanager/commonids`\n\nfunc (r AppServicePlan) Create() sdk.ResourceFunc {\n...\nid := commonids.NewAppServicePlanID(subscriptionId, resourceGroupName, workspaceName)\n...\n}\n\nfunc (r AppServicePlan) Read() sdk.ResourceFunc {\n...\nid := commonids.ParseAppServicePlanID(subscriptionId, resourceGroupName, workspaceName)\n...\n}\n</code></pre>"},{"location":"topics/guide-resource-ids/#composite-resource-ids","title":"Composite Resource IDs","text":"<p>Resource IDs that consist of a scope ID and a resource ID separated by a <code>|</code> e.g. <code>/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/group1/providers/Microsoft.Network/natGateways/gateway1|/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/mygroup1/providers/Microsoft.Network/publicIPAddresses/myPublicIpAddress1</code> should be handled using the Composite Resource ID functions in the <code>hashicorp/go-azure-helpers</code> <code>commonids</code> package.</p> <pre><code>import `github.com/hashicorp/go-azure-helpers/resourcemanager/commonids`\n\nfunc (r NatGatewayPublicIpAssociation) Create() sdk.ResourceFunc {\n...\n\npublicIpAddressId, err := commonids.ParsePublicIPAddressID(d.Get(\"public_ip_address_id\").(string))\nif err != nil {\nreturn err\n}\n\nnatGatewayId, err := natgateways.ParseNatGatewayID(d.Get(\"nat_gateway_id\").(string))\nif err != nil {\nreturn err\n}\n\nid := commonids.NewCompositeResourceID(natGatewayId, publicIpAddressId)\n...\n}\n\nfunc (r NatGatewayPublicIpAssociation) Read() sdk.ResourceFunc {\n...\nid, err := commonids.ParseCompositeResourceID(d.Id(), &amp;natgateways.NatGatewayId{}, &amp;commonids.PublicIPAddressId{})\nif err != nil {\nreturn err\n}\n...\nd.Set(\"nat_gateway_id\", id.First.ID())\nd.Set(\"public_ip_address_id\", id.Second.ID())\n...\n}\n</code></pre>"},{"location":"topics/guide-resource-ids/#generated-resource-id-parsers-and-validators-legacy","title":"Generated Resource ID Parsers and Validators (legacy)","text":"<p>Prior to generating the parser and validation functions within the SDK, we generated these functions in the provider with this automation which generates the functions for all IDs defined in <code>resourceids.go</code>.</p> <p>An example of this is shown below:</p> <pre><code>package resource\n\n//go:generate go run ../../tools/generator-resource-id/main.go -path=./ -name=ResourceGroupExample -id=/subscriptions/12345678-1234-9876-4563-123456789012/resourceGroups/group1\n</code></pre> <p>In this case, you need to specify the <code>name</code> of the Resource (in this case <code>ResourceGroupExample</code>) and the <code>id</code> which is an example of this Resource ID (in this case <code>/subscriptions/12345678-1234-9876-4563-123456789012/resourceGroups/group1</code>).</p> <p>Running <code>make generate</code> - will output the following files:</p> <ul> <li><code>./internal/service/resource/parse/resource_group_example.go</code> - contains the Resource ID Struct, Formatter and Parser.</li> <li><code>./internal/service/resource/parse/resource_group_example_test.go</code> - contains tests for those ^.</li> <li><code>./internal/service/resource/validate/resource_group_example_id.go</code> - contains Terraform validation functions for the Resource ID.</li> </ul> <p>Note: This is an outdated way of handling resource IDs in the provider and is being phased out. This method should only be used in exceptional cases.</p>"},{"location":"topics/guide-state-migrations/","title":"Guide: State Migrations","text":"<p>State migrations come into play if a resource's implementation needs to change, this can happen for a number a reasons, such as the implementation being incorrect or the API that the resource interacts with changes.</p> <p>Common scenarios where a state migration would be required in Azure are: * To correct the format of a Resource ID, the most common example is updating the casing of a segment e.g. <code>/subscriptions/12345678-1234-9876-4563-123456789012/resourcegroups/resGroup1</code> -&gt; <code>/subscriptions/12345678-1234-9876-4563-123456789012/resourceGroups/resGroup1</code> * Updating the default value of a property in the schema * Recasting property values in the schema, unlike the scenario's above this also requires changes to the user's config, thus should only be in a major version release</p> <p>Note: State migrations are one-way by design meaning they're not backward compatible. Once they've been run you can no longer downgrade to an older version of the provider. Care should be taken when adding state migrations and thorough manual testing should be done. See the section on Testing below.</p>"},{"location":"topics/guide-state-migrations/#conventions-within-the-azurerm-provider","title":"Conventions within the AzureRM Provider","text":"<p>State migrations are service specific and are thus kept under a <code>migration</code> folder of a service e.g.</p> <pre><code>\u251c\u2500\u2500 compute\n\u2502   \u251c\u2500\u2500 client\n\u2502   \u251c\u2500\u2500 migration\n\u2502   \u2502   \u251c\u2500\u2500 managed_disk_v0_to_v1.go\n\u2502   \u251c\u2500\u2500 managed_disk_resource.go\n...\n</code></pre> <p>The migration file follows the naming convention of <code>[resourceName]_[initialVersion]_to_[finalVersion].go</code> e.g. <code>managed_disk_v0_to_v1.go</code></p>"},{"location":"topics/guide-state-migrations/#walkthrough-for-adding-a-state-migration","title":"Walkthrough for adding a state migration","text":"<p>We will step through an example on how to add a state migration for a made up resource, <code>capybara_resource.go</code> in the <code>animals</code> service, where one of the Resource ID segments has been cased incorrectly. The state migration will make the following modification: <code>/subscriptions/12345678-1234-9876-4563-123456789012/resourceGroups/resGroup1/Capybaras/capybara1</code> -&gt; <code>/subscriptions/12345678-1234-9876-4563-123456789012/resourceGroups/resGroup1/capybaras/capybara1</code></p> <ol> <li> <p>Create an empty file under the service's migration folder called <code>capybara_v0_to_v1.go</code> e.g. (e.g. <code>./internal/services/animals/migration/capybara_v0_to_v1.go</code>)</p> </li> <li> <p>The bare minimum required within the file is shown below. Regardless of what the state migration is modifying, <code>Schema()</code> and <code>UpgradeFunc()</code> must be specified since these are referenced by the resource. <pre><code>package migration\n\nimport (\n\"context\"\n\n\"github.com/hashicorp/terraform-provider-azurerm/internal/tf/pluginsdk\"\n)\n\ntype CapybaraV0ToV1 struct{}\n\nfunc (CapybaraV0ToV1) Schema() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n// TODO implement me!\n}\n}\n\nfunc (CapybaraV0ToV1) UpgradeFunc() pluginsdk.StateUpgraderFunc {\nreturn func(ctx context.Context, rawState map[string]interface{}, meta interface{}) (map[string]interface{}, error) {\n// TODO implement me!\nreturn nil, nil\n}\n}\n</code></pre></p> </li> <li> <p>Copy over the schema for <code>capybara_resource.go</code>. If nothing in the schema is changing then this can be copied over 1:1, however you will want to go through and remove some property attributes that are not required.    The <code>Schema()</code> is a point-in-time reference to the Terraform Schema for this Resource at this point - and is used by Terraform to deserialize/serialize the object from the Terraform State. For this reason only a subset of attributes should be defined here (including <code>Type</code>, <code>Required</code>, <code>Optional</code>, <code>Computed</code> and <code>Elem</code> [for maps/lists/sets, including any custom hash functions]) - and the following attributes can be removed from the Schema:</p> </li> <li> <p>Default</p> </li> <li>ValidateFunc</li> <li>ForceNew</li> <li>MaxItems</li> <li>MinItems</li> <li>AtLeastOneOf</li> <li>ConflictsWith</li> <li>ExactlyOneOf</li> <li>RequiredWith</li> </ol> <p>Other caveats to look out for when copying the schema over are:    * in-lining any schema elements which are returned by functions    * removing any if/else logic within the Schema, in most cases this will be feature flags e.g. <code>features.FivePointOh()</code></p> <ol> <li> <p>Fill out the UpgradeFunc to update the Terraform State for this resource. Typically this involves parsing the old Resource ID case-insensitively and then setting the correct casing for the <code>id</code> field (which is what this example assumes) - however note that State Migrations aren't limited to the <code>id</code> field. The file should now look like this: <pre><code>package migration\n\nimport (\n\"context\"\n\"log\"\n\n\"github.com/hashicorp/go-azure-sdk/resource-manager/animals/2023-11-01/capybaras\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/tf/pluginsdk\"\n)\n\ntype CapybaraV0ToV1 struct{}\n\nfunc (s CapybaraV0ToV1) Schema() map[string]*pluginsdk.Schema {\nreturn map[string]*pluginsdk.Schema{\n\"name\": {\nType:     pluginsdk.TypeString,\nRequired: true,\n},\n\n\"cuteness\": {\nType:     pluginsdk.TypeInt,\nRequired: true,\n},\n\n\"pet_names\": {\nType:     pluginsdk.TypeList,\nOptional: true,\nElem: &amp;pluginsdk.Schema{\nType: pluginsdk.TypeString,\n},\n},\n}\n}\n\nfunc (s CapybaraV0ToV1) UpgradeFunc() pluginsdk.StateUpgraderFunc {\nreturn func(ctx context.Context, rawState map[string]interface{}, meta interface{}) (map[string]interface{}, error) {\noldId := rawState[\"id\"].(string)\nparsed, err := capybaras.ParseCapybaraIDInsensitively(oldId)\nif err != nil {\nreturn nil, err\n}\n\nnewId := parsed.ID()\nlog.Printf(\"[DEBUG] Updating ID from %q to %q\", oldId, newId)\nrawState[\"id\"] = newId\nreturn rawState, nil\n}\n}\n</code></pre></p> </li> <li> <p>Finally, we hook the state migration up to the resource. For typed resources this looks like the following <pre><code>package animal\n\nimport (\n\"context\"\n\"fmt\"\n\"time\"\n\n\"github.com/hashicorp/go-azure-sdk/resource-manager/animals/2023-11-01/capybaras\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/sdk\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/services/animals/migration\"\n\"github.com/hashicorp/terraform-provider-azurerm/internal/tf/pluginsdk\"\n)\n\ntype CapybaraResource struct{}\n\nvar (\n_ sdk.ResourceWithStateMigration = CapybaraResource{}\n)\n\ntype CapybaraResourceModel struct {\nName       string   `tfschema:\"name\"`\nCuteness   string   `tfschema:\"cuteness\"`\nPetNames   []string `tfschema:\"pet_names\"`\n\n}\n\nfunc (r CapybaraResource) StateUpgraders() sdk.StateUpgradeData {\nreturn sdk.StateUpgradeData{\nSchemaVersion: 1, // This field references the version which the state migration updates the schema to i.e. v0 -&gt; v1\nUpgraders: map[int]pluginsdk.StateUpgrade{\n0: migration.CapybaraV0ToV1{},\n},\n}\n}\n\n// The rest of the resource e.g. Create/Update/Read/Delete methods have been omitted for brevity\n</code></pre></p> </li> </ol>"},{"location":"topics/guide-state-migrations/#testing","title":"Testing","text":"<p>Currently, no automated testing for state migrations exist since the testing framework is unable to run different versions of the provider simultaneously. As a result testing for state migrations must be done manually and usually involves the following high level steps:</p> <ol> <li>Create the resource using an older version of the provider</li> <li>Locally build a version of the provider containing the state migration</li> <li>Enable development overrides for Terraform</li> <li>Run <code>terraform plan</code> and/or <code>terraform apply</code> using the locally built version of the provider</li> <li>Verify that there are no plan differences</li> </ol>"},{"location":"topics/high-level-overview/","title":"High Level Overview","text":"<p>The AzureRM Provider is a Plugin which is invoked by Terraform (Core) and comprised of Data Sources and Resources.</p> <p>Within the AzureRM Provider, these Data Sources and Resources are grouped into Service Packages - which are logical groupings of Data Sources/Resources based on the Azure Service they're related to.</p> <p>Each of these Data Sources and Resources has both Acceptance Tests and Documentation associated with each Data Source/Resource - the Acceptance Tests are also located within this Service Package, however the Documentation exists within a dedicated folder.</p>"},{"location":"topics/high-level-overview/#project-structure","title":"Project Structure","text":"<p>The Azure Provider is a large codebase which has evolved over time - but tends to follow consistent patterns for the most-part.</p> <p>The Provider is split up into Service Packages (see terminology) - with some other logic sprinkled across several packages.</p> <p>At a high-level, the Provider structure is:</p> Directory/Package Description <code>./examples</code> More complete example usages of Data Sources and Resources offered by this Provider. <code>./helpers</code> This package is deprecated (and so intentionally not documented) - new functionality should instead be added to either the Service Package or go-azure-helpers. <code>./internal/acceptance</code> The Acceptance Test wrappers that we use in the Azure Provider, offering common patterns across the Provider to be reused. <code>./internal/clients</code> Refers to the Client from each Service Package, which is used in Data Sources and Resources to access the Azure APIs. <code>./internal/common</code> Helper functions for registering Clients (for example, setting the user agent, configuring credentials etc.). <code>./internal/features</code> Feature Toggles for Provider functionality and behaviour (for example, enabling Betas or changing a resource type's soft delete or purge protection). This also contains the struct and parsing of/default values for the <code>features</code> block (within the Provider block). <code>./internal/locks</code> Common locking across resources where necessary to workaround API consistency issues. <code>./internal/provider</code> The Provider implementation itself, the Provider schema and a reference to each Service Registration so that Data Sources and Resources can be surfaced within the Provider. <code>./internal/resourceid</code> Helper functions and types for working with Azure Resource IDs. This package is deprecated in favour of <code>github.com/hashicorp/go-azure-helpers/resourcemanager/resourceids</code> and will be removed in the future. <code>./internal/resourceproviders</code> The list of Resource Providers which should be auto-registered by the Provider. <code>./internal/sdk</code> The Typed Plugin SDK functionality used in this Provider. <code>./internal/services</code> Packages for each service that the provider supports (e.g. <code>appconfiguration</code>, <code>compute</code>) which contain the Data Sources and Resources supported by the service. <code>./internal/tags</code> Helpers for parsing Tags from the Terraform Configuration and setting Tags into the Terraform State. <code>./internal/tf</code> Helpers and abstractions on top of the Terraform Plugin SDK. <code>./internal/timeouts</code> Helpers for computing the Timeouts for a Data Source / Resource - used in Untyped Data Sources and Untyped Resources. <code>./internal/tools</code> Tooling used to generate functionality within the Provider, for example for Resource IDs and Website Documentation. <code>./scripts</code> Scripts used during testing, linting, and building the provider. <code>./utils</code> Helper functions for converting simple types (e.g. bool/int/strings) to pointers (e.g. <code>pointer.To(\u201csomeValue\u201d)</code>). We intend to deprecate this folder in time and new functionality should be added to individual service packages where possible. The existing functions will be gradually moved (via aliasing) into another repository. <code>./vendor</code> Vendored copies of the go modules the provider uses. For more information please refer to the official Go Documentation. <code>./website</code> Guides and documentation for each resource (in <code>./website/docs/r</code>) and data source (in <code>./website/docs/d</code>) that are published to the Terraform registry. <p>Note: Due to the size of the codebase and open Pull Requests - when functionality is moved we use aliasing to try and avoid breaking open Pull Requests / big-bang migrations. These aliases stick around for a few weeks to allow open PRs to be merged without extra out-of-scope changes - at which point these aliases are removed.</p> <p>Each Service Package consists of (to take <code>appconfiguration</code> as an example):</p> File/Directory Description <code>./services/appconfiguration</code> <code>./client</code> A Client struct, with a reference to any SDK Clients used to access the Azure APIs within this Service Package. <code>./parse</code> Resource ID Formatters and Parsers. <code>./validate</code> Validation functions for this Service Package, including Resource ID Validators. <code>./app_configuration_data_source.go</code> The Data Source <code>azurerm_app_configuration</code>. <code>./app_configuration_data_source_test.go</code> Acceptance tests for the Data Source <code>azurerm_app_configuration</code>. <code>./app_configuration_key_resource.go</code> The Resource <code>azurerm_app_configuration_key</code>. <code>./app_configuration_key_resource_test.go</code> Acceptance Tests for the Resource <code>azurerm_app_configuration_key</code>. <code>./app_configuration_resource.go</code> The Resource <code>azurerm_app_configuration</code>. <code>./app_configuration_resource_test.go</code> Acceptance tests for the Resource <code>azurerm_app_configuration</code>. <code>./registration.go</code> The Service Registration for this Service Package. <p>Some Service Packages may also contain:</p> File/Directory Description <code>./migration</code> Any State Migrations used in Resources. <code>./sdk</code> Any Embedded SDKs used to access the Azure APIs (either Resource Manager or Data Plane). <code>./resourceids.go</code> Used to generate Resource ID Formatters, Parsers and Validators. <ul> <li>Data Sources use the filename format: <code>{name}_data_source.go</code></li> <li>Acceptance Tests for Data Sources use the filename format: <code>{name}_data_source_test.go</code> (note: Golang requires that Tests are contained within a <code>test.go</code> file)</li> <li>Resources use the filename format: <code>{name}_resource.go</code></li> <li>Acceptance Tests for Resources use the filename format: <code>{name}_resource_test.go</code> (note: Golang requires that Tests are contained within a <code>test.go</code> file)</li> </ul> <p>Note: there are a handful of exceptions to these to reduce stuttering (e.g. Resource Provider Registration Resource)</p>"},{"location":"topics/high-level-overview/#types-of-data-sourcesresources-within-the-provider","title":"Types of Data Sources/Resources within the Provider","text":"<p>Whilst the Azure Provider is built on-top of the Terraform Plugin SDK - as this is a large codebase with a number of behavioural similarities across the Provider, we've added an abstraction atop the Terraform Plugin SDK to make development easier.</p> <p>This means that at this point in time, there are four types of Data Source/Resources which can be added in this Provider:</p> <ol> <li>(Untyped) Data Sources (based on the Terraform Plugin SDK) (example).</li> <li>(Untyped) Resources (based on the Terraform Plugin SDK) (example).</li> <li>Typed Data Sources (based on top of the Typed SDK within this Repository) (example).</li> <li>Typed Resources (based on top of the Typed SDK within this Repository) (example).</li> </ol> <p>At this point in time the codebase uses a mixture of both (primarily the Untyped Data Sources/Resources) - in time we plan to migrate across to using Typed Data Sources/Resources instead. For differences between these two patterns, see the Typed vs Untyped guide.</p> <p>Ultimately this approach will allow us to switch from using the Terraform Plugin SDK to Terraform Plugin Framework, enabling us to fix a number of long-standing issues in the Provider - whilst reducing the lines of code needed for each resource.</p>"},{"location":"topics/high-level-overview/#interaction-with-azure","title":"Interaction with Azure","text":"<p>This Provider makes use of a number of SDKs to interact with both the Azure Resource Manager and a number of associated Data Plane APIs, these are:</p> <ul> <li>go-azure-sdk - an opinionated Go SDK generated by Hashicorp for interaction with Azure Resource Manager</li> <li>The Azure SDK for Go - for interaction with Azure Resource Manager (generated from the Swagger files within the Azure/azure-rest-api-specs repository).</li> <li>Hamilton - for interaction with Microsoft Graph - and obtaining an authentication token using MSAL.</li> <li>Giovanni - for interaction with the Azure Storage Data Plane APIs.</li> </ul> <p>There's also a number of Embedded SDKs within the provider for interaction with Resource Manager Services which are not supported by the Azure SDK for Go - generated from the Swagger files within the Azure/azure-rest-api-specs repository.</p> <p>At this point in time, each of the SDKs mentioned above (excluding Hamilton) make use of Azure/go-autorest as a base layer (e.g. for sending requests/responses/handling retries from Azure).</p>"},{"location":"topics/high-level-overview/#testing-the-provider","title":"Testing the Provider","text":"<p>Since the behaviour of the Azure API can change over time, the Provider leans on Acceptance Tests over Unit Tests for asserting that the Data Sources and Resources within the Provider work as expected.</p> <p>More details and guidance on how to test Data Sources/Resources can be found in the Acceptance Testing reference.</p>"},{"location":"topics/maintainer-merging/","title":"Maintainer Specific: Merging Pull Requests","text":"<p>Note: All pull requests must be reviewed and approved before they are merged. </p>"},{"location":"topics/maintainer-merging/#commit-type","title":"Commit Type","text":"<p>All pull requests must be merged using the \"Squash and merge\" option. This ensures a clean commit history and simplifies either reverting changes or cherry-picking commits in the future.</p>"},{"location":"topics/maintainer-merging/#commit-message-format","title":"Commit Message Format","text":"<p>When merging a PR, the commit message should clearly describe the change being introduced. If the PR is correctly named (as described in this guide), then the title can be used as-is. Otherwise, update the title to reflect the purpose of the PR in a way that will be meaningful in the Git history and use that as the message.</p> <p>The commit description can contain an optional changelog entry that if included, will be automatically picked up by the changelog automation system and added to the current draft changelog PR. The format for the commit message can be found in the Automated Changelog Guide section below.</p>"},{"location":"topics/maintainer-merging/#changelog-entry-format","title":"Changelog Entry Format","text":"<p>Note: When sending a Pull Request you should not include a changelog entry as a part of the Pull Request - this is to avoid conflicts. Contributors should not be concerned with updating the changelog as that is something only maintainers will do during merge.</p> <p>When a PR is merged it may or may not be included in the changelog. While most PRs deserve a changelog entry not every change should be included in the changelog as some have no user-facing impact. Some examples of PRs that should not be included are:</p> <ul> <li>Unit and acceptance test fixes</li> <li>Refactoring</li> <li>Documentation changes</li> </ul> <p>Otherwise, every PR that affects users should be added to the appropriate section:</p> <ul> <li><code>FEATURES</code> - new resources and data sources</li> <li><code>ENHANCEMENTS</code> - new properties, functionality, and features (including SDK/API upgrades)</li> <li><code>BUG FIXES</code> - bug fixes</li> </ul> <p>When adding a changelog entry, the following rules should be followed:</p> <ul> <li>Be consistent! Follow the formatting and language of the surrounding entries.</li> <li>Entries should start with a lower case, not end in a period, and always use the serial (oxford) comma.</li> <li>Each resource affected should be listed in full, i.e. do not use something like <code>azurerm_cosmosdb_*</code>.</li> <li>Each entry should link to the pull request with the placeholder <code>[GH-{number}]</code> (e.g. <code>[GH-1234]</code>), this will be replaced with a link during the release process.</li> <li>Entries should read as complete sentences such as <code>add support for the property `new_feature`</code> or <code>improve validation of the property `old_feature`</code> not <code>support `new_feature`</code>.</li> </ul> <p>And finally, when making the edit commit, the PR number should be included in the commit message so the edit is linked to the PR, and the entry from the pr. For example <code>CHANGELOG.md for #1234</code>.</p> <p>Here is a list of common changelog entries and how they should be formatted:</p> <pre><code># X.YY.0 (Unreleased)\n\nFEATURES:\n\n* **New Data Source**: `azurerm_data_source` [GH-12345]\n* **New Resource**: `azurerm_resource` [GH-12345]\n\nENHANCEMENTS:\n\n* dependencies: `go-azure-sdk` - update to `v0.20250101.1123456` [GH-12345]\n* dependencies: `service` - update API version to `2021-12-01` [GH-12345]\n* Data Source: `azurerm_data_source` - export the `value` attribute [GH-12345]\n* `azurerm_resource` - the `sku` property can now be updated to `Basic` or `Standard` without recreating the resource [GH-12345]\n* `azurerm_resource` - add support for the `thing1` property [GH-12345]\n* `azurerm_resource` - add support for the `thing2`, `thing3`, and `thing4` properties [GH-12345]\n* `azurerm_resource` - improve validation for the `timeout` property within the `termination_notification` block [GH-12345]\n\nBUG FIXES:\n\n* Data Source: `azurerm_data_source` - prevent a possible crash by setting `queue_name` correctly [GH-12345]\n* Data Source: `azurerm_data_source` - correctly populate the `kind` and `os_type` attributes [GH-12345]\n* `azurerm_data_factory_dataset_delimited_text` - set defaults properly for `column_delimiter`, `quote_character`, `escape_character`, `first_row_as_header`, and `null_value` [GH-12345]\n* `azurerm_linux_function_app` - correctly deduplicate user `app_settings` [GH-12345]\n* `azurerm_windows_function_app_slot` - correctly deduplicate user `app_settings` [GH-12345]\n</code></pre>"},{"location":"topics/maintainer-merging/#automated-changelog-guide","title":"Automated Changelog Guide","text":"<p>For maintainers, when reviewing and merging a PR that warrants a changelog entry, the changelog automation flow is documented below.</p> <p>In the Extended description box of the merge commit message type the changelog entry. </p> <p>Example: <code>[BUG] * Data Source: `azurerm_data_source` - prevent a possible crash by setting `queue_name` correctly</code></p> <p>The Github PR number (like <code>[GH-12345]</code>) will be appended by the automation.</p> <p>The options for the automation are:</p> <ul> <li> <p><code>[BUG]</code></p> </li> <li> <p><code>[ENHANCEMENT]</code></p> </li> <li> <p><code>[FEATURE]</code></p> </li> </ul> <p>Note: Breaking changes need to be added manually to the open changelog PR by editing the branch the changelog PR is open on. </p> <p>After pressing <code>Confirm squash and merge</code>, the automation will kick off. </p> <ol> <li> <p>It will pull the merge commit message and append the PR number <code>[GH-{number}]</code> </p> </li> <li> <p>It will check for the keywords <code>[BUG]</code>, <code>[ENHANCEMENT]</code>, <code>[FEATURE]</code></p> </li> <li> <p>If a keyword is used, a changelog entry will be made</p> </li> <li> <p>It will check if there is an existing Changelog PR open for the release, by checking for an open PR with the label changelog</p> </li> <li> <p>If there is not an open PR it will open a new changelog PR</p> </li> <li> <p>It will open the PR on branch automated-changelog</p> </li> <li> <p>It will add the label changelog</p> </li> <li> <p>It will format the changelog to have new Enhancements, Features, and Bug Fixes headers and the release number</p> </li> <li> <p>It will title itself \"CHANGELOG.md for $RELEASENUM\" based on the next release numbers minor version (will need to be manually adjusted for hot fixes and major releases)</p> </li> <li> <p>If a PR is already open, or has now been opened, it will add the changelog entry under the appropriate header </p> </li> <li> <p>It will push the change to the open Changelog PR</p> </li> </ol>"},{"location":"topics/reference-acceptance-testing/","title":"Acceptance Testing","text":"<p>Acceptance tests are an essential part of the provider - they provide confidence in the functionality and consistency of resources and data sources as they are introduced and over time.</p> <p>Whilst we can't test every use-case or permutation of fields - each data source/resource gets a common set of tests to ensure the core use-cases are covered.</p> <p>As a general rule, the more complex the resource the more tests there are - for example AKS, App Service and Virtual Machines all have a large number of end-to-end tests.</p>"},{"location":"topics/reference-acceptance-testing/#considerations","title":"Considerations","text":"<p>Note: Acceptance Tests provision real resources within Azure - which may have an associated charge for each resource.</p> <ul> <li> <p>When selecting SKUs for testing, pick the lowest or cheapest SKU which covers the test - unless there's good reason to otherwise (e.g. some configurations can provision more quickly using one SKU over another).</p> </li> <li> <p>Always put the resource being tested at the end of each configuration, especially if a test requires multiple resource or data declarations. This makes it easy to find the resource being tested, especially in large configurations. Example: note how the <code>azurerm_virtual_machine</code> is positioned at the end of each configuration virtual_machine_resource_test.go</p> </li> </ul>"},{"location":"topics/reference-acceptance-testing/#running-the-tests","title":"Running the Tests","text":"<p>See Running the Tests.</p>"},{"location":"topics/reference-acceptance-testing/#test-package","title":"Test Package","text":"<p>While tests reside in the same folder as resource and data source .go files, they need to be in a separate test package to prevent circular references. i.e. for the file <code>./internal/services/aab2c/aadb2c_directory_data_source_test.go</code> the package should be:</p> <pre><code>package aadb2c_test\n\nimport ...\n</code></pre> <p>This is checked by <code>make test</code> during CI.</p>"},{"location":"topics/reference-acceptance-testing/#import-step","title":"Import Step","text":"<p>During acceptance tests it is important to validate that the resource in Azure matches what Terraform expects and has saved into state. This can be done by adding a <code>data.ImportStep()</code> after every step. This will import the resource into Terraform and compare that the Terraform state matches the Azure Resource.</p> <p>As some properties (such as sensitive data like passwords) are not returned from Azure you can ignore these properties by passing them into the import step: <code>data.ImportStep(\"password\", \"database_primary_key\")</code>.</p>"},{"location":"topics/reference-acceptance-testing/#naming","title":"Naming","text":"<p>Test names should follow the convention <code>TestAcc</code> + <code>ResourceName</code> + <code>_</code> + <code>test</code> -&gt; <code>TestAccExampleResource_basic</code>, or to group tests:</p> <pre><code>func TestAccExampleResource_category_test1(t *testing.T) { ... }\nfunc TestAccExampleResource_category_test2(t *testing.T) { ... }\n</code></pre>"},{"location":"topics/reference-acceptance-testing/#acceptance-tests","title":"Acceptance Tests","text":"<p>The Acceptance Tests for both Data Sources and Resources within this Provider use a Go struct for each test, in the form <code>{Name}{DataSource|Resource}</code>, for example:</p> <pre><code>// for a data source named Example:\ntype ExampleDataSource struct {}\n\n// for a resource named Example:\ntype ExampleResource struct {}\n</code></pre> <p>They are differentiated from the implementation's struct by their package, which is the same as the implementation's but with a <code>_test</code> suffix. This allows the test configurations to be scoped (and not used unintentionally across different resources), for example a Resource may look like this:</p> <pre><code>package example_test\n\ntype ExampleResource struct {}\n\nfunc (ExampleResource) basic(data acceptance.TestData) string {\nreturn fmt.Sprintf(`\nprovider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_example_resource\" \"example\" {\n  name             = \"my_example_resource\"\n  location         = \"%s\"\n  example_property = \"bar\"\n}\n\n`, data.Locations.Primary)\n}\n</code></pre> <p>This allows the Acceptance Test for each Data Source/Resource to reference that struct and obtain the associated Terraform Configuration as a part of the test e.g.:</p> <pre><code>func TestAccExampleResource_basic(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_example_resource\", \"test\")\nr := ExampleResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{\nConfig: r.basic(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n})\n}\n</code></pre> <p>Originally, the acceptance tests were in the same package as the resource or data source. In order to avoid a name collision, test structs were suffixed with <code>Test</code>. However, moving tests to their own package made the struct suffix superfluous.</p>"},{"location":"topics/reference-acceptance-testing/#which-tests-are-required","title":"Which Tests are Required?","text":"<p>At a minimum, a Data Source requires:</p> <ul> <li>A <code>basic</code> test (Example) - this tests the minimum fields (e.g. all Required fields) for this Data Source.</li> </ul> <p>However, more complex Data Sources can warrant additional acceptance tests - consideration should be given during the development of each Data Source to what's important to be tested.</p> <p>At a minimum, a Resource requires:</p> <ul> <li> <p>A <code>basic</code> test (Example) - this tests the minimum fields (e.g. all Required fields) for this Resource.</p> </li> <li> <p>A <code>requiresImport</code> test (Example) - this test exercises the logic in the <code>create</code> function of a resource that checks for the prior existence of the resource and being created and expects an error. The acceptance test package provides a helper function is provided to be used in the test, called <code>RequiresImportErrorStep</code> for this purpose.</p> </li> <li> <p>A <code>complete</code> test (Example) - this tests all possible fields (e.g. all Required/Optional fields) for this Resource.</p> </li> <li> <p>A <code>update</code> test (Example) - This test exercises a change of values for any properties that can be updated by executing consecutive configurations to change a resource in a predictable manner. Properties which are <code>ForceNew</code> should not be tested in this way.</p> </li> </ul> <p>However, more complex Resource generally warrant additional acceptance tests - consideration should be given during the development of each Resource to what's important to be tested.</p>"},{"location":"topics/reference-acceptance-testing/#example-data-source-basic","title":"Example - Data Source - Basic","text":"<p>A Data Source generally has one or two Required properties and a number of Computed properties - as such it's typical for this test to reuse the Terraform Configuration from the <code>Complete</code> test for the associated Resource (as this exercises all options on the resource).</p> <p>Since the Data Source primarily exposes Computed-only fields which aren't specified in the Terraform Configuration, we typically assert that these computed fields have a/an expected value - which differs from the Acceptance Tests for the Resource where we'll use an Import step to confirm that the Terraform Configuration matches the imported state.</p> <pre><code>func TestAccExampleDataSource_complete(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"data.azurerm_example_resource\", \"test\")\nr := ExampleDataSource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{\nConfig: r.complete(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).Key(\"example_property\").HasValue(\"bar\"),\ncheck.That(data.ResourceName).Key(\"example_optional_bool\").HasValue(\"false\"),\ncheck.That(data.ResourceName).Key(\"example_optional_string\").HasValue(\"foo\"),\n),\n},\ndata.ImportStep(),\n})\n}\n\nfunc (ExampleDataSource) complete(data acceptance.TestData) string {\ntemplate := ExampleResource{}.basic(data)\nreturn fmt.Sprintf(`\n%[1]s\n\ndata \"azurerm_example_resource\" \"test\" {\n  name = azurerm_example_resource.test.name\n}\n`, template)\n}\n</code></pre>"},{"location":"topics/reference-acceptance-testing/#example-resource-basic","title":"Example - Resource - Basic","text":"<p>This test provisions the resource using the minimum configuration possible (e.g. only the <code>Required</code> fields), which is intended to test the happy path (of creating, reading and then destroying a resource).</p> <p>As we're testing the Resource, we make use of an <code>ImportStep</code> as a part of the Acceptance Test to ensure that each of the fields specified as a part of the Terraform Configuration are set into the state.</p> <pre><code>func TestAccExampleResource_basic(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_example_resource\", \"test\")\nr := ExampleResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{\nConfig: r.basic(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n})\n}\n\nfunc (ExampleResource) basic(data acceptance.TestData) string {\nreturn fmt.Sprintf(`\nprovider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_example_resource\" \"example\" {\n  name             = \"my_example_resource\"\n  location         = \"%s\"\n  example_property = \"bar\"\n}\n`, data.Locations.Primary)\n}\n</code></pre>"},{"location":"topics/reference-acceptance-testing/#example-resource-complete","title":"Example - Resource - Complete","text":"<p>This test provisions the resource using the maximum configuration possible (e.g. all <code>Required</code> and <code>Optional</code> fields which can be set together), which is intended to test the more complex scenario for this resource.</p> <p>As we're testing the Resource, we make use of an <code>ImportStep</code> as a part of the Acceptance Test to ensure that each of the fields specified as a part of the Terraform Configuration are set into the state.</p> <pre><code>func TestAccExampleResource_complete(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_example_resource\", \"test\")\nr := ExampleResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{\nConfig: r.complete(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n})\n}\n\nfunc (ExampleResource) complete(data acceptance.TestData) string {\nreturn fmt.Sprintf(`\nprovider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_example_resource\" \"example\" {\n  name             = \"my_example_resource\"\n  location         = \"%s\"\n  example_property = \"bar\"\n\n  example_optional_bool   = false\n  example_optional_string = \"foo\"\n\n  tags = {\n    \"Hello\" = \"World\"\n  }\n}\n\n`, data.Locations.Primary)\n}\n</code></pre>"},{"location":"topics/reference-acceptance-testing/#example-resource-requires-import","title":"Example - Resource - Requires Import","text":"<p>This test is intended to confirm that the logic within the create function (to check for the presence of an existing resource) works as intended - as the Azure Resource Manager API's are Upserts, meaning that without this check it's possible to unintentionally \"adopt\" existing resources.</p> <p>Since this test is attempting to provision the same resource, with the same identifier, twice - this test typically reuses the <code>Basic</code> test as a part of it - interpolating it's values as required.</p> <pre><code>func TestAccExampleResource_basic(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_example_resource\", \"test\")\nr := ExampleResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{\nConfig: r.basic(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.RequiresImportErrorStep(r.requiresImport),\n})\n}\n\nfunc (r ExampleResource) requiresImport(data acceptance.TestData) string {\ntemplate := r.basic(data)  return fmt.Sprintf(`\n%[1]s\n\nresource \"azurerm_example_resource\" \"import\" {\n  name             = azurerm_example_resource.example.name\n  location         = azurerm_example_resource.example.location\n  example_property = azurerm_example_resource.example.example_property\n}\n`, template)\n}\n</code></pre>"},{"location":"topics/reference-acceptance-testing/#example-resource-update","title":"Example - Resource - Update","text":"<p>This test is used to confirm that the <code>Update</code> function of the Resource works - as such only <code>Required</code>/<code>Optional</code> fields which are not <code>ForceNew</code> can be updated.</p> <p>The bare-minimum example for this is provisioning the <code>basic</code> configuration and then updating it using the <code>complete</code> test configuration above, for example:</p> <pre><code>func TestAccExampleResource_update(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_example_resource\", \"test\")\nr := ExampleResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{   // first provision the resource\nConfig: r.basic(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n{   // then perform the update\nConfig: r.complete(data),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n})\n}\n</code></pre> <p>However, this doesn't necessarily cover all use-cases for this resource - or may be too broad depending on the resource, as such it's also common to have tests covering a subset of the fields, for example:</p> <p>Note: This is a simplified example for testing purposes, we'd generally recommend a test covering a related subset of the resource (e.g. enabling/disabling a block within the resource), rather than a single field - but it depends on the resource.</p> <pre><code>func TestAccExampleResource_someSetting(t *testing.T) {\ndata := acceptance.BuildTestData(t, \"azurerm_example_resource\", \"test\")\nr := ExampleResource{}\n\ndata.ResourceTest(t, r, []acceptance.TestStep{\n{   // first provision the resource\nConfig: r.someSetting(data, true),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n{   // then perform the update to disable this setting\nConfig: r.someSetting(data, false),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n{   // finally, check we can re-enable this once it's been disabled\nConfig: r.someSetting(data, true),\nCheck: acceptance.ComposeTestCheckFunc(\ncheck.That(data.ResourceName).ExistsInAzure(r),\n),\n},\ndata.ImportStep(),\n})\n}\n\nfunc (ExampleResource) someSettingEnabled(data acceptance.TestData) string {\nreturn fmt.Sprintf(`\nprovider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_example_resource\" \"example\" {\n  name                 = \"my_example_resource\"\n  location             = \"%s\"\n}\n`, data.Locations.Primary)\n}\n</code></pre>"},{"location":"topics/reference-documentation-standards/","title":"Provider Documentation Standards","text":"<p>In an effort to keep the provider documentation consistent, this page documents some standards that have been agreed on. </p> <p>This page will grow over time, and suggestions are welcome!</p>"},{"location":"topics/reference-documentation-standards/#examples","title":"Examples","text":"<p>Each resource/data source must include an example, general guidelines for examples are as follows:</p> <ul> <li>Examples MUST be functional, i.e. if a user copies the example and runs <code>terraform plan</code> no errors should be returned.</li> <li>Generally the resource instance name should simply be <code>example</code>. E.g. <code>resource \"azurerm_resource_group\" \"example\"</code>.</li> <li>All name arguments within the example configuration should be prefixed with <code>example-</code> (unless this is disallowed by the naming restrictions), avoid overly complex naming, and ensure any naming restrictions are followed. E.g. <code>name = example-server</code>.</li> <li>Avoid multiple examples unless a specific configuration is particularly difficult to configure. If there are many complex examples to document, consider using the <code>examples</code> folder in the repository instead.</li> <li>Examples don't need to include every argument, generally the same configuration as the basic acceptance test will suffice (including any resource dependencies, i.e. the configuration from the template).</li> <li>Resource/Data Source examples should not define a <code>terraform</code> or <code>provider</code> block.</li> </ul>"},{"location":"topics/reference-documentation-standards/#arguments","title":"Arguments","text":""},{"location":"topics/reference-documentation-standards/#ordering","title":"Ordering","text":"<p>Arguments in the documentation are expected to be ordered as follows:</p> <ol> <li>Any arguments that make up the resource's ID, with the last user specified segment (usually <code>name</code>) first. E.g. <code>name</code> then <code>resource_group_name</code>, or <code>name</code> then <code>parent_resource_id</code>.</li> <li>The <code>location</code> field if present.</li> <li>Required arguments, sorted alphabetically.</li> <li>Optional arguments, sorted alphabetically.</li> </ol>"},{"location":"topics/reference-documentation-standards/#descriptions","title":"Descriptions","text":"<p>The following conventions apply to argument descriptions:</p> <ul> <li>Descriptions should be concise, avoid adding too much detail, links to external documentation, etc. If more detail must be added, use a note.</li> <li>If the argument has validation allowing only specific inputs, e.g. <code>validation.StringInSlice()</code>, these must be documented using <code>Possible values are `value1`, `value2`, and `value3.</code>. Other common entries include:</li> <li>Arguments with a single allowed value: <code>The only possible values is `value1`.</code></li> <li>Arguments allowing a range of values, e.g. <code>validation.IntBetween()</code>: <code>Possible values range between `1` and `100`.</code></li> <li>If the argument has a default value, this must be documented using <code>Defaults to `default1`.</code></li> </ul> <p>Examples:</p> <ul> <li><code>* `name` - (Required) The name which should be used for this resource.</code></li> <li><code>* `public_network_access` - (Optional) The public network access setting for this resource. Possible values are `Enabled` and `Disabled`. Defaults to `Enabled`.</code></li> <li><code>* `disk_size_in_gb` - (Optional) The disk size in gigabytes. Possible values range between `4` and `256`.</code></li> </ul>"},{"location":"topics/reference-documentation-standards/#block-arguments","title":"Block Arguments","text":"<p>Block arguments must have two entries in the documentation:</p> <ol> <li>The initial entry, e.g. <code>* `block_argument` - (Optional) A `block_argument` as defined below.</code></li> <li>A subsection, added after all top-level arguments. If multiple blocks are present in the resource, these subsections should be ordered alphabetically. </li> </ol> <p>Example:</p> <pre><code>## Arguments Reference\n\n`name` - (Required) The name which should be used for this resource.\n\n`block_argument` - (Optional) A `block_argument` as defined below.\n\n`some_other_argument` - (Optional) This argument does something magical.\n\n---\n\nA `block_argument` supports the following:\n\n* `nested_argument_1` - (Required) A nested argument that must be specified.\n\n* `nested_argument_2` - (Optional) A nested argument that may be specified.\n\n## Attributes References\n\n...\n</code></pre>"},{"location":"topics/reference-documentation-standards/#attributes","title":"Attributes","text":""},{"location":"topics/reference-documentation-standards/#ordering_1","title":"Ordering","text":"<p>Attributes in the documentation are expected to be ordered as follows:</p> <ol> <li>the <code>id</code> attribute.</li> <li>The remaining attributes, sorted alphabetically</li> </ol>"},{"location":"topics/reference-documentation-standards/#descriptions_1","title":"Descriptions","text":"<p>Attribute descriptions should be concise, and must not include possible or default values.</p>"},{"location":"topics/reference-documentation-standards/#block-attributes","title":"Block Attributes","text":"<p>Block attributes must have two entries in the documentation:</p> <ol> <li>The initial entry, e.g. <code>* `block_attribute` - A `block_attribute` as defined below.</code></li> <li>A subsection, added after all top-level attributes. If multiple blocks are present in the resource, these subsections should be ordered alphabetically. </li> </ol> <p>Example:</p> <pre><code>## Attributes Reference\n\n`id` - The ID of this resource.\n\n`block_attribute` - A `block_attribute` as defined below.\n\n`some_other_attribute` - This attribute returns something magical.\n\n---\n\nA `block_attribute` exports the following:\n\n* `nested_attribute_1` - A very whimsical attribute.\n\n* `nested_attribute_2` - A much more monotonous attribute.\n\n## Timeouts\n\n...\n</code></pre>"},{"location":"topics/reference-documentation-standards/#notes","title":"Notes","text":"<p>Note blocks are used to provide additional information to users beyond the basic description of a resource, argument or attribute.</p> <p>In the past, there have been different approaches to how notes were formatted, some examples are:</p> <ul> <li>Different words to indicate level of importance, e.g. <code>Info</code>, <code>Important</code>, <code>Caution</code>, and <code>Be Aware</code>.</li> <li>Capitalisation differences, e.g. <code>Note:</code> vs <code>NOTE:</code>.</li> <li>Whether or not a colon is included, e.g. <code>Note:</code> vs <code>Note</code>.</li> </ul> <p>Going forward, all notes should follow the exact same format (<code>(-&gt;|~&gt;|!&gt;) **Note:**</code>) where level of importance is indicated through the different types of notes as documented below.</p> <p>Breaking changes have previously been added as notes to the resource documentation. These should no longer be included, instead follow these guidelines:</p> <ul> <li>Breaking changes in a minor version should be added to the top of the changelog.</li> <li>Breaking changes in a major version should be added to the upgrade guide.</li> </ul> <p>We may revisit the guidelines above and/or add a specific place in the documentation for all breaking changes in minor versions.</p>"},{"location":"topics/reference-documentation-standards/#informational-note","title":"Informational Note","text":"<p>Informational note blocks should generally be used when a note provides additional useful information, recommendations and/or tips to the user.</p> <p>To add an informational note, use <code>-&gt; **Note:**</code>, within the Terraform registry documentation this will template as a block with an info icon.</p> <p>For example, extra information on the supported values for an argument, possibly linking to external documentation for the resource/service:</p> <pre><code>* `type` - (Required) The type. Possible values include `This`, `That`, and `Other`.\n\n-&gt; **Note:** More information on each of the supported types can be found in [type documentation](link-to-additional-info)\n</code></pre>"},{"location":"topics/reference-documentation-standards/#warning-note","title":"Warning Note","text":"<p>Warning note blocks should generally be used when a note provides information that the user will need to avoid certain errors, however if these errors are encountered they should not break anything or cause irreversible changes.</p> <p>To add a warning note, use <code>~&gt; **Note:**</code>, within the Terraform registry documentation this will template as a block with a warning icon.</p> <p>For example, an argument that is optional but required when another argument is set to <code>true</code>:</p> <pre><code>* `optional_argument_enabled` - (Optional) Is the optional argument enabled? Defaults to `false`.\n\n* `optional_argument` - (Optional) An optional argument.\n\n~&gt; **Note:** The argument `optional_argument` is required when `optional_argument_enabled` is set to `true`.\n</code></pre>"},{"location":"topics/reference-documentation-standards/#caution-note","title":"Caution Note","text":"<p>Caution note blocks should generally be used when a note provides critical information on potential irreversible changes, data loss or other things that can negatively affect a user's environment.</p> <p>To add a caution note, use <code>!&gt; **Note:**</code>, within the Terraform registry documentation this will template as a block with a caution icon.</p> <p>For example, an argument that when set to <code>true</code> cannot be reversed without recreating the resource:</p> <pre><code>* `irreversible_argument_enabled` - (Optional) Is irreversible argument enabled? Defaults to `false`.\n\n!&gt; **Note:** The argument `irreversible_argument_enabled` cannot be disabled after being enabled.\n</code></pre>"},{"location":"topics/reference-errors/","title":"Working with Errors","text":"<p>Following typical Go conventions, error variables within the AzureRM Provider codebase should be named <code>err</code>, e.g.</p> <pre><code>err := someMethodWhichReturnsAnError(...)\n</code></pre> <p>Or in the case of a method which returns multiple return types:</p> <pre><code>model, err := someMethodWhichReturnsAnObjectAndAnError(...)\n</code></pre> <p>These errors should also be wrapped with more context:</p> <pre><code>func doSomething() error {\nerr := doSomethingWhichCanError()\nif err != nil {\nreturn fmt.Errorf(\"performing somethingWhichCanError: %+v\", err)\n}\nreturn nil\n}\n</code></pre> <p>Since this method only returns an error, we can instead reduce this to:</p> <pre><code>if err := doSomethingWhichCanError(); err != nil {\nreturn fmt.Errorf(\"performing somethingWhichCanError: %+v\", err)\n}\nreturn nil\n</code></pre> <p>Note that when calling code from within a Terraform Data Source/Resource, the Resource ID type (note: not the raw Resource ID) can be used as a formatting argument, for example:</p> <pre><code>id := someResource.NewResourceGroupID(\"subscription-id\", \"my-resource-group\")\nreturn fmt.Errorf(\"deleting %s: %+v\", id, err)\n</code></pre> <p>which will output:</p> <pre><code>deleting Resource Group \"my-resource-group\" (Subscription ID \"subscription-id\"): some error\"\n</code></pre> <p>When parsing existing Resource IDs it is sufficient to return the error as is since all the parsing functions return standardised and descriptive error messages:</p> <pre><code>id, err := someResource.ParseResourceID(state.ID)\nif err != nil {\nreturn err\n}\n</code></pre>"},{"location":"topics/reference-errors/#internal-errors","title":"Internal Errors","text":"<p>Internal errors, which are entirely outside the users control (such as failed expectations) that occur within the provider should be prefixed with <code>internal-error</code>, for example:</p> <pre><code>deadline, ok := ctx.Deadline()\nif !ok {\nreturn fmt.Errorf(\"internal-error: context had no deadline\")\n}\n</code></pre>"},{"location":"topics/reference-errors/#notes","title":"Notes","text":"<p>Error messages should be both short and clear, using the context as relevant - for example use:</p> <ul> <li><code>return fmt.Errorf(\"updating %s: %+v\", id, err)</code></li> <li><code>return fmt.Errorf(\"waiting for %s to finish provisioning: %+v\", id, err)</code></li> <li><code>return fmt.Errorf(\"waiting for %s to finish updating: %+v\", id, err)</code></li> </ul> <p>instead of:</p> <ul> <li><code>return err</code></li> <li><code>return fmt.Errorf(\"failed updating thing: %+v\", err)</code></li> <li><code>return fmt.Errorf(\"something went wrong: %+v\", err)</code></li> </ul> <p>This type of error wrapping should be applied to all error handling including any nested function that contains two or more error checks (e.g., a function that calls an update API and waits for the update to finish or builds an SDK struct) so practitioners and code maintainers have a clear idea which generated the error.</p> <p>Note: Wrapped error messages should generally not start with <code>failed</code>, <code>error</code>, or an uppercase letter as there will a function higher up the stack that will prefix this.</p> <p>When returning errors in those situations, it is important to consider the calling context and to exclude any information the calling function is likely to include, while including any additional context then calling function may not have.</p>"},{"location":"topics/reference-glossary/","title":"Glossary","text":"<p>This document contains a summary of the terminology used within the Azure Provider.</p>"},{"location":"topics/reference-glossary/#azure-resource-id","title":"Azure Resource ID","text":"<p>An Azure Resource ID is used to uniquely identify this Resource within Azure - in almost all cases this is a Path of Key-Value Pairs, for example:</p> <p><code>/subscriptions/11112222-3333-4444-555566667777/resourceGroups/myGroup</code></p> <p>Contains the Key-Value pairs:</p> <pre><code>subscriptions: 11112222-3333-4444-555566667777\nresourceGroups: myGroup\n</code></pre> <p>As the Azure Resource ID consists of user-specified Key-Value Pairs, the Azure Resource ID is predictable.</p>"},{"location":"topics/reference-glossary/#data-plane-api","title":"Data Plane API","text":"<p>A Data Plane API provides access to data for resources provisioned via the Resource Manager API. Some examples:</p> <ul> <li>The App Configuration Data Plane API allows for managing Keys and Features within an App Configuration.</li> <li>The Storage Data Plane API allows for the uploading/downloading of Blobs within a Storage Container (within a Storage Account).</li> </ul>"},{"location":"topics/reference-glossary/#embedded-sdk","title":"Embedded SDK","text":"<p>An Embedded SDK is an SDK that has been added directly into the providers code base (usually into <code>services/{name}/sdk</code>) rather than using go modules and vendoring it into <code>/vendor</code>.</p> <p>Whilst we generally vendor SDKs instead, we have a number of SDKs which aren't available elsewhere and are instead vendored into the codebase (see High Level Overview for more information).</p>"},{"location":"topics/reference-glossary/#resource-id-formatter","title":"Resource ID Formatter","text":"<p>A Resource ID Formatter is a Resource ID Struct which implements the <code>ID()</code> method - returning the (Azure) Resource ID as a string - which must be parseable using the associated Resource ID Parser.</p> <p>These are generally (but not always) auto-generated - see Terraform Managed Resource ID for more information.</p>"},{"location":"topics/reference-glossary/#resource-id-parser","title":"Resource ID Parser","text":"<p>A Resource ID Parser parses an (Azure) Resource ID into a Resource ID Struct - generally case-sensitively (since both Terraform Core and some downstream Azure APIs are case sensitive), but optionally case-insensitively where required.</p> <p>These are generally (but not always) auto-generated - see Terraform Managed Resource ID for more information.</p>"},{"location":"topics/reference-glossary/#resource-id-struct","title":"Resource ID Struct","text":"<p>A Resource ID Struct is a Golang Struct defining the user-specifiable values within an (Azure) Resource ID. For example, in the case of a Resource Group ID that would be the Subscription ID and Resource Group name.</p> <p>A Resource ID Struct should have an associated Resource ID Formatter, Parser and (optionally) Validator.</p> <p>These are generally (but not always) auto-generated - see Terraform Managed Resource ID for more information.</p>"},{"location":"topics/reference-glossary/#resource-id-validator","title":"Resource ID Validator","text":"<p>A Resource ID Validator is a Terraform Validation function which validates that the specified value is a Resource ID of the expected Type (for example a Subnet ID validator checks it\u2019s a Subnet ID).</p> <p>The value is parsed case-sensitively (in some cases, an optional case-insensitive validation function is also available) using the associated Resource ID Parser.</p> <p>This Resource ID Validator can then be used as a validation function within Terraform Schema fields as necessary - to confirm that the user-specified value (for example, for a Subnet ID) is actually the specified type (for example, a Subnet ID) and not another Resource ID or value (for example, a Virtual Network ID).</p> <p>These are generally (but not always) auto-generated - see Terraform Managed Resource ID for more information.</p>"},{"location":"topics/reference-glossary/#resource-manager-api","title":"Resource Manager API","text":"<p>Some Service Teams refer to this as \"Management Plane\".</p> <p>A Resource Manager API is used to provision resources within an Azure Subscription/Management Group, for example a Resource Group or a Virtual Machine.</p> <p>Whilst the Resource Manager API can be used to provision resources, resources within those are generally exposed via Data Plane APIs (see above) - for example Blobs within a Storage Account.</p>"},{"location":"topics/reference-glossary/#service-package","title":"Service Package","text":"<p>A Service Package is a grouping of Data Sources and Resources (and any other associated functionality) which are related together, for example <code>Cosmos</code> or <code>Compute</code>.</p> <p>Each Service Package contains a Service Registration which defines the Data Sources and Resources available within that Service Package.</p> <p>Whilst these tend to map 1:1 to Azure Resource Providers (for example the <code>cosmos</code> Service Package contains the CosmosDB resources) - some are intentionally split out where the Resource Provider (or Service Package) would otherwise be too large (for example the Network package has Load Balancers split out).</p>"},{"location":"topics/reference-glossary/#service-registration","title":"Service Registration","text":"<p>Each Service Package contains a Service Registration which defines the Data Sources and Resources available within that Service Package.</p> <p>This is either a Typed Service Registration or an Untyped Service Registration (documented below) - both available within the Typed Plugin SDK.</p> <p>Note that a Service Registration can be both a Typed and Untyped Service Registration by implementing both the Typed and Untyped Service Registration interfaces. This allows the mixing of both Typed and Untyped Data Sources and Resources within a Service Package.</p>"},{"location":"topics/reference-glossary/#state-migration","title":"State Migration","text":"<p>A State Migration is used when a resource has been changed to expect something different in the state than what previous version of the provider have written to it. An example of this is if Azure started to return a Resource ID value in a different case. rather than showing this during the plan, we can write a state migration to update the ID values transparently with no action required by a user. These are found in <code>services/service/migrations</code> and documentation on how to write them can be found in the Terraform Plugin SDK documentation.</p>"},{"location":"topics/reference-glossary/#terraform-managed-resource-id","title":"Terraform Managed Resource ID","text":"<p>A Terraform Managed Resource ID is a Resource ID defined in Terraform, rather than set by the Remote API.</p> <p>The Azure Provider is moving to use Terraform Managed Resource IDs for all resources, since these are known ahead of time - which avoids issues with APIs changing these Resource IDs over time (either in casing, or renaming segments altogether).</p> <p>At present these are defined in a <code>resourceids.go</code> file within each Service Package, which generates a Resource ID Formatter, Parser and Validator for this Resource ID.</p>"},{"location":"topics/reference-glossary/#terraform-resource-data","title":"Terraform Resource Data","text":"<p>Terraform Resource Data is a wrapper around the values within either the Terraform Configuration/State, depending on when this is called.</p> <p>Values within the Resource Data can be accessed using <code>d.Get</code> (for example <code>d.Get(\u201csome_field\u201d).(string)</code>) and set using <code>d.Set</code> (for example <code>d.Set(\u201csome_field\u201d, \u201chello\u201d)</code>).</p>"},{"location":"topics/reference-glossary/#terraform-resource-id","title":"Terraform Resource ID","text":"<p>Each Data Source and Resource within Terraform has a Resource ID used to keep track of this resource, set at creation/import time.</p> <p>For a Resource this is set in the Create function after the resource has been successfully provisioned (or at Import time, when imported) - and then used in the Delete, Read and Update functions to look up this resource.</p> <p>Since Data Sources look up information about existing resources - and as such don\u2019t have a Create method - these instead set the Resource ID within the Read function.</p>"},{"location":"topics/reference-glossary/#typed-data-source","title":"Typed Data Source","text":"<p>A Typed Data Source is a Terraform Data Source built using the Typed Plugin SDK, allowing this Data Source to be defined using Native Go Types.</p>"},{"location":"topics/reference-glossary/#typed-resource","title":"Typed Resource","text":"<p>A Typed Resource is a Terraform Resource built using the Typed Plugin SDK, allowing this Resource to be defined using Native Go Types.</p>"},{"location":"topics/reference-glossary/#typed-plugin-sdk","title":"Typed Plugin SDK","text":"<p>The Typed Plugin SDK is an abstraction over the Terraform Plugin SDK housed within the AzureRM Provider repository - which allows Terraform Data Sources and Resources to be built using Native Go Types.</p> <p>The Typed Plugin SDK contains both Golang Interfaces for Data Sources and Resources (which allows verifying these are valid at compile-time) - and a wrapper around Terraform Resource Data which allows for values from the Terraform Configuration to be  Serialized/Deserialized into a Native Go Struct.</p> <p>More information can be found in the documentation for the Typed Plugin SDK.</p>"},{"location":"topics/reference-glossary/#typed-service-registration","title":"Typed Service Registration","text":"<p>A Typed Service Registration returns a list of the Typed Data Sources and Typed Resources which are available within that Service Package.</p> <p>This is implemented within the Typed Plugin SDK as the interface <code>TypedServiceRegistration</code> (see also: <code>TypedServiceRegistrationWithAGitHubLabel</code>).</p>"},{"location":"topics/reference-glossary/#untyped-data-source","title":"Untyped Data Source","text":"<p>An Untyped Data Source is a Terraform Data Source built using the Terraform Plugin SDK directly, which looks up information about an existing Resource. These are exposed as a function which returns an instance of the Plugin SDK\u2019s <code>Resource</code> struct - implementing whichever methods are necessary (generally, the Schema and Read/Timeouts functions).</p> <p>The Terraform Resource Data can be used to set fields into the Terraform State - and to set the ID using <code>d.SetId(\u201c\u201d)</code>.</p>"},{"location":"topics/reference-glossary/#untyped-resource","title":"Untyped Resource","text":"<p>An Untyped Resource is a Terraform Resource built using the Terraform Plugin SDK directly, which manages this Resource (through either creation/import onwards). These are exposed as a function which returns an instance of the Plugin SDK\u2019s <code>Resource</code> struct - implementing whichever methods are necessary (generally, the Schema and Create/Read/Update/Delete/Import/Timeouts functions).</p> <p>The Terraform Resource Data can be used to retrieve fields from the Terraform Configuration/set fields into the Terraform State - and to get/set the ID using <code>d.Id()</code> / <code>d.SetId(\u201c\u201d)</code>.</p>"},{"location":"topics/reference-glossary/#untyped-service-registration","title":"Untyped Service Registration","text":"<p>An Untyped Service Registration returns a list of the Untyped Data Sources and Untyped Resources which are available within that Service Package.</p> <p>This is implemented within the Typed Plugin SDK as the interface <code>UntypedServiceRegistration</code> (see also: <code>UntypedServiceRegistrationWithAGitHubLabel</code>).</p>"},{"location":"topics/reference-naming/","title":"Property Naming","text":"<p>As with naming variables, property naming can also be a laborious task. Given the nature of the provider careful consideration should be given to property names, since changing it is a non-negligible amount of effort.</p> <p>Whilst there are many cases where the property name can be taken over 1 to 1 from the Azure API, there are many instances where this is not the case.</p> <p>Here are some general guidelines you can turn to when naming properties:</p> <ul> <li> <p>The name should describe what the property is for succinctly, but as with many things a balance should be struck between too short or too long.</p> </li> <li> <p>Choose the officially marketed name for new properties over the ones used in the API if they differ.</p> </li> <li> <p>Abbreviations should not be used and the full words should be used instead e.g. </p> <p><code>resource_group_name</code> instead of <code>rg_name</code> or <code>virtual_machine</code> instead of <code>vm</code>.</p> </li> <li> <p>For blocks avoid redundant words in the name that don't add informational value e.g.</p> <p><code>firewall_properties</code> can be shortened to <code>firewall</code>, the same can apply to individual properties e.g. <code>email_address</code> to <code>email</code>.</p> </li> <li> <p>Properties for certificates or artifacts that must be in a certain format should be appended with the format e.g.</p> <p>A certificate that must be base64 encoded should be named <code>certificate_base64</code></p> </li> <li> <p>Similarly, properties that pertain to sizes or durations/windows/occurrences should be appended with the appropriate unit of measure e.g.</p> <p><code>duration_in_seconds</code> or <code>size_in_gb</code></p> </li> <li> <p>Time properties that are not in the format of RFC3339 or are specified as UTC in the documentation should have that appended e.g.</p> <p><code>timestamp_in_utc</code></p> </li> <li> <p>For booleans these guidelines apply:</p> </li> <li> <p>As a general rule, booleans should be appended with <code>_enabled</code> e.g.</p> <p><code>compression_enabled</code></p> </li> <li> <p>Booleans named <code>disableSomething</code> in the API should be flipped and exposed as <code>something_enabled</code> in the provider.</p> </li> <li> <p>Avoid redundant verbs like <code>is</code> at the beginning of the property e.g.</p> <p><code>is_storage_enabled</code> must be renamed to <code>storage_enabled</code>.</p> </li> <li> <p>Avoid double negatives which obfuscate the purpose of the property these should be removed and flipped e.g.</p> <p><code>no_storage_enabled</code> becomes <code>storage_enabled</code> or <code>block_user_upload_enabled</code> becomes <code>user_upload_enabled</code>.</p> </li> </ul>"},{"location":"topics/running-the-tests/","title":"Running the Tests","text":"<p>Note: Acceptance tests create real resources in Azure which often cost money to run.</p> <p>Acceptance Tests for each Data Source/Resource are located within a Service Package, as such the Acceptance Tests for a given Service Package can be run via:</p> <pre><code>make acctests SERVICE='&lt;service&gt;' TESTTIMEOUT='60m'\n</code></pre> <p>However as many Service Packages contain multiple resources, you can opt to only run a subset by specifying the test prefix/filter to run as shown below:</p> <pre><code>make acctests SERVICE='&lt;service&gt;' TESTARGS='-run=&lt;nameOfTheTest&gt;' TESTTIMEOUT='60m'\n</code></pre> <ul> <li><code>&lt;service&gt;</code> is the name of the folder which contains the file with the test(s) you want to run. The available folders are found in <code>azurerm/internal/services/</code>. So examples are <code>mssql</code>, <code>compute</code> or <code>mariadb</code></li> <li><code>&lt;nameOfTheTest&gt;</code> should be self-explanatory as it is the name of the test you want to run. An example could be <code>TestAccMsSqlServerExtendedAuditingPolicy_basic</code>. Since <code>-run</code> can be used with regular expressions you can use it to specify multiple tests like in <code>TestAccMsSqlServerExtendedAuditingPolicy_</code> to run all tests that match that expression</li> </ul> <p>The following Environment Variables must be set in your shell prior to running acceptance tests:</p> <ul> <li><code>ARM_CLIENT_ID</code></li> <li><code>ARM_CLIENT_SECRET</code></li> <li><code>ARM_SUBSCRIPTION_ID</code></li> <li><code>ARM_TENANT_ID</code></li> <li><code>ARM_ENVIRONMENT</code></li> <li><code>ARM_METADATA_HOST</code></li> <li><code>ARM_TEST_LOCATION</code></li> <li><code>ARM_TEST_LOCATION_ALT</code></li> <li><code>ARM_TEST_LOCATION_ALT2</code></li> </ul> <p>Note: Acceptance tests create real resources in Azure which often cost money to run.</p>"},{"location":"topics/schema-design-considerations/","title":"Schema Design Considerations","text":"<p>Whilst it is acceptable in certain cases to map the schema of a new resource or feature when extending an existing resource one-to-one from the Azure API, in the majority of cases more consideration needs to be given how to expose the Azure API in Terraform so that the provider presents a consistent and intuitive experience to the end user.</p> <p>Below are a list of common patterns found in the Azure API and how these typically get mapped within Terraform.</p>"},{"location":"topics/schema-design-considerations/#features-that-are-toggled-by-the-property-enabled","title":"Features that are toggled by the property <code>Enabled</code>","text":"<p>It is commonplace for features to be toggled on and off by an <code>Enabled</code> property within an object in the SDK used to interact with the Azure API. See the examples below.</p> <p>Example A. <pre><code>type ManagedClusterStorageProfileBlobCSIDriver struct {\nEnabled *bool `json:\"enabled,omitempty\"`\n}\n</code></pre></p> <p>Example B. <pre><code>type ManagedClusterWorkloadAutoScalerProfileVerticalPodAutoscaler struct {\nControlledValues ControlledValues `json:\"controlledValues\"`\nEnabled          bool             `json:\"enabled\"`\nUpdateMode       UpdateMode       `json:\"updateMode\"`\n}\n</code></pre></p> <p>This is handled in the provider one of two ways depending on if the <code>Enabled</code> field is by its self or with other fields in the object.</p> <p>In the cases where <code>Enabled</code> is the only field within the object we opt to flatten the block into a single top level property (or higher level property if already nested inside a block). So in the case of Example A, this would become:</p> <pre><code>\"storage_blob_driver_enabled\": {\nType:     pluginsdk.TypeBool,\nOptional: true,\nDefault:  false,\n},\n</code></pre> <p>However, when there are multiple fields in addition to the <code>Enabled</code> field, and they are all required for the object/feature like in Example B, a block is created with all the fields including <code>Enabled</code>. The corresponding Terraform schema would be as follows:</p> <pre><code>\"vertical_pod_autoscaler\": {\nType:     pluginsdk.TypeList,\nOptional: true,\nMaxItems: 1,\nElem: &amp;pluginsdk.Resource{\nSchema: map[string]*pluginsdk.Schema{\n\"enabled\": {\nType:     pluginsdk.TypeBool,\nOptional: true,\nDefault:  false,\n},\n\"update_mode\": {\nType:     pluginsdk.TypeString,\nRequired: true,\nValidateFunc: validation.StringInSlice([]string{\nstring(managedclusters.UpdateModeAuto),\nstring(managedclusters.UpdateModeInitial),\nstring(managedclusters.UpdateModeRecreate),\n}, false),\n},\n\"controlled_values\": {\nType:     pluginsdk.TypeString,\nRequired: true,\nValidateFunc: validation.StringInSlice([]string{\nstring(managedclusters.ControlledValuesRequestsAndLimits),\nstring(managedclusters.ControlledValuesRequestsOnly),\n}, false),\n},\n},\n},\n},\n</code></pre> <p>Finally, there are instances where the additional fields/properties for an object/feature are optional or few, as shown below.</p> <p>Example C. <pre><code>type ManagedClusterStorageProfileDiskCSIDriver struct {\nEnabled *bool   `json:\"enabled,omitempty\"`\nVersion *string `json:\"version,omitempty\"`\n}\n</code></pre></p> <p>In cases like these one option is to flatten the block into two top level properties:</p> <pre><code>\"storage_disk_driver_enabled\": {\nType:     pluginsdk.TypeBool,\nOptional: true,\nDefault:  false,\n},\n\n\"storage_disk_driver_version\": {\nType:     pluginsdk.TypeString,\nOptional: true,\nDefault:  \"V1\",\nValidateFunc: validation.StringInSlice([]string{\n\"V1\",\n\"V2\",\n}, false),\n},\n</code></pre> <p>A judgement call should be made based off the behaviour of the API and expectations of a user.</p>"},{"location":"topics/schema-design-considerations/#the-none-value-or-similar","title":"The <code>None</code> value or similar","text":"<p>Many Azure APIs and services will accept the values like <code>None</code>, <code>Off</code>, or <code>Default</code> as a default value and expose it as a constant in the API specification. </p> <pre><code>    \"shutdownOnIdleMode\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"None\",\n        \"UserAbsence\",\n        \"LowUsage\"\n      ],\n</code></pre> <p>Whilst it isn't uncommon to stumble across older resources in the provider that expose and accept these as a valid values, the provider is moving away from this pattern, since Terraform has its own null type i.e. by omitting the field. Existing <code>None</code>, <code>Off</code> or <code>Default</code> values within the provider are planned for removal in version 4.0.</p> <p>This ultimately means that the end user doesn't need to bloat their configuration with superfluous information that is implied through the omission of information.</p> <p>The resulting schema in Terraform would look as follows and also requires a conversion between the Terraform null value and <code>None</code> within the Create and Read functions.</p> <pre><code>// How the property is exposed in the schema\n\"shutdown_on_idle\": {\nType:     pluginsdk.TypeString,\nOptional: true,\nValidateFunc: validation.StringInSlice([]string{\nstring(labplan.ShutdownOnIdleModeUserAbsence),\nstring(labplan.ShutdownOnIdleModeLowUsage),\n// Note: Whilst the `None` value exists it's handled in the Create/Update and Read functions.\n// string(labplan.ShutdownOnIdleModeNone),\n}, false),\n},\n\n// Normalising in the create or expand function\nfunc (r resource) Create() sdk.ResourceFunc {\n\n...\n\nvar config resourceModel\nif err := metadata.Decode(&amp;config); err != nil {\nreturn fmt.Errorf(\"decoding: %+v\", err)\n}\n\n// The resource property shutdown_on_idle maps to the attribute shutdownOnIdle in the defined model for a typed resource in this example\nshutdownOnIdle := string(labplan.ShutdownOnIdleModeNone)\nif v := model.ShutdownOnIdle; v != \"\" {\nshutdownOnIdle = v\n}\n\n...\n\n}\n\n// Normalising in the read or flatten function\nfunc (r resource) Read() sdk.ResourceFunc {\n\n...\n\nshutdownOnIdle := \"\"\nif v := props.ShutdownOnIdle; v != nil &amp;&amp; v != string(labplan.ShutdownOnIdleModeNone) {\nshutdownOnIdle = string(*v)\n}\n\nstate.ShutdownOnIdle = shutdownOnIdle\n\n...\n\n}\n</code></pre>"},{"location":"topics/schema-design-considerations/#sku-fields","title":"SKU fields","text":"<p>Because the Azure API implementation for SKU fields tends to vary we can't easily standardise on a single approach, however, we should try to stick to one of the following two implementations:</p> <ol> <li>When the SKU can be set using a single argument (e.g. only the SKU name), use a top-level <code>sku</code> argument. </li> <li>When the SKU requires multiple arguments (e.g. <code>name</code> and <code>capacity</code>), use a <code>sku</code> block.</li> </ol> <p>Example of a <code>sku</code> argument: <pre><code>\"sku\": {\nType:     pluginsdk.TypeString,\nOptional: true,\nDefault:  string(firewallpolicies.FirewallPolicySkuTierStandard),\nForceNew: true,\nValidateFunc: validation.StringInSlice([]string{\nstring(firewallpolicies.FirewallPolicySkuTierPremium),\nstring(firewallpolicies.FirewallPolicySkuTierStandard),\nstring(firewallpolicies.FirewallPolicySkuTierBasic),\n}, false),\n}\n</code></pre></p> <p>Example of a <code>sku</code> block: <pre><code>    \"sku\": {\nType:     pluginsdk.TypeList,\nRequired: true,\nMaxItems: 1,\nElem: &amp;pluginsdk.Resource{\nSchema: map[string]*pluginsdk.Schema{\n\"name\": {\nType:         pluginsdk.TypeString,\nRequired:     true,\nValidateFunc: validation.StringInSlice(helpers.PossibleValuesForSkuName(), false),\n},\n\"capacity\": {\nType:     pluginsdk.TypeInt,\nOptional: true,\nDefault:  1,\nValidateFunc: validation.IntInSlice([]int{\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200,\n}),\n},\n},\n},\n},\n</code></pre></p> <p>While you may encounter arguments like <code>sku_name</code>, <code>sku_family</code>, and <code>capacity</code> in existing resources, new arguments should avoid this format and use one of the two options above.</p>"},{"location":"topics/schema-design-considerations/#the-type-field","title":"The <code>type</code> field","text":"<p>The Azure API makes use of classes and inheritance through discriminator types defined in the REST API specifications. A strong indicator that a resource is actually a discriminated type is through the definition of a <code>type</code> or <code>kind</code> property.</p> <p>Rather than exposing a generic resource with all the possible fields for all the possible different <code>type</code>s, we intentionally opt to split these resources by the <code>type</code> to improve the user experience. This means we can only output the relevant fields for this <code>type</code> which in turn allows us to provide more granular validation etc.</p> <p>Whilst there is a trade-off here, since this means that we have to maintain more Data Sources/Resources, this is a worthwhile trade-off since each of these resources only exposes the fields which are relevant for this resource, meaning the logic is far simpler than trying to maintain a generic resource and pushing the complexity onto end-users.</p> <p>Taking the Data Factory Linked Service resources as an example which could have all of possible types defined below, each requiring a different set of inputs:</p> <pre><code>\"type\": {\nType:     pluginsdk.TypeString,\nRequired: true,\nValidateFunc: validation.StringInSlice([]string{\nstring(datafactory.TypeBasicLinkedServiceTypeAzureBlobStorage),\nstring(datafactory.TypeBasicLinkedServiceTypeAzureDatabricks),\nstring(datafactory.TypeBasicLinkedServiceTypeAzureFileStorage),\nstring(datafactory.TypeBasicLinkedServiceTypeAzureFunction),\nstring(datafactory.TypeBasicLinkedServiceTypeAzureSearch),\n...\n}, false),\n},\n</code></pre> <p>Would be better exposed as the following resources:</p> <ul> <li><code>azurerm_data_factory_linked_service_azure_blob_storage</code></li> <li><code>azurerm_data_factory_linked_service_azure_databricks</code></li> <li><code>azurerm_data_factory_linked_service_azure_file_storage</code></li> <li><code>azurerm_data_factory_linked_service_azure_function</code></li> <li><code>azurerm_data_factory_linked_service_azure_search</code></li> </ul>"}]}